{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정 (learning rate, training epochs, batch_size)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train과 test set으로 나누어 MNIST data 불러오기\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=True,\n",
    "                         transform = transforms.ToTensor(),\n",
    "                         download =True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                        train=False, # trian=False\n",
    "                        transform = transforms.ToTensor(),\n",
    "                        download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(100) #number of size\n",
    "bn2 = torch.nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0981e-01, -1.0721e-01,  8.6952e-02,  1.5056e-02,  1.5518e-01,\n",
       "          1.9080e-01, -2.1751e-01, -1.0514e-01,  4.2662e-02,  1.0691e-01,\n",
       "         -9.3403e-02,  5.1347e-02,  3.9552e-02,  1.7345e-01, -1.9088e-01,\n",
       "         -7.8941e-02, -3.9995e-02,  1.7880e-01, -8.6480e-02, -2.1657e-01,\n",
       "          1.0383e-02,  1.4692e-01,  1.0108e-01, -1.8897e-01,  2.1710e-01,\n",
       "         -1.0112e-01, -1.2709e-01, -1.3951e-01,  4.4903e-02, -9.1287e-02,\n",
       "         -1.6695e-01, -8.1733e-02, -1.4031e-01,  1.4424e-01,  1.5719e-01,\n",
       "         -7.0156e-02,  3.2802e-02, -1.4733e-01,  7.6466e-02,  1.2095e-02,\n",
       "          1.1030e-01,  6.6536e-02,  1.1679e-01,  1.7579e-01,  1.9097e-01,\n",
       "          4.8054e-02,  1.0372e-01, -1.6012e-01,  5.2269e-03,  2.9185e-02,\n",
       "         -1.8551e-02, -1.6568e-01,  4.2194e-02,  2.0233e-01, -5.6011e-02,\n",
       "         -1.4117e-01,  1.7412e-01, -6.3179e-02, -1.2242e-02, -2.1941e-01,\n",
       "         -9.1435e-02, -1.3028e-01,  2.1283e-01, -2.5206e-02, -6.4272e-02,\n",
       "          2.4650e-03, -1.7278e-01, -8.6341e-02, -1.4553e-01, -1.0737e-01,\n",
       "          7.6404e-02,  1.4942e-01,  1.2011e-01,  1.5344e-01,  1.5590e-01,\n",
       "         -4.3781e-02,  4.5019e-02,  8.1015e-02, -1.7478e-01, -1.2553e-01,\n",
       "         -7.2088e-02, -1.8272e-01, -1.9855e-01, -3.4461e-02, -7.8216e-02,\n",
       "          1.2347e-01,  1.8350e-01, -1.5265e-01, -7.2499e-02, -4.9874e-02,\n",
       "         -2.2559e-01, -2.0009e-01,  3.5219e-02, -1.1940e-01, -2.1465e-01,\n",
       "          4.2931e-02,  1.7708e-01,  2.1880e-01,  1.0744e-01,  1.8487e-01],\n",
       "        [-1.0128e-01,  1.1325e-01,  1.8655e-01,  5.6088e-02,  5.5021e-02,\n",
       "          1.5431e-01,  5.5717e-02, -1.4430e-01, -6.0771e-03,  1.9015e-01,\n",
       "          1.1111e-01, -1.1605e-01, -6.5502e-02, -1.4979e-01, -1.4667e-01,\n",
       "          5.2331e-02, -1.0313e-01,  4.2232e-02,  4.7645e-02, -3.1340e-02,\n",
       "          5.2732e-02, -2.1914e-01, -1.5379e-02,  3.8560e-02, -2.2633e-01,\n",
       "          1.2598e-01, -2.6479e-02,  1.0396e-01,  9.7196e-02, -4.0482e-02,\n",
       "          1.6999e-01,  1.2930e-01,  1.3530e-02,  7.5093e-02,  1.0833e-01,\n",
       "          5.5791e-04, -1.1775e-01,  2.5940e-02, -1.9789e-01, -2.1154e-01,\n",
       "         -6.8103e-02, -2.1335e-01, -7.2761e-02, -9.1597e-03,  1.4191e-01,\n",
       "         -2.1788e-02, -1.0571e-01,  2.2856e-01,  1.7062e-01, -6.6178e-02,\n",
       "          1.7980e-01, -2.7168e-02, -7.0865e-03, -1.8970e-01,  2.6353e-03,\n",
       "         -8.6259e-02, -9.6999e-02,  3.2517e-03,  2.0806e-01, -1.4939e-01,\n",
       "         -7.4353e-02, -1.1650e-01,  2.3014e-01, -5.5555e-02, -8.5814e-02,\n",
       "          5.2225e-02, -1.6125e-02,  1.3520e-01,  1.1756e-01, -2.1532e-01,\n",
       "         -2.1656e-01, -1.2521e-01, -1.7281e-01,  1.7837e-01,  1.4714e-01,\n",
       "         -2.4234e-02, -1.5605e-01,  7.1001e-02,  2.0276e-01, -1.6523e-01,\n",
       "          4.5801e-02,  1.0868e-01,  8.7825e-02,  1.8059e-01,  9.1226e-02,\n",
       "         -1.4106e-01,  1.3426e-01, -1.2914e-01,  2.0589e-01,  1.9219e-01,\n",
       "         -5.0891e-02,  4.9457e-02, -1.0449e-02,  2.3314e-01,  4.3940e-02,\n",
       "         -4.7673e-02,  2.0463e-01,  1.4472e-01,  1.3192e-01,  1.8731e-01],\n",
       "        [-8.4542e-02,  1.1308e-01,  8.4419e-02,  1.2047e-01,  1.6789e-02,\n",
       "          1.9276e-01,  1.7866e-01, -1.9034e-01,  2.1049e-02,  1.4519e-02,\n",
       "          1.5971e-01,  1.2390e-01,  9.7405e-02, -1.3435e-01, -7.3032e-03,\n",
       "         -8.9234e-02,  2.0528e-01, -1.1894e-01,  1.3577e-01, -9.4064e-02,\n",
       "         -1.5366e-01, -1.5742e-01,  1.3356e-01, -2.7722e-02,  1.8473e-01,\n",
       "         -8.1299e-02, -1.6226e-01, -1.0429e-01,  1.2287e-01, -7.6285e-02,\n",
       "         -9.3500e-02,  2.0800e-01,  5.3410e-04, -1.0508e-02,  1.8793e-01,\n",
       "          2.0918e-02, -7.9525e-02,  2.5482e-02,  1.2667e-01,  1.2645e-01,\n",
       "          2.1744e-01, -1.6384e-01,  1.9506e-01,  4.6022e-03,  2.1000e-01,\n",
       "          1.7547e-01, -2.2035e-01, -1.0545e-03,  2.2136e-01,  1.3312e-01,\n",
       "         -2.3046e-01,  9.3616e-02, -1.1720e-01, -8.7212e-02,  1.0285e-01,\n",
       "          1.9526e-01,  1.8899e-02,  1.3526e-01,  3.5443e-03, -7.7462e-02,\n",
       "          1.8389e-01, -1.5171e-01, -6.8851e-02,  5.4134e-02, -4.7310e-02,\n",
       "         -2.1080e-01, -1.5073e-01, -1.0870e-01,  8.8635e-02,  1.7913e-01,\n",
       "          1.1007e-01,  1.7426e-02,  2.1204e-04,  1.8612e-01, -1.0957e-01,\n",
       "         -1.0755e-01,  2.0858e-01,  2.1465e-01,  1.8059e-01, -4.5270e-02,\n",
       "         -2.0396e-01, -1.2195e-01, -1.6267e-01, -4.8151e-02, -4.8046e-02,\n",
       "          1.2526e-01, -8.5092e-02, -1.8846e-01,  1.6334e-01,  1.0485e-01,\n",
       "          1.2651e-01,  1.1388e-01, -1.5941e-01, -3.4892e-03,  3.2515e-02,\n",
       "         -1.0016e-01, -1.8780e-01, -4.7047e-02,  2.9686e-02, -1.8076e-01],\n",
       "        [-1.6827e-01,  1.0415e-01,  1.5279e-01, -1.4153e-01, -1.1628e-01,\n",
       "          1.4200e-01, -6.2108e-02,  7.9318e-02,  1.4741e-01, -1.1644e-02,\n",
       "          1.0698e-01,  6.5757e-02,  1.6796e-01,  1.2622e-01,  2.0454e-01,\n",
       "          2.1962e-02, -1.8313e-01, -1.4462e-01, -1.2495e-01, -9.8034e-02,\n",
       "          4.9914e-02, -6.2008e-03, -9.4666e-02, -1.2734e-02, -1.2176e-02,\n",
       "         -2.4897e-02,  1.4756e-01, -2.1712e-01, -9.2072e-02,  4.1471e-02,\n",
       "          2.6059e-02, -6.1406e-02, -6.1935e-02, -7.1957e-03,  4.8697e-02,\n",
       "          7.6993e-02, -1.7714e-01, -2.1286e-01,  6.9842e-02,  6.0133e-02,\n",
       "         -8.1758e-02,  8.2349e-02, -6.3160e-02,  1.7893e-01,  7.3820e-02,\n",
       "          1.6954e-01, -1.4651e-01,  1.1470e-01, -1.9520e-01,  9.7071e-02,\n",
       "         -8.3221e-02,  2.0920e-01,  2.1381e-01,  1.7158e-01,  1.3113e-01,\n",
       "          3.2253e-02,  2.4373e-02, -1.6642e-02, -2.3208e-01, -2.0461e-01,\n",
       "         -1.8818e-01, -1.9807e-01, -1.8056e-01,  1.9197e-01, -8.7063e-02,\n",
       "         -1.1585e-01,  1.6582e-01,  6.7719e-03, -2.0697e-01, -6.5672e-02,\n",
       "         -2.1702e-02,  3.1258e-02, -1.2110e-01,  1.1754e-01,  2.1435e-01,\n",
       "         -1.2033e-01,  1.4093e-01,  3.5188e-02,  1.1235e-01, -8.3915e-03,\n",
       "          1.3962e-01,  2.2104e-01,  1.5926e-01,  4.3630e-02, -3.4826e-02,\n",
       "          2.1855e-01, -1.1610e-01,  1.8660e-01,  4.4497e-02,  6.7628e-02,\n",
       "          1.4102e-01,  1.1407e-01, -1.4888e-01,  1.8730e-01,  8.9941e-02,\n",
       "          1.2240e-01, -2.0534e-01,  2.2622e-02, -1.3211e-01,  2.2993e-01],\n",
       "        [-1.0554e-01,  2.1250e-01, -2.0097e-01,  3.1472e-02, -1.5675e-01,\n",
       "         -1.1663e-01,  4.6958e-02,  1.8984e-01, -2.1679e-01,  6.4281e-02,\n",
       "         -2.1247e-02, -2.0654e-01, -2.9742e-02,  1.0938e-02,  1.9494e-01,\n",
       "          1.6119e-01,  1.2554e-01, -4.0418e-02,  1.9877e-01,  1.9938e-01,\n",
       "          6.3917e-02,  7.0818e-02, -4.9005e-02, -7.4230e-02, -6.0546e-02,\n",
       "          1.5641e-01, -6.2000e-02, -9.5443e-02, -2.2622e-01,  1.0254e-01,\n",
       "          1.9520e-01, -5.9438e-02,  6.2021e-02, -1.7510e-02, -1.8715e-01,\n",
       "          1.2932e-01, -2.0824e-01,  2.0249e-01,  1.7132e-01, -8.9027e-02,\n",
       "         -4.2782e-02, -1.6115e-01, -8.5588e-02, -5.0610e-02, -1.3573e-01,\n",
       "         -1.1904e-01, -1.3642e-01,  9.5338e-03,  1.7330e-01,  8.1193e-02,\n",
       "         -1.3988e-01, -5.6629e-02,  1.2642e-01, -3.3258e-02, -1.5375e-01,\n",
       "          1.3736e-01,  7.4608e-02, -1.5418e-01,  6.3358e-02, -1.8504e-01,\n",
       "          8.5542e-02,  1.9212e-01,  4.8832e-02, -6.1738e-02,  1.1171e-01,\n",
       "          6.5698e-02,  1.3804e-02, -1.2605e-01,  6.7858e-03, -1.2787e-01,\n",
       "          1.0380e-01, -1.9976e-01, -9.9732e-02, -1.1953e-01, -8.1695e-02,\n",
       "         -2.1755e-01, -7.2140e-02,  1.8432e-01,  1.9501e-01, -5.3798e-02,\n",
       "          2.1055e-01, -1.6511e-01,  1.5862e-01,  2.2246e-01,  8.7864e-02,\n",
       "          1.4285e-01,  2.3150e-01, -2.0033e-01, -3.2699e-02,  1.4385e-01,\n",
       "          1.1855e-01, -1.9786e-02, -9.6244e-02,  1.3484e-01, -1.8201e-01,\n",
       "          1.0602e-01,  1.6751e-01, -7.2576e-03,  2.1109e-01, -1.3181e-01],\n",
       "        [-8.2252e-02,  1.4864e-01,  8.3386e-02, -1.9983e-01,  1.1762e-01,\n",
       "          1.4112e-01, -1.1277e-01,  3.2908e-03,  2.2551e-01, -1.9288e-01,\n",
       "          9.2880e-02,  7.9519e-02, -8.4870e-02, -6.6110e-03,  5.0991e-03,\n",
       "          1.5435e-02, -1.5126e-01, -1.9659e-01, -5.7308e-02,  2.1686e-01,\n",
       "          2.0353e-01,  7.5964e-02, -2.1628e-02,  1.3467e-01,  2.3321e-01,\n",
       "         -1.1273e-01, -1.9551e-01,  6.0504e-02,  1.3093e-01, -9.4874e-02,\n",
       "          1.3800e-01, -1.5655e-01,  2.1285e-02, -1.8383e-02, -1.4530e-01,\n",
       "         -8.6880e-02, -2.2951e-01, -4.8034e-02,  2.0953e-01,  4.5898e-02,\n",
       "          2.3015e-01, -1.2992e-01, -1.9236e-02,  1.9842e-01, -1.7435e-01,\n",
       "         -6.6248e-03,  1.1090e-01, -1.6355e-01,  2.7542e-02,  9.2871e-02,\n",
       "         -3.1132e-02,  6.2135e-02,  2.1872e-01, -1.4783e-01,  4.4673e-02,\n",
       "          2.0755e-02, -4.0766e-02,  9.1495e-02, -1.5644e-01,  8.0998e-02,\n",
       "         -1.1465e-01, -4.0448e-02,  1.0695e-02, -1.6267e-01,  2.6171e-02,\n",
       "         -1.8269e-01, -1.0977e-01, -6.6636e-02,  8.0732e-02, -2.0953e-01,\n",
       "         -1.5827e-01,  1.6871e-01, -8.8935e-02, -1.5590e-01,  1.7491e-02,\n",
       "          7.5636e-02,  2.1126e-01,  1.8213e-01,  1.9858e-01, -2.1539e-02,\n",
       "         -2.2087e-01,  1.0368e-01,  2.0953e-01,  4.9570e-02, -1.0028e-01,\n",
       "          2.0010e-01,  2.1604e-01, -1.9478e-01,  2.2281e-01,  1.7754e-01,\n",
       "          5.4976e-02,  1.3156e-01,  2.9365e-03, -2.2094e-01,  1.1220e-01,\n",
       "          2.0786e-01,  8.7563e-02,  2.2209e-01,  1.5529e-01,  1.7556e-02],\n",
       "        [ 9.4004e-02,  1.0395e-02,  7.3124e-02,  1.4272e-01, -1.8515e-01,\n",
       "          1.8820e-01, -8.1181e-02, -1.7246e-02, -2.0770e-01,  1.2162e-01,\n",
       "         -8.8400e-02, -5.3190e-02, -2.2253e-01,  3.3181e-02,  1.1415e-01,\n",
       "         -2.4125e-02, -1.1814e-01,  1.3264e-01, -4.3203e-03, -1.3613e-01,\n",
       "          1.9969e-01,  8.9912e-02, -7.5653e-02, -7.9337e-02, -1.6238e-01,\n",
       "         -1.4149e-01, -8.2917e-02,  1.8050e-01,  2.2363e-01,  9.6932e-02,\n",
       "          1.2406e-01, -1.7825e-01, -4.5152e-02,  1.4283e-01, -1.9252e-02,\n",
       "         -1.0149e-01,  6.9317e-03,  9.8935e-02,  1.1880e-01,  1.2401e-01,\n",
       "         -1.3909e-01, -1.4894e-01,  8.3229e-02, -9.5479e-03,  7.9870e-02,\n",
       "          1.4237e-01, -1.2593e-01,  7.2072e-02, -9.0990e-02, -1.8393e-01,\n",
       "          1.0528e-01, -1.7149e-01,  5.0399e-03, -1.9387e-01,  5.7397e-02,\n",
       "          1.0367e-01,  4.7139e-02,  3.9824e-02,  1.2928e-01, -4.2600e-02,\n",
       "         -1.7953e-01, -1.1136e-01,  8.6331e-02,  1.1245e-01,  6.4030e-02,\n",
       "         -1.6223e-01,  1.3895e-01, -9.1444e-02,  5.3293e-02, -1.9299e-02,\n",
       "         -2.2199e-01,  9.6953e-02,  1.9650e-02,  8.2306e-02,  3.1325e-02,\n",
       "         -4.0832e-02, -7.7086e-02,  1.8495e-01,  6.5808e-02,  1.5039e-01,\n",
       "         -2.7282e-02, -1.6266e-01, -2.1046e-02,  3.6932e-02,  1.8022e-01,\n",
       "         -1.3192e-01, -6.3063e-02,  8.3229e-02, -6.8693e-02, -6.4369e-02,\n",
       "         -4.5046e-02,  2.0560e-01,  7.6940e-02,  9.7044e-02,  3.9561e-02,\n",
       "         -7.8175e-02,  1.9446e-01,  1.9696e-01,  2.2079e-01, -5.1869e-02],\n",
       "        [ 3.5608e-02,  8.6639e-02,  5.1243e-02,  4.3912e-02,  7.7627e-02,\n",
       "         -1.1290e-01, -1.2979e-01,  1.9289e-01, -1.4722e-01,  1.3671e-01,\n",
       "         -8.6943e-02,  4.4030e-02, -1.2742e-01, -4.0607e-02,  8.9006e-03,\n",
       "          3.8656e-02, -2.0652e-01, -1.7878e-01,  1.9104e-01,  4.9862e-02,\n",
       "          1.1353e-01,  2.2676e-01, -2.1862e-02,  2.2685e-01, -1.8942e-01,\n",
       "          1.1296e-01,  2.1496e-01,  7.0926e-02, -2.1472e-01, -1.0760e-01,\n",
       "         -1.7801e-01,  3.7263e-02, -8.7933e-02, -4.3651e-02,  9.7087e-02,\n",
       "          2.1964e-01, -2.0269e-01,  1.4350e-01, -1.2456e-01,  2.0073e-01,\n",
       "          1.7986e-02,  1.0531e-01,  3.7523e-02, -9.5253e-02, -2.9536e-03,\n",
       "          1.0462e-03,  1.2073e-01,  1.9788e-01,  1.1266e-01, -1.5008e-01,\n",
       "         -5.4909e-02,  3.6342e-02,  9.4581e-02,  1.6703e-01,  1.6517e-01,\n",
       "          2.1624e-01, -2.1096e-01, -1.3210e-01,  3.4758e-02, -1.7773e-02,\n",
       "         -2.3023e-01, -4.3593e-02,  1.9048e-01,  2.2447e-01,  1.9919e-01,\n",
       "         -2.2078e-01, -2.6405e-02,  2.0213e-01, -1.2642e-01,  1.0889e-01,\n",
       "          2.4489e-03,  1.6778e-01,  1.7400e-02, -7.9289e-02, -8.4484e-02,\n",
       "          1.5193e-01, -1.1550e-01,  1.1041e-01, -1.8835e-01,  1.6043e-02,\n",
       "          1.5661e-01,  6.1725e-02,  4.8890e-02,  2.2380e-01,  2.0690e-01,\n",
       "          7.4972e-02, -8.7053e-02, -2.6021e-02, -1.9882e-01,  1.1639e-01,\n",
       "          1.6146e-01,  5.9875e-02,  1.2303e-01,  7.6760e-03, -2.0204e-01,\n",
       "         -3.3589e-02,  2.0790e-01, -1.9644e-01,  1.7735e-02,  1.4442e-01],\n",
       "        [-4.2686e-02, -1.4991e-02,  2.2322e-01,  3.2631e-02, -4.1893e-02,\n",
       "          9.7883e-02, -3.7477e-03,  1.0955e-01,  8.9706e-02,  3.8882e-03,\n",
       "         -8.9703e-02,  1.0943e-01,  1.3733e-01,  1.2789e-01, -2.0447e-01,\n",
       "          2.0979e-01,  9.5336e-02,  1.1565e-01, -2.0763e-01,  8.5899e-02,\n",
       "          1.7204e-01, -8.0873e-02, -1.7980e-01,  2.2975e-01, -2.1969e-01,\n",
       "         -2.1179e-01, -1.0181e-01,  2.6852e-02,  1.4402e-01,  2.3852e-02,\n",
       "          1.5856e-01,  1.7721e-01, -2.0645e-01, -1.5106e-01, -8.4717e-02,\n",
       "         -3.2578e-02,  1.1217e-01,  2.2334e-01,  1.1190e-01, -6.3122e-02,\n",
       "         -1.3979e-02, -1.8239e-01, -8.3661e-02,  1.1208e-01,  2.3579e-02,\n",
       "         -3.0340e-03,  1.0771e-01, -2.5112e-02, -1.1171e-01, -1.9375e-01,\n",
       "         -1.1710e-01, -1.0268e-01,  1.3065e-01, -6.6717e-02,  9.6136e-02,\n",
       "         -1.9592e-01, -1.6338e-01, -7.8873e-02, -2.0217e-02, -2.1183e-01,\n",
       "          8.9205e-02,  8.5812e-02, -1.1027e-01,  1.8481e-01,  1.4071e-01,\n",
       "          1.3889e-03,  1.8950e-01, -6.7166e-02, -2.2067e-01,  4.3700e-02,\n",
       "          1.6933e-02,  3.2983e-02, -1.4079e-01,  1.6691e-01,  3.5300e-02,\n",
       "          2.9240e-02, -1.9606e-02,  1.3176e-01, -2.3327e-01, -2.1277e-01,\n",
       "         -1.9901e-01, -3.6783e-03, -1.0113e-01,  1.3221e-01, -1.6686e-01,\n",
       "          1.2623e-01,  2.3140e-01,  4.1765e-02, -2.0684e-01, -1.2220e-01,\n",
       "          6.9772e-02, -3.2088e-03,  2.1866e-01, -2.1768e-02,  5.1634e-04,\n",
       "          1.5522e-01,  1.4911e-01, -1.1599e-01,  1.7423e-01, -2.2196e-03],\n",
       "        [ 3.2832e-02, -1.5877e-02,  2.0064e-01, -1.1250e-02,  2.1412e-01,\n",
       "         -1.1051e-01,  8.7332e-02,  1.6988e-01,  1.6669e-01,  1.3649e-01,\n",
       "          1.3549e-01,  1.8298e-01,  1.0826e-01,  1.7298e-01, -8.3242e-03,\n",
       "         -2.7094e-02,  1.5539e-01, -1.7782e-01, -1.2226e-01,  1.2596e-01,\n",
       "         -4.5651e-02, -7.5352e-02, -3.8205e-02,  1.7494e-01, -1.6085e-01,\n",
       "          2.2156e-01, -2.2537e-01,  4.3736e-02,  2.3148e-01, -6.5361e-02,\n",
       "          1.5901e-01,  4.8097e-02, -1.7947e-01,  8.6435e-02, -1.8488e-01,\n",
       "          8.8487e-02,  3.6644e-02,  1.8966e-02,  3.0428e-02,  7.8718e-02,\n",
       "          1.8954e-01, -1.4982e-01, -1.1906e-01,  1.6692e-01,  1.8861e-01,\n",
       "         -1.6512e-01, -6.2644e-05,  2.8773e-02,  1.7786e-01, -6.6097e-02,\n",
       "          5.2212e-02, -2.3137e-01, -1.7370e-01, -1.1026e-01, -1.7388e-01,\n",
       "          1.8015e-02, -9.0507e-02, -3.4635e-02,  2.2307e-01,  4.7814e-02,\n",
       "         -6.9264e-02,  2.2933e-01,  1.5792e-01, -1.0309e-01, -1.0659e-01,\n",
       "          5.0767e-02, -1.8894e-01,  1.2951e-02,  2.2852e-01,  1.7794e-01,\n",
       "          4.8591e-02,  1.9171e-01, -6.4014e-02,  2.2090e-01, -1.9589e-01,\n",
       "          1.1236e-01,  1.0028e-01, -4.6965e-02,  1.9438e-01,  2.2172e-01,\n",
       "          4.2924e-02,  5.8571e-02, -4.3751e-02,  4.1888e-02,  1.0186e-02,\n",
       "         -1.8068e-01,  1.6286e-01, -6.0733e-02, -1.7113e-01, -1.0209e-01,\n",
       "         -1.9940e-01, -2.6814e-02, -1.0450e-01, -1.0672e-01,  2.5729e-02,\n",
       "          7.2453e-02,  2.4740e-02, -7.0965e-02, -1.3791e-01, -2.1825e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "model = torch.nn.Sequential(linear1,  bn1, relu, dropout, \n",
    "                           linear2, bn2, relu,  dropout,\n",
    "                            linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.488649964\n",
      "Epoch: 0002 cost= 0.362622976\n",
      "Epoch: 0003 cost= 0.321941078\n",
      "Epoch: 0004 cost= 0.308767676\n",
      "Epoch: 0005 cost= 0.302153140\n",
      "Epoch: 0006 cost= 0.280230403\n",
      "Epoch: 0007 cost= 0.279722035\n",
      "Epoch: 0008 cost= 0.269009650\n",
      "Epoch: 0009 cost= 0.263899088\n",
      "Epoch: 0010 cost= 0.254575878\n",
      "Epoch: 0011 cost= 0.257850736\n",
      "Epoch: 0012 cost= 0.252794951\n",
      "Epoch: 0013 cost= 0.244105220\n",
      "Epoch: 0014 cost= 0.241841078\n",
      "Epoch: 0015 cost= 0.238994747\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "model.train()\n",
    "\n",
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0 \n",
    "    \n",
    "    # train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    for X,Y in train_loader:\n",
    "        \n",
    "        # data\n",
    "        X= X.view(-1,28*28) # reshaping  X data\n",
    "        Y= Y\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #H(X) \n",
    "        hypothesis = model(X)\n",
    "        \n",
    "        #Cost\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        \n",
    "        # H(X) 개선; backpropagation\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost +=cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9373000264167786\n",
      "Label:  5\n",
      "Prediction:  3\n"
     ]
    }
   ],
   "source": [
    "## test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "## X_test 불러올 때 view를 사용하여 차원 변환할 것 / Y_test를 불러올때 labels 사용\n",
    "## accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    prediction = model(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction,1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    # test set에서 random으로 data를 뽑아 label과 prediction 비교\n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지는 Layer의 수를 바꾸거나, Batch Normalization Layer를 추가하는 등 Layer에만 변화를 주며 모델의 성능을 향상 시켰습니다.  \n",
    "이번 문제에서는 위에서 만든 모델에서 있던 Layer 들의 Hidden node 수를 증가 또는 감소 (ex: 200, 300, 50...) 시켰을 때, train set에서의 cost와 test set에서 Accuracy가 기존 결과와 비교하였을 때 어떻게 달라졌는지 비교해주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 200 -> 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1951, -0.2133, -0.2134, -0.1415, -0.1620, -0.1431, -0.1249, -0.1338,\n",
       "          0.0800,  0.1021, -0.0372, -0.2148, -0.1905,  0.1356, -0.2102, -0.0140,\n",
       "         -0.1480,  0.1875,  0.1343, -0.1517, -0.0526, -0.1073,  0.1789,  0.1999,\n",
       "         -0.1290, -0.1908, -0.2164,  0.2228, -0.2028,  0.2091,  0.1757,  0.1191,\n",
       "          0.0966,  0.0155, -0.0659,  0.2021, -0.0796, -0.1018,  0.2250, -0.0780,\n",
       "         -0.1173,  0.1345,  0.1729, -0.2312, -0.0326, -0.1480,  0.1174, -0.0660,\n",
       "          0.1260,  0.1695,  0.0371,  0.1525, -0.2014, -0.1760, -0.1217,  0.1358,\n",
       "         -0.1405, -0.0584, -0.0756,  0.0834, -0.1673,  0.0778,  0.0291, -0.0301,\n",
       "          0.1514, -0.1559, -0.1588,  0.0123, -0.2090, -0.1544, -0.1687, -0.2231,\n",
       "          0.0927, -0.0825, -0.1326, -0.0513, -0.1794, -0.0619, -0.0980, -0.1099,\n",
       "         -0.1337, -0.1644, -0.0753, -0.1961, -0.0322,  0.0352,  0.0598, -0.1656,\n",
       "         -0.0871, -0.0680,  0.1298,  0.1650,  0.1186,  0.1699, -0.1877, -0.0783,\n",
       "         -0.1983, -0.2219,  0.1992, -0.2048],\n",
       "        [-0.1456,  0.1731, -0.0460,  0.0955, -0.1259,  0.0901,  0.0913, -0.0294,\n",
       "          0.2106,  0.0646,  0.1684,  0.1154, -0.1419,  0.2322, -0.1673,  0.0351,\n",
       "          0.1361, -0.1014,  0.1577, -0.1922,  0.0117,  0.0118, -0.0060,  0.2305,\n",
       "         -0.2163,  0.1879, -0.0761, -0.2158, -0.0316,  0.2270, -0.0402,  0.0991,\n",
       "          0.0414, -0.0116, -0.1633, -0.2311, -0.0914, -0.2212, -0.1781,  0.1178,\n",
       "          0.1978,  0.1131, -0.0122, -0.1068, -0.1385, -0.0484,  0.0510, -0.1520,\n",
       "         -0.0526,  0.1221,  0.1648, -0.0810, -0.2072,  0.1210, -0.1596, -0.0345,\n",
       "          0.0598, -0.2325,  0.1966,  0.0366,  0.0893,  0.1091, -0.2248, -0.1696,\n",
       "          0.0911,  0.1338,  0.1007,  0.1456,  0.2202,  0.0022,  0.1281,  0.1278,\n",
       "         -0.2321, -0.2021,  0.1347, -0.1997, -0.0978, -0.1647, -0.0508,  0.0805,\n",
       "          0.0124,  0.1988,  0.1592,  0.2322,  0.0994, -0.0261, -0.1553, -0.0055,\n",
       "          0.0691, -0.1766, -0.2131, -0.1699, -0.1974, -0.1091,  0.1538,  0.0234,\n",
       "          0.1823, -0.0013, -0.1062, -0.1925],\n",
       "        [-0.0934, -0.2127,  0.0828, -0.0932,  0.1418,  0.1450,  0.0063,  0.1496,\n",
       "         -0.0450,  0.2208, -0.0663,  0.1432,  0.1019,  0.1040,  0.1669, -0.2187,\n",
       "          0.2264,  0.2129,  0.0556, -0.0859,  0.1213,  0.1277,  0.0892,  0.1474,\n",
       "          0.0969, -0.0434,  0.0039,  0.1451,  0.1023,  0.1028, -0.1512, -0.0225,\n",
       "         -0.1099, -0.0194,  0.1463,  0.1017, -0.1180, -0.2103, -0.1278,  0.1154,\n",
       "          0.1275, -0.1735, -0.1958, -0.1826,  0.1153, -0.0874,  0.0287, -0.2037,\n",
       "          0.1486,  0.1903,  0.0787, -0.1840,  0.0668, -0.1886,  0.0864,  0.0992,\n",
       "         -0.1392, -0.1855,  0.1388, -0.1811, -0.0441, -0.1588,  0.1532, -0.1971,\n",
       "          0.1628,  0.0844,  0.0534, -0.0067, -0.1216, -0.1376, -0.1421,  0.1272,\n",
       "          0.0785,  0.0872,  0.1442, -0.1720,  0.0024, -0.0207,  0.2290,  0.1305,\n",
       "          0.1032,  0.1961,  0.1345, -0.0873,  0.1694, -0.0432,  0.1239, -0.0776,\n",
       "         -0.2277,  0.0647, -0.2130, -0.2075,  0.0187, -0.0599, -0.2330,  0.0592,\n",
       "          0.0809,  0.2160,  0.1664,  0.1143],\n",
       "        [-0.1468, -0.0319,  0.1692, -0.0131,  0.0348, -0.1235,  0.0271,  0.0803,\n",
       "          0.0954,  0.0638, -0.1168, -0.0152, -0.1193,  0.0460, -0.0633,  0.1261,\n",
       "          0.1372, -0.0968, -0.0649, -0.0312,  0.0953, -0.0856, -0.0826, -0.1887,\n",
       "         -0.0655,  0.0275, -0.1752,  0.0509, -0.0771,  0.1488,  0.2253, -0.0830,\n",
       "          0.1671,  0.2054,  0.1729,  0.0082, -0.1770,  0.1422, -0.0767, -0.1356,\n",
       "          0.1284, -0.0568, -0.1320, -0.2059, -0.0886, -0.2268,  0.2166,  0.0890,\n",
       "          0.0543, -0.0693, -0.1414, -0.1826, -0.0867, -0.2208,  0.1752, -0.2011,\n",
       "          0.2067, -0.0086, -0.0030, -0.2158, -0.2309, -0.1269,  0.1649,  0.1457,\n",
       "          0.2119,  0.2124, -0.0795, -0.2333,  0.1126, -0.1234, -0.2128,  0.1051,\n",
       "          0.1282,  0.1330, -0.1610, -0.0860, -0.1195,  0.0367,  0.1857,  0.2254,\n",
       "          0.0849, -0.1638,  0.2120,  0.0986, -0.1852,  0.0134,  0.0062,  0.1842,\n",
       "          0.1704, -0.2331,  0.1272,  0.1750, -0.1984, -0.1165, -0.1460,  0.0095,\n",
       "         -0.1284,  0.1312, -0.0795, -0.2171],\n",
       "        [-0.1590, -0.0278, -0.0916, -0.1929,  0.1428,  0.0182, -0.0336,  0.0789,\n",
       "          0.0707, -0.0256,  0.2235, -0.1988, -0.2134, -0.0634,  0.0393, -0.0760,\n",
       "         -0.1322,  0.0473,  0.0888,  0.0130, -0.0760,  0.2142,  0.1275, -0.1915,\n",
       "         -0.0019, -0.0502,  0.1682,  0.2317, -0.2009,  0.0599, -0.1480,  0.2061,\n",
       "         -0.0013,  0.2087, -0.1709, -0.0107,  0.2055, -0.0376, -0.0900, -0.0735,\n",
       "          0.0144, -0.1045,  0.1314,  0.0934,  0.1129, -0.1956, -0.1456,  0.1031,\n",
       "         -0.1059, -0.1336, -0.1266, -0.0686,  0.2184, -0.1203, -0.0222,  0.2140,\n",
       "          0.1808, -0.1539,  0.1088, -0.2020, -0.0451, -0.1695, -0.1211,  0.0748,\n",
       "          0.1507,  0.1449,  0.0882, -0.1084, -0.0167, -0.0879, -0.0696, -0.0601,\n",
       "         -0.0306, -0.1498,  0.1303,  0.1264,  0.1172,  0.0334,  0.0686,  0.1837,\n",
       "         -0.0142, -0.1531,  0.2066,  0.2235, -0.1027, -0.0304, -0.1431, -0.2070,\n",
       "         -0.2054, -0.1629, -0.2026, -0.2132, -0.0607, -0.1306, -0.1743, -0.0886,\n",
       "          0.1385, -0.0718,  0.0497,  0.0495],\n",
       "        [-0.1172,  0.1710, -0.1085,  0.2223,  0.2070,  0.0162, -0.1512,  0.0055,\n",
       "         -0.1826,  0.2091, -0.1393,  0.0710,  0.2129, -0.2071, -0.0620, -0.0899,\n",
       "         -0.1464, -0.0026, -0.2128, -0.0395,  0.0978,  0.1486,  0.0367, -0.0616,\n",
       "          0.0185, -0.1028,  0.0882, -0.1206, -0.1469,  0.1450,  0.0335, -0.1498,\n",
       "         -0.2278,  0.1554,  0.0346, -0.0471, -0.1243, -0.0289,  0.0079, -0.1480,\n",
       "         -0.1396, -0.0928, -0.1440, -0.0276, -0.0898,  0.0692,  0.0115, -0.2245,\n",
       "          0.0348,  0.2248, -0.0890,  0.0626,  0.1498,  0.1993, -0.2163, -0.1828,\n",
       "          0.1856, -0.0497,  0.0744, -0.1736, -0.1593, -0.2268,  0.1932,  0.1586,\n",
       "         -0.1073, -0.1233,  0.1793,  0.0322,  0.1142,  0.1608, -0.2107, -0.0571,\n",
       "         -0.0080, -0.0706, -0.1429,  0.1782, -0.1255,  0.0418, -0.2256, -0.1412,\n",
       "         -0.1868,  0.1842,  0.1383, -0.0621,  0.0166,  0.2183, -0.1142,  0.1803,\n",
       "          0.0391,  0.0072, -0.2030, -0.2002, -0.0183,  0.1237,  0.0933,  0.0224,\n",
       "          0.2092, -0.0240, -0.1290,  0.1336],\n",
       "        [ 0.2122, -0.1522, -0.2100,  0.1951, -0.1097,  0.1497, -0.1239, -0.1854,\n",
       "         -0.1107, -0.1372, -0.0150,  0.1991,  0.0554,  0.1210,  0.2240,  0.0320,\n",
       "          0.1837,  0.0328,  0.1137, -0.1947,  0.0952, -0.0169, -0.1393,  0.1278,\n",
       "          0.2271,  0.1897,  0.0837, -0.1414, -0.1857,  0.0928,  0.0706,  0.2249,\n",
       "          0.1960,  0.1639,  0.1309, -0.2098,  0.0660,  0.2319,  0.1463, -0.0759,\n",
       "         -0.1546,  0.0964, -0.1571,  0.2061,  0.0882, -0.0257,  0.0953, -0.1061,\n",
       "         -0.2118, -0.0420, -0.1182,  0.1637,  0.1710,  0.0272, -0.0325, -0.0811,\n",
       "          0.2116,  0.1251,  0.1410, -0.0381,  0.0135, -0.0798,  0.1607,  0.1746,\n",
       "         -0.0475,  0.1168,  0.2110, -0.0375,  0.1862, -0.1556, -0.1493, -0.1128,\n",
       "         -0.0677, -0.1062, -0.1983,  0.2277, -0.1695,  0.0296,  0.1642, -0.0377,\n",
       "         -0.0901, -0.0475, -0.0116,  0.1567, -0.2025,  0.2325, -0.0009, -0.1105,\n",
       "         -0.0970, -0.1581, -0.1253,  0.1268,  0.1385,  0.0157, -0.1247, -0.1575,\n",
       "          0.1261, -0.0597, -0.2315,  0.0083],\n",
       "        [ 0.0186,  0.0013, -0.1459, -0.0443,  0.1034,  0.1120,  0.1008,  0.1781,\n",
       "         -0.0598,  0.0942,  0.0542,  0.0474, -0.1743,  0.0745, -0.0715, -0.0552,\n",
       "          0.0446, -0.2145,  0.2235, -0.2134,  0.1784,  0.0309, -0.0498, -0.1622,\n",
       "          0.1764, -0.0696, -0.1833, -0.1009,  0.0373, -0.1165, -0.1616,  0.1834,\n",
       "          0.1133,  0.1934,  0.0486,  0.0411,  0.1061, -0.0010, -0.0965, -0.0375,\n",
       "         -0.0867,  0.1258, -0.0448, -0.1355, -0.1082, -0.1860, -0.0153, -0.1161,\n",
       "          0.1352, -0.2012,  0.1426,  0.1979, -0.1424, -0.0977,  0.1162, -0.1031,\n",
       "         -0.1146, -0.0913,  0.1047,  0.1732, -0.0019,  0.0319, -0.1480,  0.1464,\n",
       "          0.0416, -0.1655,  0.2083,  0.1917,  0.1450, -0.0682, -0.0537,  0.0035,\n",
       "         -0.0609, -0.1455, -0.1238, -0.2114, -0.1377,  0.1591,  0.0504,  0.2126,\n",
       "         -0.1048,  0.2078, -0.0968, -0.1294,  0.1501,  0.0817,  0.0740,  0.1972,\n",
       "          0.2138,  0.1057,  0.1142, -0.0261, -0.2004,  0.0613,  0.1851, -0.0283,\n",
       "         -0.0368, -0.0163, -0.0807, -0.1409],\n",
       "        [ 0.1244,  0.0015, -0.1550, -0.0112, -0.1759,  0.1405, -0.0648,  0.1341,\n",
       "          0.1825,  0.0022,  0.0983,  0.0397, -0.2161, -0.1826,  0.0165, -0.0981,\n",
       "          0.1033, -0.0680, -0.2211,  0.1123, -0.2035, -0.2328, -0.1350,  0.0080,\n",
       "         -0.1592, -0.0496, -0.1033, -0.0645,  0.0057,  0.0187, -0.1547,  0.0999,\n",
       "          0.0956, -0.0572, -0.0583,  0.1120,  0.1993, -0.2033, -0.1228, -0.0733,\n",
       "         -0.2093,  0.0930, -0.0557,  0.0416, -0.0343, -0.2014, -0.2156,  0.1260,\n",
       "          0.0674,  0.1783, -0.0717,  0.0455,  0.1979,  0.0452, -0.1952, -0.1546,\n",
       "         -0.0161,  0.1463, -0.1454, -0.0918, -0.0987,  0.0746,  0.0638,  0.1562,\n",
       "         -0.0429,  0.1588,  0.1879, -0.2198, -0.0780, -0.2056,  0.0537,  0.1620,\n",
       "         -0.0664, -0.1875, -0.1198, -0.0102,  0.1376, -0.0299,  0.2266, -0.1699,\n",
       "         -0.0424, -0.1992,  0.0781, -0.0674, -0.1739,  0.1748,  0.0922,  0.2093,\n",
       "          0.1672,  0.0553, -0.0266,  0.1511, -0.1518, -0.1936,  0.1312,  0.0969,\n",
       "          0.0552,  0.0977,  0.0541, -0.1504],\n",
       "        [-0.1535,  0.0633,  0.1585, -0.1887,  0.1346, -0.0156,  0.1661, -0.1977,\n",
       "          0.0380,  0.0578, -0.0258, -0.0237,  0.2335,  0.1115, -0.1624, -0.0413,\n",
       "         -0.0380, -0.2072, -0.1325,  0.1613, -0.0452,  0.0883, -0.1475,  0.0515,\n",
       "          0.0560, -0.0667, -0.1797, -0.1935,  0.0737, -0.2156, -0.1273,  0.1858,\n",
       "          0.0582,  0.0723,  0.1792, -0.1143, -0.1483, -0.0278,  0.0181,  0.0214,\n",
       "         -0.1852,  0.1353, -0.2042,  0.2124, -0.2231, -0.2179,  0.0461,  0.1274,\n",
       "         -0.0714, -0.0037, -0.1663,  0.0509,  0.0327, -0.0442, -0.0549,  0.0819,\n",
       "          0.0473, -0.1658,  0.2124,  0.1848, -0.1568, -0.0219,  0.0706,  0.0819,\n",
       "          0.0997,  0.0120, -0.0396,  0.0418, -0.1579,  0.1039,  0.0494,  0.0779,\n",
       "          0.0577,  0.0508, -0.1928,  0.2045, -0.1253, -0.0078,  0.0638, -0.0967,\n",
       "          0.1415,  0.1600,  0.1487, -0.2243,  0.1815,  0.1001,  0.0083,  0.1994,\n",
       "          0.1351,  0.1421,  0.1015, -0.0763,  0.0581, -0.2091, -0.0671, -0.1772,\n",
       "          0.1612,  0.0948,  0.1790, -0.2243]], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear1 = torch.nn.Linear(784, 200, bias=True)\n",
    "linear2 = torch.nn.Linear(200, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(200)\n",
    "bn2 = torch.nn.BatchNorm1d(100)\n",
    "\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.nn.Sequential(linear1,  bn1, relu, dropout, \n",
    "                           linear2, bn2, relu,  dropout,\n",
    "                            linear3)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.474311352\n",
      "Epoch: 0002 cost= 0.334704727\n",
      "Epoch: 0003 cost= 0.293660253\n",
      "Epoch: 0004 cost= 0.266254723\n",
      "Epoch: 0005 cost= 0.246628985\n",
      "Epoch: 0006 cost= 0.242055714\n",
      "Epoch: 0007 cost= 0.233642489\n",
      "Epoch: 0008 cost= 0.231927484\n",
      "Epoch: 0009 cost= 0.220189765\n",
      "Epoch: 0010 cost= 0.215765893\n",
      "Epoch: 0011 cost= 0.212811112\n",
      "Epoch: 0012 cost= 0.209594056\n",
      "Epoch: 0013 cost= 0.202941656\n",
      "Epoch: 0014 cost= 0.200198144\n",
      "Epoch: 0015 cost= 0.196202755\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "train_total_batch = len(train_loader)\n",
    "model2.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0 \n",
    "    \n",
    "    for X,Y in train_loader:\n",
    "        \n",
    "        X= X.view(-1,28*28) # reshaping  X data\n",
    "        Y= Y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model2(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost +=cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9151999950408936\n",
      "Label:  2\n",
      "Prediction:  2\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model2.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    prediction = model2(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction,1) == Y_test\n",
    "    accuracy2 = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy2.item())\n",
    "    \n",
    "    # test set에서 random으로 data를 뽑아 label과 prediction 비교\n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model2(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 300 -> 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-4.9427e-02,  4.2683e-02, -4.8571e-02,  ..., -1.6365e-01,\n",
       "          1.1969e-01, -1.2394e-01],\n",
       "        [ 9.4951e-02, -1.0727e-04,  3.8698e-02,  ...,  5.4700e-02,\n",
       "         -3.3416e-02, -1.2180e-01],\n",
       "        [ 8.9520e-03, -8.9457e-02, -1.9016e-01,  ..., -1.5570e-01,\n",
       "          1.7070e-01,  1.5789e-01],\n",
       "        ...,\n",
       "        [ 4.9225e-02, -4.4570e-02, -9.2898e-02,  ...,  3.3420e-03,\n",
       "         -1.6494e-01,  7.3739e-02],\n",
       "        [-1.6762e-01, -1.7451e-01,  1.2406e-02,  ..., -1.7601e-01,\n",
       "         -1.1124e-01,  4.0459e-02],\n",
       "        [-1.3576e-01, -2.6659e-02, -8.2791e-02,  ...,  5.2860e-02,\n",
       "         -1.0762e-02,  1.1018e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear1 = torch.nn.Linear(784, 300, bias=True)\n",
    "linear2 = torch.nn.Linear(300, 150, bias=True)\n",
    "linear3 = torch.nn.Linear(150, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(300)\n",
    "bn2 = torch.nn.BatchNorm1d(150)\n",
    "\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = torch.nn.Sequential(linear1,  bn1, relu, dropout, \n",
    "                           linear2, bn2, relu,  dropout,\n",
    "                            linear3)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.439125389\n",
      "Epoch: 0002 cost= 0.323592544\n",
      "Epoch: 0003 cost= 0.278251022\n",
      "Epoch: 0004 cost= 0.251083732\n",
      "Epoch: 0005 cost= 0.233822539\n",
      "Epoch: 0006 cost= 0.224421576\n",
      "Epoch: 0007 cost= 0.218976259\n",
      "Epoch: 0008 cost= 0.218363106\n",
      "Epoch: 0009 cost= 0.206216887\n",
      "Epoch: 0010 cost= 0.200487524\n",
      "Epoch: 0011 cost= 0.200869381\n",
      "Epoch: 0012 cost= 0.189756408\n",
      "Epoch: 0013 cost= 0.184653997\n",
      "Epoch: 0014 cost= 0.178289533\n",
      "Epoch: 0015 cost= 0.180036753\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "train_total_batch = len(train_loader)\n",
    "model3.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0 \n",
    "    \n",
    "    for X,Y in train_loader:\n",
    "        \n",
    "        X= X.view(-1,28*28) # reshaping  X data\n",
    "        Y= Y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model3(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost +=cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9247000217437744\n",
      "Label:  9\n",
      "Prediction:  9\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model3.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    prediction = model3(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction,1) == Y_test\n",
    "    accuracy3 = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy3.item())\n",
    "    \n",
    "    # test set에서 random으로 data를 뽑아 label과 prediction 비교\n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model3(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 50 -> 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\njj06\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0859,  0.2939,  0.0792, -0.0595,  0.2942, -0.0709, -0.3069,  0.0708,\n",
       "         -0.2937, -0.2490,  0.1329, -0.2324,  0.0138,  0.1527, -0.2444, -0.1057,\n",
       "         -0.0701,  0.1633, -0.1426,  0.3095,  0.1796, -0.2392,  0.2409, -0.2607,\n",
       "         -0.1302, -0.2881, -0.3119, -0.1508,  0.3113,  0.2817, -0.1143, -0.1484,\n",
       "          0.0127, -0.0560, -0.0056, -0.0517, -0.1593,  0.2496,  0.2327,  0.1428,\n",
       "         -0.1397,  0.2655, -0.1027, -0.2741, -0.0955,  0.0878,  0.0401, -0.0931,\n",
       "         -0.1616, -0.2649],\n",
       "        [-0.1367, -0.2729,  0.2768,  0.2472,  0.0869, -0.0482,  0.0973,  0.2266,\n",
       "         -0.0029,  0.1113,  0.2314, -0.3105, -0.2541,  0.0431,  0.2128,  0.0305,\n",
       "          0.2885, -0.1136,  0.1639, -0.0057, -0.2585, -0.2884,  0.1348,  0.3016,\n",
       "         -0.3062, -0.2462, -0.2749, -0.0954, -0.2362,  0.2584,  0.2907, -0.0131,\n",
       "          0.2138,  0.2711, -0.3119,  0.2027, -0.0644,  0.2886,  0.2370, -0.0249,\n",
       "         -0.0667, -0.2123,  0.0812,  0.2988,  0.0283,  0.0288, -0.1792, -0.2517,\n",
       "          0.0392, -0.0987],\n",
       "        [-0.0702,  0.2660,  0.1354, -0.2912,  0.2173,  0.0692,  0.0640,  0.2114,\n",
       "          0.1836,  0.0674,  0.1912,  0.0309, -0.1682,  0.1113, -0.0694, -0.0345,\n",
       "         -0.0739, -0.2194, -0.1423, -0.1806,  0.0813, -0.1425, -0.0991, -0.2598,\n",
       "          0.1313,  0.0413,  0.1450,  0.0505,  0.0059,  0.2619, -0.2485, -0.1644,\n",
       "          0.2668,  0.2429,  0.0433, -0.2999, -0.0073, -0.0755,  0.3148, -0.2065,\n",
       "         -0.0223,  0.2113,  0.2884, -0.2478,  0.3107,  0.1808, -0.1968,  0.1470,\n",
       "          0.0553, -0.3085],\n",
       "        [ 0.1873, -0.2890, -0.2401,  0.2489,  0.1476, -0.0407,  0.2995, -0.0491,\n",
       "          0.1551,  0.1092, -0.2779,  0.0928, -0.1700, -0.0837, -0.1810,  0.3023,\n",
       "         -0.1084, -0.0122,  0.2092,  0.1487, -0.0866,  0.2750, -0.0886,  0.2163,\n",
       "          0.0309, -0.0374,  0.0057, -0.0451, -0.1188, -0.1805,  0.2003,  0.3022,\n",
       "          0.0540,  0.1588, -0.0216, -0.0613, -0.2082,  0.2928, -0.2996,  0.0140,\n",
       "          0.1771,  0.2129,  0.0270,  0.1308, -0.2071,  0.0152, -0.0010, -0.2595,\n",
       "         -0.2129, -0.0500],\n",
       "        [ 0.2188,  0.1951, -0.1987,  0.1700, -0.0151,  0.1027, -0.0428,  0.1675,\n",
       "         -0.2550, -0.2127,  0.0703, -0.1035,  0.0926,  0.0409, -0.1541,  0.0988,\n",
       "          0.1596,  0.0299,  0.0124, -0.0997, -0.2662, -0.0265, -0.2037,  0.0170,\n",
       "          0.1060, -0.1951, -0.2350,  0.2093,  0.2608,  0.1860,  0.0389,  0.2437,\n",
       "         -0.0665, -0.0287,  0.1102,  0.2045,  0.1282,  0.2540, -0.1285,  0.0656,\n",
       "         -0.0335,  0.2863, -0.1293, -0.0133,  0.2885, -0.1203, -0.2909,  0.2114,\n",
       "          0.2012,  0.0957],\n",
       "        [ 0.1674, -0.0801, -0.2124, -0.1087,  0.3082,  0.1705,  0.0447, -0.1218,\n",
       "          0.1500, -0.0210,  0.0641,  0.2957,  0.0745, -0.0256,  0.1169,  0.0387,\n",
       "         -0.0191, -0.2104, -0.2191, -0.2018,  0.1146, -0.1532,  0.0890, -0.0558,\n",
       "         -0.2842,  0.1901, -0.1712,  0.0758, -0.0028,  0.2610, -0.1385, -0.1361,\n",
       "          0.1300,  0.0537,  0.1067,  0.0662, -0.2494, -0.3055,  0.1501, -0.3078,\n",
       "         -0.1242,  0.0704, -0.0248,  0.1327,  0.1150, -0.3076, -0.2307,  0.0284,\n",
       "         -0.0631,  0.2258],\n",
       "        [-0.1307,  0.2017, -0.2152, -0.2123, -0.0464, -0.2079,  0.0449, -0.2080,\n",
       "         -0.0054,  0.0574,  0.0905,  0.0128, -0.0051,  0.0768,  0.1051,  0.2172,\n",
       "         -0.0718, -0.1320,  0.0010,  0.0394,  0.1280,  0.0237,  0.2358,  0.0382,\n",
       "          0.0047,  0.1290,  0.1421,  0.1243, -0.0134,  0.0562, -0.3047,  0.0186,\n",
       "          0.0800,  0.1888, -0.2546,  0.0732, -0.1256, -0.1318, -0.3135, -0.2787,\n",
       "         -0.1923,  0.2171,  0.2588,  0.2202, -0.3146, -0.1126,  0.2984, -0.0142,\n",
       "          0.2513, -0.1437],\n",
       "        [ 0.2988, -0.0784, -0.0526,  0.2494,  0.2247, -0.1506, -0.1930,  0.1228,\n",
       "         -0.2892, -0.2311,  0.3039,  0.0098,  0.2006, -0.0287,  0.2458, -0.2626,\n",
       "          0.0124, -0.2091, -0.2572,  0.1359,  0.1537,  0.0874,  0.2796,  0.2884,\n",
       "         -0.1100,  0.2644, -0.1099,  0.1514,  0.1368,  0.0616,  0.2917,  0.0435,\n",
       "          0.2159,  0.2429, -0.2619,  0.2081,  0.0318, -0.1106,  0.2054, -0.2624,\n",
       "         -0.1207, -0.2318, -0.0936, -0.1419, -0.1159, -0.1502, -0.0101, -0.1554,\n",
       "          0.1641,  0.1728],\n",
       "        [ 0.0387, -0.0159, -0.0327,  0.0158,  0.0008,  0.1197, -0.1529,  0.2625,\n",
       "          0.2397, -0.1378,  0.3012,  0.0701, -0.2300, -0.2454,  0.1263,  0.0746,\n",
       "          0.1310,  0.1631,  0.2685, -0.1740, -0.2179, -0.2761,  0.1327,  0.2443,\n",
       "         -0.0675, -0.0383, -0.0203, -0.1402, -0.2315,  0.0574,  0.0931,  0.0812,\n",
       "          0.0991,  0.2195, -0.0018,  0.2815,  0.1758, -0.0773,  0.1985, -0.1971,\n",
       "         -0.0137,  0.0250,  0.3042, -0.2320,  0.2104, -0.1852,  0.1675,  0.1735,\n",
       "          0.0788,  0.0980],\n",
       "        [ 0.2215,  0.3013,  0.2788, -0.2561, -0.1439,  0.0605,  0.0603,  0.3029,\n",
       "         -0.2277,  0.0475,  0.0612, -0.1045,  0.1365, -0.1973, -0.0294, -0.2936,\n",
       "          0.1529,  0.1705, -0.0173, -0.2653,  0.2958,  0.2283,  0.0725, -0.1007,\n",
       "          0.2146, -0.0508,  0.0405, -0.2592,  0.2156,  0.0866,  0.1491,  0.1412,\n",
       "          0.3094, -0.1251, -0.1921, -0.1288, -0.1130,  0.0202, -0.0053,  0.2410,\n",
       "         -0.0630,  0.2877,  0.0491, -0.1705, -0.0418, -0.1469,  0.1458, -0.1186,\n",
       "          0.0830,  0.1716]], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear1 = torch.nn.Linear(784, 50, bias=True)\n",
    "linear2 = torch.nn.Linear(50, 50, bias=True)\n",
    "linear3 = torch.nn.Linear(50, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(50)\n",
    "bn2 = torch.nn.BatchNorm1d(50)\n",
    "\n",
    "torch.nn.init.xavier_uniform(linear1.weight)\n",
    "torch.nn.init.xavier_uniform(linear2.weight)\n",
    "torch.nn.init.xavier_uniform(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = torch.nn.Sequential(linear1,  bn1, relu, dropout, \n",
    "                           linear2, bn2, relu,  dropout,\n",
    "                            linear3)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model4.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.601092041\n",
      "Epoch: 0002 cost= 0.442086041\n",
      "Epoch: 0003 cost= 0.405375123\n",
      "Epoch: 0004 cost= 0.387025237\n",
      "Epoch: 0005 cost= 0.379317105\n",
      "Epoch: 0006 cost= 0.364590406\n",
      "Epoch: 0007 cost= 0.364192456\n",
      "Epoch: 0008 cost= 0.354141831\n",
      "Epoch: 0009 cost= 0.345203608\n",
      "Epoch: 0010 cost= 0.342546344\n",
      "Epoch: 0011 cost= 0.338277072\n",
      "Epoch: 0012 cost= 0.331260473\n",
      "Epoch: 0013 cost= 0.333992779\n",
      "Epoch: 0014 cost= 0.328149199\n",
      "Epoch: 0015 cost= 0.325725675\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "train_total_batch = len(train_loader)\n",
    "model4.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0 \n",
    "    \n",
    "    for X,Y in train_loader:\n",
    "        \n",
    "        X= X.view(-1,28*28) # reshaping  X data\n",
    "        Y= Y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model4(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost +=cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9459999799728394\n",
      "Label:  0\n",
      "Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model4.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1,28*28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    prediction = model4(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction,1) == Y_test\n",
    "    accuracy4 = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy4.item())\n",
    "    \n",
    "    # test set에서 random으로 data를 뽑아 label과 prediction 비교\n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r+1].view(-1,28*28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r+1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model4(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9373000264167786\n",
      "0.9151999950408936\n",
      "0.9247000217437744\n",
      "0.9459999799728394\n"
     ]
    }
   ],
   "source": [
    "print(accuracy.item()) # 100,100\n",
    "print(accuracy2.item()) # 200, 100\n",
    "print(accuracy3.item()) # 300, 150\n",
    "print(accuracy4.item()) # 50, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
