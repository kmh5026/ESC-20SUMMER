{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정 (learning rate, training epochs, batch_size)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set과 test set 나누어서 불러오기\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "linear1=torch.nn.Linear(784,100)\n",
    "linear2=torch.nn.Linear(100,100)\n",
    "linear3=torch.nn.Linear(100,10)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "relu=torch.nn.ReLU()\n",
    "\n",
    "bn1=torch.nn.BatchNorm1d(100)\n",
    "bn2=torch.nn.BatchNorm1d(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1734,  0.2040,  0.0221,  0.0829,  0.2211, -0.0367,  0.1658,  0.1042,\n",
       "          0.1742, -0.0707, -0.0363,  0.1077,  0.1091,  0.1657,  0.0090,  0.0979,\n",
       "         -0.0237, -0.0873, -0.1394,  0.1187,  0.1299, -0.1470,  0.0780,  0.0291,\n",
       "          0.0050,  0.0103, -0.1480,  0.2285,  0.0913,  0.0559,  0.1850, -0.1609,\n",
       "          0.2012,  0.0269,  0.1427,  0.0512,  0.0652, -0.2246,  0.2166,  0.0095,\n",
       "         -0.0218, -0.0104, -0.1898,  0.1537, -0.0284, -0.2309,  0.2303, -0.0937,\n",
       "         -0.1007,  0.1054,  0.1868,  0.1807,  0.1922,  0.1272, -0.0015,  0.1771,\n",
       "          0.1740, -0.2284,  0.1472, -0.0109, -0.1430,  0.1879,  0.0008, -0.1404,\n",
       "          0.1173, -0.2211, -0.2128,  0.2255, -0.1946,  0.0614, -0.2046, -0.0526,\n",
       "         -0.0140, -0.1702,  0.1584, -0.0005,  0.1556,  0.1162,  0.0061, -0.2322,\n",
       "         -0.2224, -0.0422, -0.1489,  0.0529, -0.0744, -0.0817,  0.1051, -0.1647,\n",
       "         -0.0159,  0.1319, -0.2163,  0.1371,  0.0791, -0.0847,  0.0275,  0.1335,\n",
       "          0.1343, -0.0984,  0.0166,  0.1465],\n",
       "        [-0.2177,  0.1949,  0.0646, -0.0393, -0.0900, -0.0202,  0.1944,  0.0664,\n",
       "          0.1996,  0.1707, -0.0810, -0.0873, -0.1960,  0.1043, -0.2040, -0.1206,\n",
       "         -0.0828, -0.1834,  0.0520,  0.1256,  0.1364,  0.0232,  0.0904,  0.0693,\n",
       "         -0.0293, -0.0924, -0.0398,  0.1601,  0.0825, -0.1918, -0.0127,  0.0988,\n",
       "          0.0414,  0.0348, -0.1587,  0.1792,  0.1977, -0.0462, -0.2242,  0.0401,\n",
       "          0.0308, -0.1996,  0.1074,  0.0512,  0.1616,  0.1483, -0.1787,  0.0895,\n",
       "         -0.2084, -0.2202,  0.2037, -0.2234, -0.0753,  0.0904, -0.1764, -0.1746,\n",
       "          0.1820,  0.1962,  0.0242, -0.0968, -0.1543,  0.0992,  0.0816, -0.1220,\n",
       "          0.0086,  0.0608, -0.2125,  0.2335, -0.1599,  0.1329, -0.2299, -0.0271,\n",
       "         -0.2264,  0.1323,  0.1264,  0.0029,  0.1605, -0.0121,  0.0361, -0.0883,\n",
       "         -0.0004,  0.0121,  0.0624,  0.1938,  0.0315, -0.0803, -0.2276, -0.0328,\n",
       "         -0.1327,  0.1841, -0.1636, -0.0373,  0.2088,  0.1151,  0.1519, -0.0230,\n",
       "          0.1466, -0.0440,  0.0933, -0.0080],\n",
       "        [-0.0692,  0.1885,  0.2200, -0.0358, -0.1279, -0.0121,  0.2240,  0.0092,\n",
       "         -0.1998, -0.0637, -0.1662,  0.1331, -0.1336, -0.1259, -0.1065,  0.0024,\n",
       "          0.1701,  0.0911,  0.0596,  0.0796, -0.1554,  0.1115,  0.0022,  0.0904,\n",
       "          0.2003,  0.0921,  0.1963, -0.1947, -0.0639,  0.0616,  0.1848, -0.0761,\n",
       "         -0.1001,  0.0361, -0.0961, -0.0094,  0.0320,  0.2136, -0.1942,  0.0237,\n",
       "         -0.1654, -0.2288, -0.0754, -0.0335,  0.0995,  0.0092,  0.1698,  0.2058,\n",
       "         -0.0579,  0.1375,  0.1133,  0.2231,  0.2138,  0.1700, -0.2074,  0.0431,\n",
       "         -0.1158,  0.1857,  0.2198,  0.1996, -0.0769,  0.0132, -0.1805, -0.1214,\n",
       "         -0.0035, -0.1479, -0.0897, -0.0707,  0.1795,  0.2201, -0.1170, -0.1935,\n",
       "         -0.0411, -0.0166,  0.1490, -0.1688, -0.0445, -0.2142,  0.1886,  0.1554,\n",
       "         -0.1872,  0.0204, -0.1708,  0.0670,  0.1196,  0.0962,  0.1204, -0.2065,\n",
       "          0.0827, -0.1577, -0.2001, -0.0470, -0.0602, -0.0395, -0.0018, -0.0613,\n",
       "         -0.0687,  0.1199, -0.2112,  0.0300],\n",
       "        [ 0.1620,  0.2215,  0.0723,  0.2099, -0.0480, -0.0165,  0.0514, -0.0208,\n",
       "          0.0639, -0.2025,  0.0547,  0.0456, -0.1507, -0.1932, -0.1082, -0.1586,\n",
       "         -0.1067,  0.0576,  0.0812, -0.0374, -0.1073,  0.0126, -0.0896,  0.1298,\n",
       "          0.0254,  0.1204, -0.0945,  0.0989,  0.0520, -0.1734, -0.1469, -0.1989,\n",
       "         -0.1743,  0.1362,  0.2247, -0.1888,  0.0509, -0.2311, -0.2145, -0.0470,\n",
       "          0.2263, -0.1926,  0.0744, -0.1354,  0.0429, -0.1289, -0.2032,  0.0578,\n",
       "          0.0950,  0.0534,  0.0904, -0.0980, -0.1729,  0.1886,  0.1199, -0.0409,\n",
       "          0.0202,  0.1620, -0.0204,  0.0481,  0.1477, -0.1053,  0.1610, -0.2228,\n",
       "          0.1753, -0.1744,  0.1960, -0.1935, -0.2306,  0.1852, -0.0389,  0.1277,\n",
       "         -0.2201, -0.0466, -0.1130,  0.0919,  0.0622, -0.0397,  0.0031, -0.1627,\n",
       "         -0.2164, -0.2280, -0.0430,  0.1542,  0.2138,  0.2213,  0.1391,  0.0658,\n",
       "         -0.2132, -0.0686,  0.1421,  0.0364,  0.0787,  0.1295, -0.0162, -0.1011,\n",
       "         -0.0507,  0.0091,  0.1534,  0.1276],\n",
       "        [-0.1811,  0.0946, -0.2328, -0.0626, -0.1384,  0.1219,  0.1306,  0.1546,\n",
       "         -0.1532, -0.1369,  0.0638,  0.1743, -0.1512,  0.0353,  0.0369, -0.0099,\n",
       "         -0.2003, -0.1182, -0.0685, -0.1219,  0.1536, -0.0921, -0.1261,  0.1136,\n",
       "          0.2150,  0.0420, -0.2110, -0.0475,  0.0888, -0.1347,  0.1786, -0.0587,\n",
       "         -0.0549, -0.1305,  0.2088, -0.1160,  0.2261,  0.0619,  0.1049, -0.1172,\n",
       "         -0.1208, -0.1244, -0.1693, -0.0868, -0.0566,  0.0090,  0.2004, -0.1882,\n",
       "         -0.1485,  0.0441, -0.0278, -0.0912, -0.0986, -0.0060, -0.0617,  0.0548,\n",
       "         -0.0868, -0.0040, -0.0784, -0.1890, -0.1553, -0.1309, -0.1168, -0.1691,\n",
       "          0.1694, -0.0341, -0.2302,  0.0011, -0.0807,  0.1901,  0.2068, -0.2054,\n",
       "         -0.1540,  0.0511, -0.0465, -0.0529, -0.0129,  0.0786, -0.1313, -0.1544,\n",
       "         -0.0040,  0.1443, -0.2134,  0.2111,  0.0866,  0.0424, -0.0030,  0.1727,\n",
       "         -0.1881, -0.1419,  0.1049, -0.2053, -0.0490,  0.0806, -0.0907, -0.1639,\n",
       "         -0.0357,  0.0472, -0.1816, -0.0126],\n",
       "        [-0.0230, -0.0815, -0.0674,  0.1002, -0.1116,  0.1360,  0.2046,  0.1189,\n",
       "         -0.0283, -0.1293,  0.0675, -0.2201,  0.1438, -0.0910, -0.0346,  0.1662,\n",
       "          0.2025,  0.1774, -0.0543, -0.0601, -0.1027,  0.1095, -0.1558,  0.1338,\n",
       "         -0.1906,  0.2319,  0.0693, -0.1146,  0.1355, -0.0877,  0.1541,  0.0078,\n",
       "          0.1285, -0.0539,  0.1684,  0.1702,  0.0007,  0.0811, -0.1538,  0.0713,\n",
       "          0.2136,  0.0440, -0.1694, -0.1101, -0.2037,  0.1533,  0.0262, -0.1438,\n",
       "          0.1613,  0.1194, -0.0833,  0.2038,  0.1501, -0.0942,  0.1287,  0.1363,\n",
       "         -0.2219,  0.0271, -0.1478,  0.2255,  0.2299,  0.1501,  0.1226,  0.1135,\n",
       "          0.1136, -0.1784, -0.1130, -0.1060, -0.0852,  0.2331,  0.1231, -0.1957,\n",
       "          0.2017,  0.0312,  0.1943, -0.2189,  0.1917, -0.2016, -0.0399,  0.0495,\n",
       "          0.0859,  0.0771,  0.1601,  0.1132, -0.1578, -0.1916, -0.2210,  0.2275,\n",
       "          0.0841,  0.2271,  0.0649,  0.0017,  0.0125, -0.1912,  0.1960,  0.0983,\n",
       "         -0.2151,  0.0286, -0.0305,  0.2262],\n",
       "        [-0.1671, -0.0268,  0.1308, -0.1843, -0.0778,  0.0266, -0.1789,  0.0306,\n",
       "         -0.1750, -0.1674,  0.0539, -0.1902,  0.1263, -0.1848,  0.0414, -0.0388,\n",
       "         -0.0912, -0.0520,  0.1349, -0.1704, -0.2235,  0.1083,  0.0756, -0.1838,\n",
       "         -0.1913,  0.0265,  0.0848,  0.1727, -0.0932, -0.0818, -0.1672, -0.0822,\n",
       "          0.1601, -0.0377,  0.1168,  0.0717, -0.1183, -0.1514,  0.1569,  0.2238,\n",
       "         -0.1800, -0.0825, -0.2232, -0.1521, -0.1093,  0.1155, -0.0419, -0.1341,\n",
       "          0.0506, -0.1024, -0.0366, -0.2149, -0.0851, -0.1949, -0.0605, -0.2141,\n",
       "          0.2146,  0.2257, -0.2163,  0.0495, -0.1126, -0.2127,  0.0309,  0.0570,\n",
       "         -0.1482,  0.0326, -0.1731,  0.1479,  0.1763, -0.0039,  0.0843,  0.2105,\n",
       "         -0.1954, -0.1216,  0.1928, -0.1813,  0.0757, -0.1796, -0.1329, -0.0756,\n",
       "          0.0536, -0.0994,  0.0916,  0.1768,  0.2145,  0.0830,  0.0898,  0.1440,\n",
       "          0.0102, -0.1247,  0.0544, -0.1648, -0.0033,  0.0586, -0.0530,  0.1594,\n",
       "         -0.2055, -0.0162,  0.1028,  0.0404],\n",
       "        [-0.2005, -0.2136,  0.0775,  0.1769,  0.1049,  0.2206, -0.1033, -0.2222,\n",
       "          0.1463, -0.1514,  0.1187,  0.0216, -0.0696,  0.1564, -0.0005,  0.0412,\n",
       "         -0.1096, -0.0883,  0.0099, -0.0416, -0.1893, -0.1355, -0.1413,  0.0324,\n",
       "         -0.2151, -0.0232, -0.2119,  0.1734, -0.1465, -0.0011, -0.1196, -0.0827,\n",
       "         -0.0288, -0.0288, -0.0853, -0.0440,  0.1446, -0.1517,  0.0583, -0.2134,\n",
       "         -0.0287, -0.0780,  0.0646,  0.0770, -0.0731,  0.0509,  0.0478,  0.1507,\n",
       "         -0.1150, -0.0976,  0.1567,  0.0025, -0.1840,  0.0288, -0.2215,  0.2293,\n",
       "         -0.0449, -0.0981, -0.0944,  0.1530,  0.2085, -0.0377,  0.0602,  0.0997,\n",
       "          0.1033,  0.1305,  0.0351,  0.0197,  0.1895, -0.0598,  0.1550,  0.1222,\n",
       "         -0.0457, -0.1362,  0.1810,  0.1396,  0.0082,  0.1280,  0.0142,  0.0901,\n",
       "         -0.0153, -0.0504, -0.1732, -0.0594,  0.0953, -0.0540,  0.0221, -0.1063,\n",
       "          0.1463, -0.2209,  0.1030, -0.2006,  0.2022, -0.0651, -0.0280,  0.0345,\n",
       "          0.0973, -0.2276,  0.1462,  0.0155],\n",
       "        [ 0.0275,  0.2204,  0.1597,  0.1212, -0.1249, -0.0357,  0.1085, -0.0888,\n",
       "         -0.1670, -0.0810, -0.0023, -0.1949,  0.1650,  0.2256, -0.0172,  0.0849,\n",
       "          0.2171,  0.1004, -0.1721, -0.0418, -0.0053,  0.0232,  0.0513, -0.0057,\n",
       "         -0.1218,  0.0317, -0.0545,  0.0599,  0.1705,  0.2191,  0.0815, -0.1604,\n",
       "          0.1342, -0.0494, -0.0344, -0.2280,  0.0051, -0.1899,  0.0815,  0.1935,\n",
       "         -0.1793, -0.0769, -0.0761, -0.1619, -0.1671,  0.0009, -0.0038, -0.0625,\n",
       "          0.1299, -0.2021, -0.0323,  0.1086, -0.2163, -0.0641,  0.0146, -0.1333,\n",
       "         -0.0013, -0.0004,  0.0980,  0.1891, -0.1054, -0.1659, -0.2018, -0.0575,\n",
       "         -0.0319,  0.1667, -0.1060,  0.0881, -0.1408, -0.1028,  0.1774, -0.1707,\n",
       "          0.0277, -0.0190, -0.0880,  0.1611,  0.1403, -0.1327, -0.1963, -0.1782,\n",
       "          0.1648,  0.2276, -0.2161,  0.0722, -0.1397, -0.1725, -0.1885,  0.1820,\n",
       "         -0.1208, -0.1498, -0.1691, -0.0452, -0.0943,  0.0854,  0.2291,  0.1119,\n",
       "         -0.1788, -0.1663, -0.0142,  0.0638],\n",
       "        [-0.0428, -0.1101, -0.0593, -0.0744,  0.1184, -0.0261, -0.1963,  0.1420,\n",
       "          0.0860,  0.2137, -0.1716,  0.1219, -0.1377, -0.1326,  0.1602, -0.0618,\n",
       "          0.2245, -0.0432, -0.0129,  0.0217,  0.1710, -0.1066, -0.0143,  0.0596,\n",
       "          0.0526, -0.0263, -0.1245,  0.2229,  0.0119,  0.1246,  0.1222, -0.1587,\n",
       "          0.0235, -0.0131,  0.1690, -0.1169,  0.0932, -0.0357, -0.0758,  0.2014,\n",
       "          0.1720,  0.0459, -0.0690, -0.0556,  0.2217, -0.1033, -0.0663, -0.0643,\n",
       "          0.1357,  0.2169,  0.1951,  0.0084, -0.1987,  0.2307, -0.0591,  0.0478,\n",
       "         -0.2251, -0.0821, -0.0122, -0.0526, -0.0938, -0.1482,  0.1809, -0.1297,\n",
       "          0.1230, -0.0173,  0.0083,  0.0940,  0.0604, -0.1457, -0.0195,  0.2026,\n",
       "         -0.0037,  0.1492, -0.2301,  0.1536, -0.0409,  0.0989,  0.0865,  0.1425,\n",
       "          0.1833,  0.0026,  0.1301,  0.1415,  0.1896,  0.0538, -0.1609, -0.1936,\n",
       "          0.1748, -0.2271,  0.0334, -0.1772,  0.1966, -0.1886, -0.0311, -0.0324,\n",
       "          0.1241,  0.1524, -0.0278,  0.1450]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout,linear2, bn2, relu, dropout, linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.498689592\n",
      "Epoch: 0002 cost = 0.371965975\n",
      "Epoch: 0003 cost = 0.338367999\n",
      "Epoch: 0004 cost = 0.302482396\n",
      "Epoch: 0005 cost = 0.297170192\n",
      "Epoch: 0006 cost = 0.284864366\n",
      "Epoch: 0007 cost = 0.282527566\n",
      "Epoch: 0008 cost = 0.278607279\n",
      "Epoch: 0009 cost = 0.259173632\n",
      "Epoch: 0010 cost = 0.259774745\n",
      "Epoch: 0011 cost = 0.254910409\n",
      "Epoch: 0012 cost = 0.255829126\n",
      "Epoch: 0013 cost = 0.240515888\n",
      "Epoch: 0014 cost = 0.239837259\n",
      "Epoch: 0015 cost = 0.239134982\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost=0\n",
    "    \n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28)\n",
    "        Y = Y\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(X)\n",
    "        loss = criterion(prediction, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += loss / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9315000176429749\n",
      "Label:  7\n",
      "Prediction:  7\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction,1)==Y_test\n",
    "    acc = correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", acc.item())\n",
    "    \n",
    "     ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_linear1=torch.nn.Linear(784,350)\n",
    "w_linear2=torch.nn.Linear(350,150)\n",
    "w_linear3=torch.nn.Linear(150,10)\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "relu=torch.nn.ReLU()\n",
    "bn1=torch.nn.BatchNorm1d(350)\n",
    "bn2=torch.nn.BatchNorm1d(150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1081,  0.0570, -0.1428,  ..., -0.1672,  0.1690,  0.1234],\n",
       "        [-0.0508, -0.1914, -0.1333,  ..., -0.0103,  0.1287, -0.1543],\n",
       "        [-0.1072,  0.0993,  0.0144,  ..., -0.0063, -0.1570,  0.0284],\n",
       "        ...,\n",
       "        [ 0.1629,  0.1409,  0.1243,  ...,  0.1090,  0.0985, -0.0090],\n",
       "        [ 0.0698,  0.0843,  0.1059,  ...,  0.1033,  0.1689,  0.1600],\n",
       "        [ 0.0010, -0.0058, -0.1603,  ...,  0.1752,  0.0054, -0.0565]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(w_linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(w_linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(w_linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_model = torch.nn.Sequential(w_linear1, bn1, relu, dropout,w_linear2, bn2, relu, dropout, w_linear3)\n",
    "\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer=torch.optim.Adam(w_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.434796959\n",
      "Epoch: 0002 cost = 0.303037107\n",
      "Epoch: 0003 cost = 0.271271706\n",
      "Epoch: 0004 cost = 0.254768103\n",
      "Epoch: 0005 cost = 0.236294970\n",
      "Epoch: 0006 cost = 0.222446769\n",
      "Epoch: 0007 cost = 0.215036392\n",
      "Epoch: 0008 cost = 0.197678506\n",
      "Epoch: 0009 cost = 0.214336842\n",
      "Epoch: 0010 cost = 0.188481390\n",
      "Epoch: 0011 cost = 0.193039224\n",
      "Epoch: 0012 cost = 0.179785520\n",
      "Epoch: 0013 cost = 0.171679839\n",
      "Epoch: 0014 cost = 0.183403775\n",
      "Epoch: 0015 cost = 0.172819927\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "train_total_batch = len(train_loader)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    w_model.train()\n",
    "    avg_cost=0\n",
    "    \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28)\n",
    "        Y = Y\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction = w_model(X)\n",
    "        loss = criterion(prediction, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += loss / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8891000151634216\n",
      "Label:  8\n",
      "Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    w_model.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = w_model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction,1)==Y_test\n",
    "    acc = correct_prediction.float().mean()\n",
    "    print(\"Accuracy: \", acc.item())\n",
    "    \n",
    "     ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = w_model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
