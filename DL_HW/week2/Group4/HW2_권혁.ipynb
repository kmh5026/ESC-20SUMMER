{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 1-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train과 test set 나누어 MNIST data 불러오기\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True, transform=transforms.ToTensor(),\n",
    "                          download=False)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=False, transform=transforms.ToTensor(),\n",
    "                          download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader에 train, test 할당하기\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,\n",
    "                                         shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,batch_size=batch_size,\n",
    "                                         shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 쌓기\n",
    "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1479,  0.0584, -0.0677, -0.1924,  0.2141, -0.1536,  0.0003,  0.1571,\n",
       "         -0.1759, -0.1425, -0.1968,  0.0435, -0.0940,  0.1455, -0.0497,  0.1693,\n",
       "         -0.2082, -0.0038,  0.0512,  0.0077,  0.0101,  0.0898,  0.1196, -0.0272,\n",
       "          0.1343,  0.0084, -0.0544,  0.1222,  0.1573, -0.2330,  0.2058, -0.2101,\n",
       "          0.0464, -0.2219,  0.1613,  0.0013, -0.0433, -0.2094,  0.0847, -0.1192,\n",
       "          0.1494,  0.1668, -0.1581,  0.0658, -0.1719, -0.1338,  0.1115, -0.0143,\n",
       "          0.0178, -0.0565,  0.0130,  0.1343, -0.0008, -0.0731,  0.2197, -0.1627,\n",
       "          0.1970,  0.1480,  0.1809, -0.2208,  0.1775,  0.1300, -0.0036,  0.1076,\n",
       "         -0.0837, -0.0259,  0.0949, -0.1149, -0.1612, -0.0453,  0.0822, -0.0819,\n",
       "         -0.0386,  0.1041,  0.0669,  0.2001, -0.0025, -0.1266,  0.0711, -0.2116,\n",
       "         -0.1043, -0.1606,  0.1253,  0.0368,  0.0990,  0.0702, -0.2066, -0.1482,\n",
       "         -0.0578,  0.1987, -0.1100,  0.0316,  0.1025, -0.1254,  0.1480,  0.1081,\n",
       "         -0.1511, -0.1819, -0.0884, -0.0441],\n",
       "        [ 0.0115,  0.0132,  0.0889, -0.1989, -0.1662, -0.2205,  0.2129,  0.1386,\n",
       "          0.0578,  0.1133, -0.1341,  0.2176, -0.2175, -0.1745, -0.0292,  0.1043,\n",
       "          0.0069, -0.1240, -0.0545,  0.0501, -0.1717, -0.1847, -0.0833,  0.1094,\n",
       "         -0.0545, -0.0376,  0.0754,  0.2333, -0.0510,  0.1838, -0.2085,  0.0423,\n",
       "          0.2328,  0.1269,  0.0673,  0.0888, -0.2279, -0.0563, -0.0558, -0.1019,\n",
       "         -0.2152,  0.0136, -0.1207, -0.1635,  0.0653, -0.1347, -0.0448, -0.0196,\n",
       "          0.0354,  0.0741, -0.0092, -0.0785,  0.1240,  0.1726,  0.2091,  0.0650,\n",
       "          0.1384,  0.0717, -0.0870, -0.1220,  0.0976, -0.1445, -0.0164, -0.2257,\n",
       "          0.1822,  0.0306, -0.0568, -0.0032, -0.1652, -0.0802, -0.0273,  0.0100,\n",
       "         -0.0531, -0.1253, -0.1407,  0.1467, -0.1869,  0.1709,  0.2003,  0.1699,\n",
       "          0.2101, -0.0603,  0.0092, -0.2323, -0.1213, -0.1528,  0.2222, -0.0417,\n",
       "          0.1677, -0.0827,  0.1689, -0.1018,  0.1185, -0.2130, -0.0723, -0.0786,\n",
       "         -0.0421,  0.2321,  0.1440, -0.2159],\n",
       "        [-0.1284, -0.0221, -0.0522,  0.1098, -0.1691,  0.0197,  0.0074,  0.1873,\n",
       "         -0.1160,  0.2026, -0.1202, -0.1229, -0.0431,  0.0593,  0.2321, -0.0431,\n",
       "         -0.0658,  0.1717, -0.1248,  0.2091,  0.1586,  0.1852,  0.1344,  0.0938,\n",
       "         -0.2093, -0.2312,  0.1453, -0.1033, -0.2097,  0.0801,  0.1700, -0.0815,\n",
       "          0.1300, -0.0692, -0.2151, -0.0575, -0.0981,  0.1383, -0.0601, -0.1555,\n",
       "         -0.0621,  0.0109, -0.0681, -0.2041, -0.2110, -0.1298, -0.1014, -0.0040,\n",
       "         -0.1532,  0.2184, -0.0471, -0.1067, -0.1146,  0.1911, -0.0173,  0.0942,\n",
       "          0.0924,  0.0950, -0.0756,  0.0875,  0.0605, -0.1913, -0.0608,  0.1750,\n",
       "          0.1989, -0.1804, -0.2144, -0.0734,  0.0662, -0.0278,  0.0730, -0.0200,\n",
       "         -0.1402,  0.0096,  0.1688,  0.0316,  0.1412, -0.1138,  0.0863, -0.0746,\n",
       "         -0.1924,  0.0680, -0.1612, -0.2100,  0.1714, -0.2093, -0.0986, -0.0418,\n",
       "          0.0738, -0.1125,  0.0587, -0.1156, -0.0591,  0.0304, -0.0841,  0.0075,\n",
       "          0.1384, -0.0544,  0.1232,  0.2178],\n",
       "        [-0.0948, -0.1652, -0.0575,  0.0527,  0.0085, -0.0431, -0.1034, -0.2040,\n",
       "          0.1605, -0.1199,  0.1389, -0.0988, -0.1174, -0.0372,  0.1463, -0.1868,\n",
       "          0.0075,  0.2247, -0.0787, -0.1565,  0.2333,  0.0478, -0.2129,  0.2205,\n",
       "          0.1891,  0.1010, -0.1257, -0.2076,  0.0923,  0.0771,  0.1978, -0.2238,\n",
       "          0.0089,  0.0729, -0.0034,  0.2248, -0.2128, -0.1195, -0.0706, -0.2002,\n",
       "         -0.1285,  0.0683,  0.2020, -0.1788,  0.1815, -0.0432,  0.2192, -0.0298,\n",
       "          0.0314,  0.1530, -0.1911,  0.0527,  0.1425, -0.0292, -0.0452, -0.1264,\n",
       "         -0.2056, -0.2114, -0.1393,  0.1088,  0.1622, -0.1434,  0.0026,  0.1961,\n",
       "         -0.0682, -0.1672,  0.1715,  0.0830, -0.1767,  0.0359,  0.1422, -0.0213,\n",
       "         -0.1907, -0.0898, -0.1204, -0.1050, -0.0688,  0.1351,  0.1003, -0.0267,\n",
       "         -0.1511, -0.1101, -0.1649,  0.1108, -0.1914,  0.0023,  0.2013,  0.0557,\n",
       "         -0.2289,  0.1412, -0.0734, -0.0087,  0.1091,  0.1363, -0.0802,  0.0294,\n",
       "         -0.0513, -0.1779,  0.2151, -0.2137],\n",
       "        [ 0.0135,  0.0875,  0.0803, -0.1514,  0.0776,  0.0243, -0.0578, -0.0654,\n",
       "          0.0363,  0.0075,  0.1714,  0.1269,  0.1578,  0.2213,  0.1342, -0.0541,\n",
       "          0.0560,  0.1151, -0.1286, -0.0664, -0.1582,  0.0418,  0.0429,  0.1687,\n",
       "         -0.0050, -0.0599, -0.0034, -0.1260, -0.1137,  0.0403,  0.1576, -0.1761,\n",
       "          0.0370, -0.0734, -0.1545, -0.0672, -0.2286,  0.1991, -0.1886,  0.0079,\n",
       "         -0.1962,  0.1048, -0.1169, -0.1589, -0.2172,  0.0497, -0.1902,  0.2191,\n",
       "          0.0976, -0.1728,  0.0661, -0.1486, -0.2051, -0.0335,  0.1157,  0.0438,\n",
       "         -0.0193, -0.0250,  0.1075, -0.0586, -0.0023, -0.1013, -0.0487, -0.1487,\n",
       "         -0.2145,  0.1259,  0.0075,  0.2010,  0.0224,  0.1265, -0.1126,  0.2015,\n",
       "         -0.1983, -0.2322,  0.0500, -0.1427, -0.2275,  0.0254,  0.0840,  0.0870,\n",
       "         -0.0447,  0.0753, -0.0236,  0.1676,  0.0833,  0.1452, -0.0441,  0.0458,\n",
       "          0.0159, -0.1953,  0.1776,  0.0459, -0.0229, -0.0247,  0.0128,  0.2055,\n",
       "         -0.2325,  0.1547, -0.1524, -0.1808],\n",
       "        [ 0.1166, -0.1696,  0.0138, -0.0620, -0.2221, -0.0998, -0.1462,  0.0169,\n",
       "          0.0731, -0.1842, -0.1803, -0.1406,  0.0764,  0.0299,  0.1940, -0.1656,\n",
       "          0.1243,  0.0208, -0.1328, -0.0876,  0.0832, -0.0610,  0.1060, -0.0675,\n",
       "          0.0169,  0.2309,  0.0561,  0.0863,  0.1327,  0.0830,  0.0483,  0.0676,\n",
       "         -0.1098,  0.0940,  0.0832, -0.2332,  0.2040,  0.0570,  0.1253,  0.0588,\n",
       "          0.0654,  0.0886,  0.1637,  0.0607,  0.0596, -0.2019, -0.1551,  0.0641,\n",
       "          0.2189, -0.0978,  0.1846, -0.0065,  0.0445, -0.1371, -0.1136,  0.1561,\n",
       "         -0.2086, -0.1386, -0.0516, -0.0767,  0.1132, -0.1877, -0.0771,  0.1280,\n",
       "          0.2158, -0.1449, -0.0237,  0.0805, -0.0320, -0.2139, -0.1951, -0.2270,\n",
       "         -0.0122, -0.0128,  0.2215, -0.1204,  0.0715, -0.2218, -0.2332, -0.1758,\n",
       "         -0.2248,  0.1198,  0.2065,  0.1692,  0.0305, -0.0079,  0.0276,  0.0499,\n",
       "          0.1135,  0.0808,  0.1277, -0.0681, -0.1361, -0.1967, -0.0709,  0.2124,\n",
       "          0.2314,  0.2067, -0.2119, -0.1483],\n",
       "        [-0.1129,  0.1509, -0.0962, -0.0584, -0.1948,  0.0713, -0.2281, -0.1363,\n",
       "          0.1759, -0.0308,  0.1326, -0.1915, -0.0586, -0.1415, -0.0288,  0.0012,\n",
       "          0.0468, -0.0443, -0.0343, -0.0895,  0.1095, -0.0894,  0.1701, -0.1308,\n",
       "         -0.0012,  0.0337, -0.1105,  0.2303, -0.0080, -0.0770,  0.0567, -0.0627,\n",
       "         -0.1306,  0.1863,  0.0480,  0.0757, -0.1192,  0.0907,  0.0591,  0.2008,\n",
       "         -0.0314, -0.1672, -0.1930,  0.2014, -0.1914,  0.0053,  0.1843, -0.1462,\n",
       "         -0.1839,  0.0385, -0.0723, -0.0603, -0.0684,  0.0602, -0.2250,  0.1747,\n",
       "         -0.1304,  0.1289, -0.0263, -0.1750,  0.0436, -0.1482, -0.2238,  0.0490,\n",
       "          0.0864,  0.2068, -0.1684, -0.2316, -0.1759, -0.0597,  0.1142, -0.1469,\n",
       "         -0.1341,  0.2191, -0.1313, -0.2284,  0.0825, -0.1874, -0.1713,  0.2172,\n",
       "         -0.0019,  0.0927, -0.0330, -0.1709, -0.1188, -0.1255, -0.1411,  0.0875,\n",
       "         -0.0593, -0.2219,  0.1544, -0.1630, -0.0347,  0.0062,  0.2191,  0.1431,\n",
       "         -0.0110,  0.0624,  0.1630, -0.1575],\n",
       "        [-0.1448, -0.1487,  0.1372, -0.1958,  0.0339, -0.1153, -0.0409, -0.0944,\n",
       "         -0.2083, -0.2209, -0.1076,  0.0054,  0.2206,  0.0655, -0.1619, -0.2235,\n",
       "          0.2334,  0.1188,  0.1609, -0.0224, -0.1223,  0.0724, -0.2272, -0.1238,\n",
       "         -0.1670,  0.0232, -0.0868,  0.2294,  0.0160, -0.0347, -0.1644,  0.0009,\n",
       "          0.1388,  0.1741, -0.0223, -0.1006,  0.0804, -0.1301,  0.1342, -0.1648,\n",
       "          0.1766, -0.0423,  0.1135,  0.1787,  0.2158, -0.1609,  0.0570, -0.1378,\n",
       "          0.0687,  0.0364, -0.0165, -0.1947, -0.1794, -0.0638, -0.0539, -0.1309,\n",
       "          0.0089, -0.0221, -0.1616, -0.1574,  0.2019, -0.1051,  0.0922,  0.1292,\n",
       "         -0.1124,  0.1304,  0.0537, -0.0289, -0.1179, -0.2221,  0.0885, -0.0125,\n",
       "          0.1515,  0.1480, -0.1096,  0.2054,  0.2316, -0.0510, -0.0292, -0.0837,\n",
       "          0.0497,  0.0343,  0.2108,  0.0484,  0.0461,  0.1391,  0.0081,  0.2245,\n",
       "          0.1170, -0.1895,  0.0138,  0.0097,  0.0931, -0.0153,  0.0431,  0.1320,\n",
       "         -0.0201,  0.1684,  0.1441, -0.0642],\n",
       "        [-0.1133,  0.1567,  0.0954,  0.2074, -0.0313, -0.1091,  0.1929,  0.1237,\n",
       "          0.0929,  0.1018, -0.0892, -0.0407,  0.0889,  0.1938, -0.1616, -0.1581,\n",
       "         -0.1224, -0.0892,  0.1646, -0.1357,  0.2312, -0.1210,  0.0242, -0.0501,\n",
       "         -0.1521,  0.1690,  0.1361,  0.1290,  0.1476,  0.0006, -0.0682,  0.1226,\n",
       "         -0.0327,  0.0882, -0.0215,  0.1529, -0.1820, -0.0074,  0.1733,  0.0572,\n",
       "          0.2300,  0.1513,  0.0110, -0.1623,  0.0095, -0.1449,  0.0770,  0.1735,\n",
       "         -0.2125, -0.1941, -0.0093,  0.0499, -0.0451,  0.1218, -0.0401,  0.1304,\n",
       "          0.1542, -0.1449,  0.0989,  0.1928, -0.0028,  0.1552, -0.0536,  0.1790,\n",
       "          0.1955,  0.0118, -0.0965, -0.0445, -0.0178,  0.1333, -0.1428, -0.2265,\n",
       "         -0.2255, -0.1596, -0.0337,  0.1106, -0.1190, -0.0953,  0.0788, -0.2120,\n",
       "          0.1426, -0.0267,  0.2053,  0.0731, -0.2052, -0.1534,  0.2251, -0.0494,\n",
       "          0.2071,  0.1300,  0.1859, -0.1467,  0.0788,  0.2254,  0.0076,  0.1928,\n",
       "          0.0758, -0.0422, -0.0267,  0.1093],\n",
       "        [ 0.0847,  0.1045,  0.1198, -0.1453,  0.1021, -0.1013,  0.0123,  0.0505,\n",
       "         -0.1044, -0.1860, -0.0860,  0.0357,  0.1722, -0.1400, -0.1723,  0.1731,\n",
       "         -0.0684, -0.0873, -0.1405,  0.2230,  0.1685,  0.0514,  0.0752, -0.2015,\n",
       "         -0.0066,  0.0356,  0.0433, -0.0427, -0.1859, -0.1382, -0.2142, -0.0471,\n",
       "         -0.1424,  0.0564,  0.0941,  0.0168, -0.0935,  0.1675, -0.2020, -0.1221,\n",
       "          0.1926, -0.0319, -0.1242,  0.1333,  0.0226, -0.0917,  0.0451,  0.2153,\n",
       "         -0.1863, -0.0869, -0.0230,  0.0653,  0.0574, -0.0325, -0.0630,  0.0293,\n",
       "         -0.0270, -0.0015,  0.0735, -0.0363, -0.0167, -0.0366,  0.1851, -0.2238,\n",
       "          0.1666,  0.1312, -0.1528, -0.1897,  0.2298,  0.2149,  0.1527,  0.0533,\n",
       "         -0.0168, -0.2148,  0.2069, -0.0119,  0.0642,  0.2188,  0.0271, -0.2073,\n",
       "          0.1721, -0.1470, -0.1234, -0.2293,  0.2317, -0.1243, -0.1989,  0.0129,\n",
       "         -0.0884,  0.0620, -0.0190, -0.2306, -0.0902,  0.0882, -0.1491,  0.0945,\n",
       "          0.2133,  0.0467,  0.1575, -0.1537]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기\n",
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기\n",
    "# optimizer 정의하기\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.506110966\n",
      "Epoch: 0002 cost = 0.385601014\n",
      "Epoch: 0003 cost = 0.337459683\n",
      "Epoch: 0004 cost = 0.309609890\n",
      "Epoch: 0005 cost = 0.298802972\n",
      "Epoch: 0006 cost = 0.289947689\n",
      "Epoch: 0007 cost = 0.267638892\n",
      "Epoch: 0008 cost = 0.265033185\n",
      "Epoch: 0009 cost = 0.258958697\n",
      "Epoch: 0010 cost = 0.260056317\n",
      "Epoch: 0011 cost = 0.253153592\n",
      "Epoch: 0012 cost = 0.249791428\n",
      "Epoch: 0013 cost = 0.241146311\n",
      "Epoch: 0014 cost = 0.249875352\n",
      "Epoch: 0015 cost = 0.238394678\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9283000230789185\n",
      "Label:  5\n",
      "Prediction:  5\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "accuracy = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "  \n",
    "  ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 1-2) (1) hidden node 수 : 200 => 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear4 = torch.nn.Linear(784, 200, bias=True)\n",
    "linear5 = torch.nn.Linear(200, 150, bias=True)\n",
    "linear6 = torch.nn.Linear(150, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "bn3 = torch.nn.BatchNorm1d(200)\n",
    "bn4 = torch.nn.BatchNorm1d(150)\n",
    "dropout = torch.nn.Dropout(p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.9068e-01, -6.2199e-02, -1.4252e-01,  ...,  4.5238e-02,\n",
       "         -1.2357e-01, -8.0516e-02],\n",
       "        [ 6.5690e-02,  2.4245e-02,  7.0571e-02,  ...,  1.9027e-03,\n",
       "         -9.8914e-05, -9.1301e-02],\n",
       "        [-5.2039e-02, -1.6119e-01,  1.4351e-01,  ..., -1.4897e-01,\n",
       "          3.8543e-02,  5.3871e-04],\n",
       "        ...,\n",
       "        [-1.0427e-01, -1.7911e-01, -1.7444e-01,  ..., -1.5858e-01,\n",
       "          1.8313e-01,  6.3561e-02],\n",
       "        [-1.3753e-01, -5.3235e-02,  2.6823e-02,  ...,  8.2039e-02,\n",
       "          5.4786e-02,  1.9252e-01],\n",
       "        [ 9.8617e-02,  3.9332e-03, -1.0863e-01,  ...,  1.2813e-03,\n",
       "         -4.3141e-02,  1.2183e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)\n",
    "torch.nn.init.xavier_uniform_(linear6.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.nn.Sequential(linear4, bn3, relu, dropout,\n",
    "                             linear5, bn4, relu, dropout,\n",
    "                             linear6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.468592763\n",
      "Epoch: 0002 cost = 0.329932898\n",
      "Epoch: 0003 cost = 0.285081834\n",
      "Epoch: 0004 cost = 0.279221296\n",
      "Epoch: 0005 cost = 0.254149199\n",
      "Epoch: 0006 cost = 0.257239342\n",
      "Epoch: 0007 cost = 0.246597201\n",
      "Epoch: 0008 cost = 0.218884617\n",
      "Epoch: 0009 cost = 0.229808465\n",
      "Epoch: 0010 cost = 0.220741987\n",
      "Epoch: 0011 cost = 0.214828297\n",
      "Epoch: 0012 cost = 0.213137433\n",
      "Epoch: 0013 cost = 0.192335472\n",
      "Epoch: 0014 cost = 0.199395984\n",
      "Epoch: 0015 cost = 0.198151141\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것)\n",
    "model2.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model2(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.927299976348877\n",
      "Label:  1\n",
      "Prediction:  1\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "with torch.no_grad():\n",
    "    model2.eval()\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model2(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "  \n",
    "  ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    \n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model2(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q 1-2) (2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear7 = torch.nn.Linear(784, 80, bias=True)\n",
    "linear8 = torch.nn.Linear(80, 40, bias=True)\n",
    "linear9 = torch.nn.Linear(40, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "bn5 = torch.nn.BatchNorm1d(80)\n",
    "bn6 = torch.nn.BatchNorm1d(40)\n",
    "dropout = torch.nn.Dropout(p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.1439e-01, -3.4353e-01,  4.6309e-02, -3.0787e-01, -2.8631e-01,\n",
       "          3.1436e-01, -1.2091e-02,  2.4332e-01,  2.0685e-01,  2.5263e-01,\n",
       "         -1.2524e-01,  2.2020e-01,  1.3973e-01, -8.2670e-02, -2.7067e-01,\n",
       "          2.1708e-01, -2.3986e-01, -1.0857e-01, -3.6727e-02,  1.1108e-01,\n",
       "          2.7367e-01,  3.1921e-02, -6.7093e-02,  1.9017e-01,  9.5596e-02,\n",
       "         -2.9438e-01,  9.2366e-02,  1.4966e-01,  1.2890e-01,  1.4049e-01,\n",
       "          2.1794e-01, -2.9305e-04, -1.5955e-02,  3.0355e-01,  3.3802e-01,\n",
       "         -2.9909e-01, -2.7273e-01,  9.2785e-02,  2.5354e-01, -2.0231e-01],\n",
       "        [ 1.8191e-01,  1.1805e-01,  1.6448e-01,  2.8627e-01, -5.5894e-02,\n",
       "         -7.9493e-02, -3.4101e-01, -1.8534e-01, -1.2129e-02,  8.2012e-02,\n",
       "          1.4794e-02,  1.6948e-02,  1.3592e-01, -1.3372e-01, -1.7592e-02,\n",
       "         -3.3916e-01,  2.3865e-01,  3.1275e-01, -2.9194e-01, -1.1431e-01,\n",
       "          3.1404e-01,  2.0820e-01, -1.1172e-01, -3.2147e-01,  2.8090e-01,\n",
       "          1.0434e-01,  2.3922e-01, -7.4358e-02, -2.2440e-01, -1.9458e-01,\n",
       "          5.3522e-02, -2.9988e-01,  3.1303e-01, -6.4302e-02,  2.0900e-01,\n",
       "         -2.7015e-01, -3.9317e-03,  2.8579e-01,  2.8874e-01, -3.6437e-03],\n",
       "        [-3.1279e-01, -3.4311e-01, -2.2552e-01, -2.0014e-01,  1.2431e-01,\n",
       "         -3.3261e-01,  2.1583e-01, -2.5339e-01, -1.4449e-01,  2.5871e-01,\n",
       "         -5.5388e-02,  2.9189e-01,  1.3839e-01,  3.3336e-01, -2.6501e-01,\n",
       "          4.2061e-02, -2.1231e-01, -2.5890e-01,  1.7729e-01, -7.5213e-03,\n",
       "         -3.8862e-02, -4.0201e-02, -2.2188e-01, -1.0892e-03,  8.7546e-02,\n",
       "         -3.0817e-01, -2.6708e-01,  2.4201e-01, -3.2362e-01,  2.3688e-01,\n",
       "         -3.4347e-01,  2.7011e-01, -8.5072e-03, -1.8687e-01, -3.4408e-01,\n",
       "          1.8602e-01,  1.6623e-01, -2.1985e-01,  3.4286e-01,  1.1203e-01],\n",
       "        [ 2.2758e-01, -3.2085e-01, -3.0146e-02,  1.6859e-01, -2.5066e-01,\n",
       "          3.2092e-02, -3.4055e-01,  5.3539e-02, -2.1374e-01,  2.9158e-01,\n",
       "          3.2482e-02,  8.2174e-02,  2.1602e-01, -1.6646e-01,  3.3889e-01,\n",
       "         -3.3647e-01,  2.9477e-01, -3.4076e-01, -1.5604e-01, -7.0435e-03,\n",
       "         -1.9341e-01,  5.4387e-03,  1.9510e-01, -5.5284e-02, -2.6153e-01,\n",
       "         -4.5440e-02, -1.0392e-01,  3.2297e-01, -3.3941e-01, -9.0215e-02,\n",
       "          3.0868e-01, -6.8020e-02,  2.9732e-01, -2.4847e-02,  4.9801e-02,\n",
       "          2.3993e-01,  2.1995e-01,  2.3241e-01, -3.1306e-01, -8.1688e-03],\n",
       "        [ 1.6954e-01,  1.9071e-01, -1.8684e-01, -1.8032e-01, -6.0728e-02,\n",
       "          2.7980e-01, -5.8156e-02, -2.5115e-01, -3.4205e-01, -9.1751e-03,\n",
       "          2.9239e-01,  2.9814e-01, -1.2906e-01, -2.5759e-01,  1.2076e-01,\n",
       "          4.1738e-02, -2.5804e-01,  1.0290e-02,  2.3641e-02, -2.4453e-01,\n",
       "         -2.0855e-01,  2.3078e-01,  2.8834e-01, -1.5862e-01,  5.7995e-02,\n",
       "          2.7216e-01,  1.8496e-01, -4.5829e-02, -3.4352e-01, -3.0869e-01,\n",
       "          1.1555e-01, -1.5978e-01,  2.0602e-01, -2.2665e-01, -1.4918e-01,\n",
       "         -3.0763e-01,  1.6384e-01, -1.5217e-01, -1.7511e-01, -8.9502e-02],\n",
       "        [-6.2232e-02,  5.5948e-03,  3.0631e-01, -3.0970e-01, -2.5098e-01,\n",
       "          2.0986e-02,  2.5102e-01, -1.7623e-01,  1.9018e-01,  1.9220e-01,\n",
       "          2.6552e-01,  1.0252e-01, -1.0322e-01, -2.6764e-01, -3.2683e-01,\n",
       "          1.8235e-01, -1.1454e-01,  1.7371e-01,  1.9608e-01, -1.4538e-01,\n",
       "          3.1483e-01,  2.2703e-01, -3.2286e-01, -3.2588e-02, -3.1329e-01,\n",
       "          7.7474e-02, -2.0585e-02,  1.5949e-01,  1.3558e-01, -4.7656e-03,\n",
       "          1.7167e-01, -1.6924e-01,  1.7414e-02, -2.0377e-01, -3.3476e-02,\n",
       "          2.4369e-01, -3.4324e-02, -2.9635e-02,  1.6368e-01,  4.4470e-02],\n",
       "        [ 1.4993e-01, -2.5247e-01, -3.2800e-01,  1.4127e-01, -5.6701e-02,\n",
       "         -2.9903e-01, -9.0145e-02,  2.2121e-03, -1.9745e-01,  1.4450e-01,\n",
       "          7.4476e-02,  2.3028e-01,  2.4865e-01, -2.5363e-01,  2.7097e-02,\n",
       "          2.0744e-01,  2.5869e-01, -1.5707e-01, -2.2110e-01,  3.1468e-01,\n",
       "         -2.6186e-01, -1.4216e-01,  2.5850e-01,  1.0774e-01, -1.6485e-01,\n",
       "          2.5118e-02, -1.3115e-01, -2.6337e-01,  3.3402e-01, -1.9391e-01,\n",
       "          2.2031e-01, -2.5728e-01, -1.9106e-01,  2.0422e-01, -3.0609e-01,\n",
       "          2.0665e-01,  2.6360e-01,  3.4492e-01, -2.5253e-01, -3.0381e-01],\n",
       "        [ 1.9661e-01,  1.6522e-01,  6.2722e-04, -1.1094e-01, -2.5136e-01,\n",
       "          2.2713e-02,  2.0076e-01,  1.0404e-01, -1.9817e-01, -2.4697e-01,\n",
       "          3.8211e-02, -1.0672e-01,  1.4771e-01,  1.9904e-01,  6.8697e-02,\n",
       "         -7.1906e-02, -1.5579e-01, -3.7424e-02, -1.0121e-02,  2.7457e-01,\n",
       "         -2.5207e-01,  2.7064e-01,  3.3565e-01,  2.0456e-01, -3.4136e-01,\n",
       "         -2.6339e-01,  1.9085e-01, -2.5334e-02,  1.9179e-01,  5.5584e-02,\n",
       "         -3.3463e-02, -2.4059e-01, -2.3872e-01,  1.7943e-01, -1.1625e-01,\n",
       "          1.0929e-01, -1.0269e-01,  1.6321e-01,  2.7123e-01,  3.0357e-01],\n",
       "        [-7.3129e-03, -2.8672e-01, -8.1272e-02,  6.5016e-02, -1.6750e-01,\n",
       "         -3.1663e-01, -1.7539e-01,  4.2674e-02, -2.4413e-01, -3.2559e-01,\n",
       "          1.8461e-01, -8.5276e-02,  2.0686e-01, -4.2522e-02,  5.7763e-02,\n",
       "         -1.4048e-01, -2.2536e-01, -2.0155e-01, -2.8351e-01,  1.7486e-01,\n",
       "          2.5807e-01,  3.3923e-01,  3.4358e-01,  2.8721e-01, -3.2271e-01,\n",
       "          3.4269e-01, -2.6888e-01, -3.1744e-01,  3.4274e-01, -2.5528e-01,\n",
       "         -3.7634e-02,  1.6505e-01, -2.9900e-01,  3.2819e-01, -6.4363e-02,\n",
       "         -3.6797e-03, -2.6635e-01,  2.7511e-01,  2.5144e-01,  2.8594e-01],\n",
       "        [-2.0059e-01,  4.1245e-02, -1.2980e-01,  3.4060e-01,  1.1425e-01,\n",
       "         -3.2123e-01,  2.9844e-01, -1.8496e-01, -5.9460e-02,  2.6748e-01,\n",
       "         -1.5264e-01, -2.2887e-02, -1.4452e-01,  9.5587e-02, -3.1492e-01,\n",
       "          2.5428e-01,  1.9137e-01,  8.3527e-02, -2.2001e-01,  2.3557e-01,\n",
       "          2.4270e-01,  1.1561e-01,  2.3615e-01,  5.8034e-02,  9.9198e-02,\n",
       "          1.5457e-02, -3.2975e-01,  3.1346e-01,  7.1791e-02,  2.3614e-01,\n",
       "          9.3686e-02,  3.4386e-01, -1.5033e-02,  7.6717e-02,  7.5536e-02,\n",
       "         -2.6397e-01,  9.5894e-02, -4.4568e-02,  2.8165e-01,  3.2659e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear7.weight)\n",
    "torch.nn.init.xavier_uniform_(linear8.weight)\n",
    "torch.nn.init.xavier_uniform_(linear9.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = torch.nn.Sequential(linear7, bn5, relu, dropout, \n",
    "                                linear8, bn6, relu, dropout,\n",
    "                                linear9).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.529559612\n",
      "Epoch: 0002 cost = 0.382952601\n",
      "Epoch: 0003 cost = 0.342321306\n",
      "Epoch: 0004 cost = 0.319407374\n",
      "Epoch: 0005 cost = 0.310401887\n",
      "Epoch: 0006 cost = 0.300189525\n",
      "Epoch: 0007 cost = 0.300844997\n",
      "Epoch: 0008 cost = 0.289053589\n",
      "Epoch: 0009 cost = 0.278807044\n",
      "Epoch: 0010 cost = 0.281914711\n",
      "Epoch: 0011 cost = 0.266391963\n",
      "Epoch: 0012 cost = 0.263719916\n",
      "Epoch: 0013 cost = 0.270280004\n",
      "Epoch: 0014 cost = 0.259037703\n",
      "Epoch: 0015 cost = 0.259047896\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것)\n",
    "model3.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model3(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / train_total_batch\n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9455999732017517\n",
      "Label:  1\n",
      "Prediction:  1\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "with torch.no_grad():\n",
    "    model3.eval()\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model3(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "  \n",
    "  ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model3(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden node 수 증가 했을 경우 cost가 더 빠르게 감소했으나, accuracy에는 큰 차이가 없었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
