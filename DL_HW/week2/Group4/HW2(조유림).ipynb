{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1-1) 아래에 주어진 주석을 기반으로 하여 코딩을 해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train과 test set으로 나누어 MNIST data 불러오기\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor())\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:45: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.train_labels[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "#직관적으로 데이터를 불러올 때 사용되는 함수 \n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f02aeaf2b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image , label = mnist_train[0]\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "l1 = torch.nn.Linear(784,100,bias=True)\n",
    "l2 = torch.nn.Linear(100,100,bias=True)\n",
    "l3 = torch.nn.Linear(100,10,bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU() #activation function\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1894, -0.1239, -0.1013,  0.2004, -0.0154,  0.1758, -0.0501, -0.2008,\n",
       "          0.0062,  0.1097,  0.0684,  0.1347, -0.0835, -0.0482, -0.1981,  0.0104,\n",
       "          0.1410, -0.0185, -0.0041, -0.1341,  0.1585,  0.0456,  0.2028, -0.1464,\n",
       "          0.1983,  0.1811, -0.1629, -0.0149,  0.1978,  0.1461, -0.1211,  0.0390,\n",
       "         -0.1649, -0.1296,  0.1514,  0.0626, -0.1835, -0.1530, -0.1928, -0.2168,\n",
       "          0.0942,  0.2312, -0.2182,  0.0818,  0.0477,  0.2167, -0.1938,  0.0443,\n",
       "         -0.1492,  0.0459, -0.1471, -0.1495,  0.2237,  0.1723,  0.0889, -0.2027,\n",
       "          0.0707,  0.0936, -0.0731, -0.2113, -0.2113,  0.0158, -0.1726, -0.2272,\n",
       "          0.0161,  0.1554, -0.0544,  0.0032, -0.2256, -0.2018,  0.1094, -0.0353,\n",
       "          0.0479,  0.0754,  0.0026, -0.1751,  0.1005,  0.1881,  0.0698, -0.1987,\n",
       "         -0.2081,  0.1048, -0.0348, -0.0504, -0.0235,  0.1259,  0.1735, -0.1751,\n",
       "          0.0509, -0.2267,  0.0851,  0.0124, -0.0926, -0.0371, -0.0496,  0.1537,\n",
       "          0.1935, -0.0120,  0.1788,  0.0075],\n",
       "        [-0.0963,  0.1115, -0.2176, -0.1219, -0.2121,  0.2216,  0.0530, -0.0724,\n",
       "         -0.1097,  0.2195, -0.1536, -0.0356, -0.0276, -0.1621,  0.0349, -0.2172,\n",
       "         -0.1405,  0.0181, -0.0280, -0.0340,  0.1338,  0.2096,  0.2004,  0.1169,\n",
       "         -0.1267,  0.2187,  0.1006,  0.0854, -0.0023, -0.1519, -0.0619,  0.2181,\n",
       "         -0.0847,  0.1341, -0.0965, -0.2047,  0.1773,  0.0959,  0.0102,  0.0235,\n",
       "         -0.1055, -0.0202,  0.0684, -0.1209,  0.0256,  0.1761,  0.0473,  0.0213,\n",
       "         -0.0305,  0.0174, -0.2025,  0.1683, -0.1881,  0.2265,  0.0980, -0.0554,\n",
       "          0.1392, -0.0412,  0.0615, -0.1106, -0.2211, -0.0048, -0.0137,  0.0315,\n",
       "          0.1474, -0.1297,  0.0054, -0.0542, -0.0057, -0.1695, -0.0159, -0.1057,\n",
       "         -0.2277, -0.0307, -0.1036,  0.0425,  0.0783,  0.1790, -0.1915, -0.0490,\n",
       "         -0.2248,  0.0317,  0.1544,  0.0595, -0.1645,  0.1649,  0.1119,  0.1169,\n",
       "          0.1993, -0.0939, -0.0749, -0.1174, -0.1815, -0.1869, -0.2115,  0.1608,\n",
       "         -0.0545, -0.1860, -0.2306, -0.1318],\n",
       "        [-0.1713, -0.2192,  0.2042,  0.0747,  0.1621,  0.0744, -0.0149,  0.0481,\n",
       "          0.2309,  0.0744,  0.1094, -0.1097,  0.1896,  0.1953,  0.1697, -0.0662,\n",
       "         -0.2119, -0.0477,  0.1971, -0.0012,  0.0027,  0.1354, -0.1180, -0.1440,\n",
       "         -0.2168,  0.1823, -0.2208, -0.1388, -0.0120, -0.1679,  0.0527,  0.2323,\n",
       "          0.2014,  0.1378,  0.0848, -0.0955,  0.1488,  0.0731, -0.1195, -0.1142,\n",
       "         -0.0666, -0.0319, -0.1232, -0.1625,  0.0724, -0.0763,  0.0558, -0.0651,\n",
       "         -0.1631, -0.1532, -0.2102,  0.0400, -0.1339, -0.0013, -0.0979,  0.1522,\n",
       "         -0.1993, -0.1236, -0.0574, -0.2295,  0.1501,  0.0927,  0.2175,  0.1496,\n",
       "         -0.1525,  0.2312, -0.0263, -0.0405,  0.0408,  0.1424, -0.0689,  0.0673,\n",
       "          0.0041, -0.2162,  0.0347, -0.1777, -0.1813, -0.1843, -0.1990,  0.0093,\n",
       "          0.2095,  0.2117, -0.1815,  0.0394,  0.0431, -0.2312, -0.0933,  0.0383,\n",
       "          0.1382,  0.1740,  0.0028,  0.1428,  0.0527,  0.1367, -0.2302,  0.1383,\n",
       "          0.2089,  0.1704,  0.1695,  0.0735],\n",
       "        [-0.2211, -0.0860,  0.0396, -0.0945,  0.2271,  0.2004,  0.1576,  0.2318,\n",
       "          0.0029,  0.2244,  0.0057,  0.0253, -0.1801,  0.0323, -0.1853,  0.0573,\n",
       "          0.0469, -0.1323,  0.1864, -0.1901,  0.0463,  0.0721, -0.0041,  0.1748,\n",
       "          0.1497, -0.0354, -0.1627,  0.1971, -0.0991, -0.2089,  0.1278,  0.1349,\n",
       "         -0.1675, -0.1043,  0.1998, -0.1694,  0.1814, -0.1468, -0.1221,  0.0404,\n",
       "          0.0698, -0.2192, -0.1612, -0.1853, -0.1902, -0.0162,  0.0666, -0.0592,\n",
       "          0.0341, -0.0706, -0.1409,  0.1505, -0.1133,  0.2203,  0.2213, -0.0203,\n",
       "         -0.0555, -0.0466,  0.1948, -0.0644, -0.0676,  0.0712,  0.0263, -0.0147,\n",
       "         -0.1386,  0.1326,  0.1452,  0.1580, -0.1336, -0.1935, -0.0423, -0.0595,\n",
       "         -0.0536,  0.1256, -0.2003,  0.1175, -0.1365,  0.2197,  0.0800,  0.1957,\n",
       "          0.0233, -0.1731, -0.1967, -0.0897, -0.0446, -0.0463, -0.2264, -0.0592,\n",
       "          0.1037,  0.0198, -0.1226, -0.1695,  0.2320,  0.1042, -0.1095, -0.1078,\n",
       "          0.0893, -0.1002,  0.0590,  0.1949],\n",
       "        [ 0.0547,  0.1682,  0.1568, -0.0954, -0.2015, -0.1836,  0.1933, -0.0872,\n",
       "         -0.0287,  0.0485,  0.2325, -0.0164, -0.2213,  0.0690,  0.0745, -0.1704,\n",
       "         -0.0675,  0.1393, -0.1374,  0.2152, -0.0361, -0.0707,  0.0171,  0.1316,\n",
       "         -0.0363, -0.2039, -0.0134,  0.2322,  0.0730, -0.0217,  0.1007, -0.1012,\n",
       "         -0.1272, -0.0888, -0.2161,  0.1078,  0.1508,  0.1585,  0.1546, -0.2315,\n",
       "          0.1393,  0.0497,  0.2048, -0.2202,  0.1597, -0.1468,  0.1453, -0.1628,\n",
       "          0.2003, -0.1495,  0.1255, -0.1879, -0.0786, -0.1934,  0.1090,  0.1206,\n",
       "         -0.1511,  0.0504, -0.0966, -0.1881, -0.0680, -0.0610, -0.0828, -0.0924,\n",
       "         -0.1847,  0.0809,  0.1309, -0.0348, -0.1968,  0.1416, -0.0792, -0.2322,\n",
       "          0.1492, -0.0296, -0.0790,  0.1050, -0.1190, -0.0641,  0.1923, -0.2206,\n",
       "         -0.0199,  0.1255,  0.1443, -0.0747,  0.1841,  0.1215, -0.1836, -0.0904,\n",
       "         -0.0574,  0.1914, -0.1011, -0.1666, -0.2086, -0.1366,  0.1229, -0.2194,\n",
       "          0.0386,  0.1122,  0.0343,  0.0362],\n",
       "        [-0.0251, -0.2109,  0.1747, -0.0763, -0.0547, -0.1610,  0.0447, -0.0749,\n",
       "          0.1793, -0.0886,  0.2019,  0.0591,  0.0327, -0.1862,  0.2162, -0.1012,\n",
       "         -0.0869, -0.1661, -0.1275, -0.0539,  0.0228,  0.1282, -0.2305,  0.1218,\n",
       "         -0.0183, -0.1513,  0.1309,  0.0594,  0.1362,  0.1614, -0.2327, -0.2325,\n",
       "         -0.0027, -0.2297,  0.0516, -0.1153,  0.1806,  0.0423, -0.1407,  0.0113,\n",
       "         -0.2098,  0.1227,  0.0145, -0.0735,  0.1309, -0.1192, -0.1919,  0.0021,\n",
       "         -0.0815,  0.1888,  0.1556, -0.1685, -0.1378, -0.0727,  0.1169,  0.1500,\n",
       "          0.0090,  0.2201,  0.1753, -0.1064,  0.0956, -0.0751,  0.2108,  0.1434,\n",
       "          0.2078,  0.0183, -0.0535,  0.1254, -0.1342, -0.0197,  0.0153,  0.0123,\n",
       "         -0.1648,  0.0588,  0.1443, -0.0335,  0.1842,  0.0300, -0.1195, -0.2115,\n",
       "          0.0634, -0.0372, -0.1287, -0.0845, -0.0545, -0.1032, -0.0128, -0.0512,\n",
       "         -0.2313,  0.0759,  0.0263,  0.1296,  0.0598,  0.1283, -0.0421,  0.1492,\n",
       "         -0.1362, -0.2210,  0.2107, -0.0226],\n",
       "        [ 0.0883, -0.2104,  0.2079,  0.1874, -0.1877, -0.1448, -0.0009, -0.1781,\n",
       "          0.1618, -0.0958, -0.1147,  0.2041, -0.1418, -0.2052,  0.1149,  0.0547,\n",
       "          0.0938,  0.0027, -0.1350,  0.1710, -0.0603,  0.0803,  0.1148, -0.0004,\n",
       "         -0.1702,  0.1740, -0.1546,  0.0659,  0.2025,  0.0297,  0.1547, -0.0518,\n",
       "         -0.1481,  0.0761,  0.0271,  0.2172, -0.0074,  0.0399, -0.1001,  0.2019,\n",
       "         -0.0058,  0.1502, -0.0192, -0.2167,  0.0111,  0.2319,  0.2077, -0.0977,\n",
       "          0.1721,  0.1845,  0.0137,  0.1413,  0.1155, -0.1412, -0.2245,  0.2309,\n",
       "         -0.0749,  0.1501, -0.1757,  0.1216, -0.1139, -0.0637, -0.1107, -0.1924,\n",
       "          0.0571,  0.1810,  0.1789,  0.0936, -0.1482, -0.1096,  0.0892, -0.1338,\n",
       "          0.0929, -0.0997,  0.0979,  0.2115, -0.1414, -0.1329, -0.1467,  0.0460,\n",
       "          0.0414, -0.1063, -0.0339, -0.0887,  0.1955,  0.1920, -0.1888,  0.1106,\n",
       "          0.0663,  0.1445,  0.1351,  0.0612,  0.0310, -0.1151, -0.0016,  0.0626,\n",
       "         -0.1481,  0.0422,  0.0799,  0.1102],\n",
       "        [ 0.1459,  0.2123, -0.1388, -0.2282, -0.1187, -0.1791,  0.1351,  0.1032,\n",
       "          0.0375, -0.0686,  0.0955, -0.1904,  0.2007, -0.0297, -0.2250, -0.1807,\n",
       "          0.0288, -0.1081,  0.0958,  0.1160,  0.1529, -0.0678,  0.1783,  0.1160,\n",
       "         -0.1767, -0.0679,  0.0548,  0.1059,  0.1105, -0.0728, -0.1420, -0.0623,\n",
       "          0.0443, -0.1176,  0.2011, -0.0597,  0.0851,  0.2257, -0.1546, -0.1255,\n",
       "         -0.0365,  0.2112,  0.2231,  0.2280,  0.1222, -0.1218,  0.0608, -0.0225,\n",
       "         -0.1699, -0.2272,  0.1098,  0.1602,  0.0765, -0.2284, -0.2105,  0.2325,\n",
       "         -0.1841,  0.0304,  0.0805,  0.1474, -0.1381, -0.1081, -0.1290,  0.1666,\n",
       "          0.1214, -0.2123,  0.1265,  0.2002, -0.0348,  0.0824,  0.0721, -0.0428,\n",
       "          0.1502, -0.2000,  0.0679,  0.0016,  0.1824, -0.1091, -0.0751, -0.1427,\n",
       "         -0.0212,  0.1873, -0.0285,  0.2187, -0.1921, -0.1918, -0.1890,  0.0581,\n",
       "          0.1372, -0.2213, -0.0891,  0.0289, -0.2273, -0.1176, -0.0273,  0.2111,\n",
       "          0.2311,  0.1907, -0.1050, -0.1720],\n",
       "        [-0.2217, -0.1221,  0.1160,  0.0629,  0.1043, -0.0334,  0.1115, -0.2005,\n",
       "         -0.0431, -0.1320, -0.1471, -0.0939,  0.2240,  0.1450, -0.1870,  0.2238,\n",
       "         -0.1276, -0.1148,  0.2297,  0.0261, -0.1177,  0.0982,  0.0150,  0.0448,\n",
       "         -0.1667, -0.1056,  0.1686,  0.0667, -0.1243, -0.1039,  0.0338, -0.1656,\n",
       "         -0.1013,  0.1963, -0.0909, -0.2298,  0.0571, -0.0984, -0.0389, -0.0318,\n",
       "          0.1958, -0.0952, -0.0472, -0.1656,  0.1459, -0.2332, -0.2317, -0.1280,\n",
       "         -0.0671, -0.2035,  0.0855,  0.1800,  0.1380,  0.1197,  0.0346,  0.0822,\n",
       "          0.1403, -0.2304,  0.1830,  0.2134,  0.1709,  0.0016, -0.1062,  0.0172,\n",
       "          0.0362, -0.2319, -0.1991, -0.1337, -0.0765, -0.0104,  0.0345,  0.1883,\n",
       "         -0.1304,  0.1662,  0.0607,  0.2099, -0.1380,  0.1025,  0.0032, -0.1408,\n",
       "         -0.0581,  0.0461, -0.0724,  0.0374, -0.0190, -0.1383,  0.1794, -0.1258,\n",
       "          0.1708,  0.1740, -0.0822, -0.0039, -0.1692,  0.2011,  0.2135,  0.0290,\n",
       "          0.0397, -0.0471,  0.1688,  0.1621],\n",
       "        [-0.1898, -0.0661,  0.0763, -0.0501,  0.1867, -0.1263, -0.0241,  0.0030,\n",
       "          0.1424, -0.1591,  0.2005,  0.0366, -0.2243, -0.0683, -0.1108,  0.1966,\n",
       "          0.1835,  0.0136,  0.0850, -0.1743,  0.1018,  0.0646, -0.0711,  0.0679,\n",
       "          0.1775, -0.0719, -0.0908,  0.1634,  0.0225,  0.0993,  0.0736, -0.0100,\n",
       "          0.0615,  0.0827,  0.1586, -0.0080, -0.2248,  0.2306, -0.0759,  0.0979,\n",
       "          0.0099, -0.1870,  0.0489,  0.0139, -0.2264,  0.1814, -0.1996, -0.0766,\n",
       "          0.0138,  0.0099,  0.0267, -0.1047,  0.2266,  0.0195, -0.0225, -0.0627,\n",
       "         -0.2212, -0.1733,  0.1843,  0.2185,  0.0850,  0.1967,  0.2111,  0.2010,\n",
       "         -0.2189, -0.0692, -0.0374, -0.0809,  0.2211, -0.0623,  0.2070,  0.0636,\n",
       "         -0.1401, -0.0033, -0.0144,  0.0254, -0.1256, -0.1435, -0.0871,  0.0829,\n",
       "          0.1974,  0.1975, -0.0089, -0.0083, -0.1739, -0.1982,  0.1768,  0.1608,\n",
       "         -0.2333,  0.1168,  0.1339,  0.1464,  0.2265,  0.1855, -0.1741, -0.1635,\n",
       "          0.2186, -0.0383,  0.0118, -0.1086]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "torch.nn.init.xavier_uniform_(l1.weight)\n",
    "torch.nn.init.xavier_uniform_(l2.weight)\n",
    "torch.nn.init.xavier_uniform_(l3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "model = torch.nn.Sequential(l1,bn1, relu, dropout,\n",
    "                            l2,bn2, relu, dropout,\n",
    "                            l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)\n",
    "print(train_total_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.511439741\n",
      "Epoch: 0002 cost = 0.372274667\n",
      "Epoch: 0003 cost = 0.341008902\n",
      "Epoch: 0004 cost = 0.323567450\n",
      "Epoch: 0005 cost = 0.312733978\n",
      "Epoch: 0006 cost = 0.297143221\n",
      "Epoch: 0007 cost = 0.275262803\n",
      "Epoch: 0008 cost = 0.264566690\n",
      "Epoch: 0009 cost = 0.273316175\n",
      "Epoch: 0010 cost = 0.260910660\n",
      "Epoch: 0011 cost = 0.258301675\n",
      "Epoch: 0012 cost = 0.260308534\n",
      "Epoch: 0013 cost = 0.248466671\n",
      "Epoch: 0014 cost = 0.239863127\n",
      "Epoch: 0015 cost = 0.247576818\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것)\n",
    "model.train()\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for x,y in train_loader:\n",
    "        x = x.view(-1,28*28)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        cost = criterion(y_pred,y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / train_total_batch \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9025999903678894\n",
      "Label:  3\n",
      "Prediction:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "with torch.no_grad(): #메모리를 아낄 수 있음.. gradient값을 구하는 과정 생략함으로써\n",
    "    model.eval() #dropout/batchnormalization\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    y_pred = model(X_test)\n",
    "    correct_prediction = torch.argmax(y_pred,1) == Y_test \n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy : ',accuracy.item())\n",
    "    \n",
    "    r = random.randint(0,len(mnist_test)-1)\n",
    "    x_data = mnist_test.test_data[r:r+1].view(-1,28*28).float()\n",
    "    y_data = mnist_test.test_labels[r:r+1]\n",
    "    \n",
    "    print('Label: ',y_data.item())\n",
    "    y_pred = model(x_data)\n",
    "    print('Prediction: ',torch.argmax(y_pred).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 . 위에서 만든 모델에서 있던 Layer 들의 Hidden node 수를 증가 또는 감소 (ex: 200, 300, 50...) 시켰을 때, train set에서의 cost와 test set에서 Accuracy가 기존 결과와 비교하였을 때 어떻게 달라졌는지 비교해주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hidden layer를 증가하는 경우에 cost함수가 더 빠르게 감소하나, accuracy는 거의 비슷한 값을 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 증가하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = torch.nn.Linear(784,200,bias=True)\n",
    "l2 = torch.nn.Linear(200,200,bias=True)\n",
    "l3 = torch.nn.Linear(200,10,bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU() #activation function\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(200)\n",
    "bn2 = torch.nn.BatchNorm1d(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0399, -0.1278,  0.0917,  ...,  0.1126, -0.0359, -0.1236],\n",
       "        [ 0.1018, -0.0072, -0.1543,  ..., -0.0329,  0.0926,  0.0879],\n",
       "        [-0.0161,  0.1412,  0.0879,  ...,  0.1578,  0.0945,  0.1456],\n",
       "        ...,\n",
       "        [-0.0406,  0.0049, -0.1609,  ..., -0.0372, -0.1637, -0.0608],\n",
       "        [-0.1424, -0.1625, -0.0999,  ..., -0.0940, -0.1423,  0.1660],\n",
       "        [ 0.0776, -0.1507, -0.1234,  ..., -0.0895, -0.0903, -0.0076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(l1.weight)\n",
    "torch.nn.init.xavier_uniform_(l2.weight)\n",
    "torch.nn.init.xavier_uniform_(l3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.nn.Sequential(l1,bn1, relu, dropout,\n",
    "                            l2,bn2, relu, dropout,\n",
    "                            l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.471155494\n",
      "Epoch: 0002 cost = 0.337008059\n",
      "Epoch: 0003 cost = 0.303309500\n",
      "Epoch: 0004 cost = 0.266442627\n",
      "Epoch: 0005 cost = 0.271706462\n",
      "Epoch: 0006 cost = 0.246140346\n",
      "Epoch: 0007 cost = 0.241504520\n",
      "Epoch: 0008 cost = 0.228270620\n",
      "Epoch: 0009 cost = 0.223125830\n",
      "Epoch: 0010 cost = 0.209814787\n",
      "Epoch: 0011 cost = 0.211759433\n",
      "Epoch: 0012 cost = 0.206263870\n",
      "Epoch: 0013 cost = 0.204197139\n",
      "Epoch: 0014 cost = 0.199717239\n",
      "Epoch: 0015 cost = 0.191915855\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것)\n",
    "model2.train()\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for x,y in train_loader:\n",
    "        x = x.view(-1,28*28)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model2(x)\n",
    "        cost = criterion(y_pred,y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / train_total_batch \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9025999903678894\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval() #dropout/batchnormalization\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "    \n",
    "    y_pred = model(X_test)\n",
    "    correct_prediction = torch.argmax(y_pred,1) == Y_test \n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy : ',accuracy.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
