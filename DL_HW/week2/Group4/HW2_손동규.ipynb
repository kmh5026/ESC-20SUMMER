{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2_손동규.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7otFJRub4WoI",
        "colab_type": "text"
      },
      "source": [
        "Q1-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nR-AnVk36tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pylab as plt\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQz6IDRJ4b1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.1\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcz74Uqb4dkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train과 test set으로 나누어 MNIST data 불러오기\n",
        "\n",
        "train_dataset = dsets.MNIST(root='MNIST_data', train=True, download = True,\n",
        "                      transform=transforms.ToTensor())\n",
        "test_dataset = dsets.MNIST(root='MNIST_data', train=False, download = True,\n",
        "                      transform=transforms.ToTensor())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QQ_0crI4m8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
        "\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
        "                         shuffle=False, drop_last=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvUN07m47_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
        "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
        "\n",
        "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
        "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
        "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
        "\n",
        "dropout = torch.nn.Dropout(p=0.3)\n",
        "relu = torch.nn.ReLU()\n",
        "\n",
        "bn1 = torch.nn.BatchNorm1d(100)\n",
        "bn2 = torch.nn.BatchNorm1d(100)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djuRy_JF5Fp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ececf431-d8ef-4d48-fce6-806e24d06d83"
      },
      "source": [
        "# Xavier initialization을 이용하여 각 layer의 weight 초기화\n",
        "\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0126, -0.1293, -0.0315, -0.0310, -0.0951,  0.2176,  0.1688, -0.2306,\n",
              "          0.0361, -0.2157, -0.2206, -0.2074, -0.0371, -0.0545,  0.0856, -0.1273,\n",
              "          0.0408, -0.0457,  0.0361, -0.2145,  0.0731,  0.0580, -0.1159, -0.2087,\n",
              "         -0.0515,  0.1005,  0.0267, -0.0963,  0.2126, -0.0618,  0.0241,  0.1729,\n",
              "          0.1903,  0.1138,  0.1376, -0.0008, -0.1123, -0.1937,  0.1390,  0.0327,\n",
              "          0.1554,  0.1404, -0.0818, -0.2174,  0.2211,  0.0924, -0.2182,  0.1230,\n",
              "          0.2216,  0.1602,  0.1113,  0.1103,  0.0634, -0.1021, -0.1447, -0.1013,\n",
              "          0.0942,  0.0921,  0.0309, -0.1407,  0.1626, -0.2149,  0.0861,  0.1962,\n",
              "          0.0577, -0.1054, -0.1821,  0.1212,  0.1301,  0.1409,  0.2263, -0.1725,\n",
              "         -0.1907,  0.1189,  0.1414, -0.1760, -0.2238,  0.1133, -0.1035,  0.0544,\n",
              "          0.0689, -0.1950, -0.1508, -0.1563, -0.0184,  0.1570,  0.0518, -0.1999,\n",
              "          0.2039, -0.1264,  0.0629, -0.0543, -0.0214, -0.0658, -0.1383,  0.0314,\n",
              "         -0.0505, -0.0807,  0.0683, -0.1424],\n",
              "        [ 0.1614, -0.2200, -0.1726,  0.2261,  0.2089,  0.0572,  0.1203, -0.1211,\n",
              "          0.0770,  0.0150,  0.1458, -0.1449,  0.2313,  0.0717, -0.0073,  0.1830,\n",
              "          0.1566, -0.1439, -0.1556,  0.0707,  0.0201,  0.1345,  0.0560,  0.0889,\n",
              "         -0.0993,  0.1646, -0.0201, -0.0653, -0.0470,  0.0630,  0.2062,  0.2283,\n",
              "         -0.0997, -0.0787,  0.1421,  0.0770, -0.2214,  0.0917, -0.2161, -0.2261,\n",
              "          0.0964, -0.1882, -0.0388, -0.1940,  0.2310,  0.0755,  0.1744, -0.0025,\n",
              "          0.0196, -0.0808, -0.0017, -0.1773,  0.1547, -0.0320, -0.1675,  0.1037,\n",
              "          0.1950, -0.2138, -0.0507, -0.1780, -0.0620,  0.0064,  0.1945,  0.0104,\n",
              "         -0.0698,  0.1610,  0.1701, -0.0733,  0.0215,  0.1521, -0.1862, -0.1317,\n",
              "         -0.0260, -0.0684,  0.1980, -0.0075, -0.0485, -0.0657,  0.0283,  0.0144,\n",
              "         -0.1700,  0.0919, -0.0526, -0.2262,  0.2315,  0.1942,  0.2008,  0.0234,\n",
              "         -0.0699, -0.1859, -0.0719,  0.0717,  0.0103, -0.2182,  0.2316,  0.1469,\n",
              "         -0.1371,  0.0885, -0.0480, -0.1287],\n",
              "        [-0.1678, -0.2249,  0.1141, -0.1911, -0.1573, -0.0093,  0.2206,  0.0677,\n",
              "         -0.1549, -0.1197,  0.0533, -0.0140, -0.1926,  0.2175, -0.0351, -0.0701,\n",
              "         -0.1952, -0.1982, -0.2226, -0.0527, -0.1390, -0.1144,  0.0698, -0.1514,\n",
              "         -0.1408,  0.1939,  0.2296, -0.1110, -0.0555,  0.1989, -0.2029,  0.1836,\n",
              "         -0.1637, -0.0242,  0.0643, -0.1820, -0.2244,  0.1377, -0.1323, -0.1042,\n",
              "         -0.1482,  0.1566,  0.0048, -0.0249, -0.0689,  0.1524,  0.0961,  0.1831,\n",
              "         -0.0906, -0.1548,  0.0997,  0.0499, -0.1900, -0.0214,  0.1480,  0.1647,\n",
              "          0.1327,  0.0413, -0.1169, -0.0305,  0.1991, -0.2213,  0.1843,  0.0237,\n",
              "         -0.0604, -0.1955, -0.2082,  0.1254, -0.1485, -0.1347,  0.1690, -0.0143,\n",
              "         -0.0062, -0.0927, -0.1936, -0.0368,  0.2045, -0.1218, -0.1398, -0.1035,\n",
              "          0.1362,  0.0606, -0.2304, -0.1181, -0.0008, -0.0749,  0.1851, -0.0657,\n",
              "          0.0002,  0.0169,  0.1775, -0.2288,  0.1797, -0.1807, -0.0887, -0.1350,\n",
              "          0.1427, -0.0141,  0.2239,  0.1935],\n",
              "        [ 0.1942,  0.0779,  0.0709,  0.0009,  0.1369,  0.1997, -0.1472, -0.0127,\n",
              "          0.1681,  0.1455,  0.0293, -0.0486, -0.1026,  0.2010, -0.2314,  0.1683,\n",
              "          0.1995,  0.0346,  0.0741, -0.2303,  0.1748, -0.0480, -0.1129,  0.1677,\n",
              "         -0.1127, -0.1822, -0.1263, -0.0918,  0.0987,  0.0058,  0.1091,  0.1164,\n",
              "          0.2152,  0.1039, -0.1553,  0.0809,  0.1274,  0.1494,  0.0547, -0.0791,\n",
              "          0.0942,  0.0494,  0.1200, -0.1654, -0.1830, -0.1805,  0.2245, -0.1010,\n",
              "         -0.0405, -0.0087, -0.0262, -0.2073,  0.1028, -0.0814, -0.1696,  0.1817,\n",
              "         -0.1412, -0.0219,  0.0446, -0.2055, -0.1882, -0.0199, -0.0126, -0.1503,\n",
              "          0.2084, -0.2288,  0.1933,  0.2231,  0.0858,  0.1468,  0.1446,  0.1356,\n",
              "         -0.0182, -0.0729, -0.0256, -0.0543, -0.0129, -0.0241,  0.0997,  0.0517,\n",
              "          0.2149, -0.2068, -0.0990, -0.1752, -0.1813,  0.1484,  0.0635,  0.1718,\n",
              "          0.1937,  0.0686, -0.1789,  0.0947, -0.1543,  0.0558, -0.1489,  0.0616,\n",
              "         -0.0636,  0.0020, -0.1994,  0.0037],\n",
              "        [-0.2186, -0.1579, -0.1964, -0.0748, -0.2210,  0.0119, -0.1150, -0.2119,\n",
              "         -0.1037,  0.0307, -0.2217,  0.1729,  0.0509,  0.0948, -0.2049,  0.0462,\n",
              "         -0.2163, -0.0025,  0.0758,  0.0888,  0.0840,  0.1252,  0.1067,  0.0338,\n",
              "          0.0017, -0.2235, -0.0568,  0.1304, -0.2133,  0.1105, -0.1974,  0.0530,\n",
              "         -0.1684, -0.1873,  0.1437, -0.1316,  0.2037, -0.2238,  0.0701,  0.0726,\n",
              "         -0.1184, -0.0093,  0.1142, -0.1107,  0.0435,  0.2048, -0.1075,  0.0431,\n",
              "         -0.0268,  0.0167,  0.1209, -0.1382,  0.1023, -0.1602, -0.1703,  0.1548,\n",
              "         -0.1348,  0.0570,  0.0401,  0.1977,  0.2320,  0.0439, -0.0428, -0.1353,\n",
              "          0.0334,  0.0499,  0.1488, -0.0562,  0.2301, -0.1328,  0.1838, -0.1994,\n",
              "         -0.2118,  0.1734, -0.2168, -0.1384,  0.2077, -0.1408, -0.0527, -0.0336,\n",
              "         -0.0929, -0.2036, -0.1524, -0.2317,  0.0866, -0.0520, -0.1948,  0.1970,\n",
              "         -0.1714, -0.0825,  0.1860,  0.0498, -0.2059, -0.1010, -0.1207, -0.0454,\n",
              "          0.0218, -0.2023, -0.1280,  0.2057],\n",
              "        [-0.0652, -0.1167,  0.2325, -0.2101,  0.2082,  0.0720,  0.1576,  0.0137,\n",
              "          0.0244,  0.0272,  0.0865,  0.1138,  0.1503,  0.1821, -0.1284,  0.0796,\n",
              "          0.0477, -0.1770,  0.2049,  0.1857,  0.2088,  0.0780,  0.0210, -0.0409,\n",
              "         -0.0529, -0.1252,  0.2152, -0.0511,  0.0401, -0.0406,  0.1733,  0.0901,\n",
              "         -0.1577, -0.1191,  0.2120, -0.1332, -0.0447,  0.1444, -0.1479, -0.1427,\n",
              "          0.0743, -0.1441, -0.2145, -0.1713, -0.1031, -0.1367, -0.1869, -0.0800,\n",
              "         -0.0667,  0.2066, -0.2212,  0.1421, -0.1768,  0.2009, -0.1331, -0.1557,\n",
              "         -0.1466,  0.1381, -0.0963,  0.1382,  0.0677,  0.0829, -0.1589, -0.0844,\n",
              "          0.1119, -0.1706, -0.1384, -0.1769, -0.1405,  0.0569,  0.1985,  0.1804,\n",
              "          0.0352, -0.1724, -0.0901, -0.0596,  0.1878,  0.1295, -0.1685,  0.2203,\n",
              "          0.0154,  0.0624,  0.1115,  0.2284,  0.1794,  0.0135, -0.0509,  0.0229,\n",
              "         -0.0680, -0.0597,  0.0347, -0.1261,  0.1952, -0.1921, -0.1744,  0.0364,\n",
              "          0.1462, -0.2084,  0.1916, -0.0092],\n",
              "        [-0.1809,  0.0258,  0.1854, -0.1756,  0.1193,  0.1104,  0.1595, -0.1269,\n",
              "          0.0150,  0.2246,  0.0031, -0.1805,  0.0839,  0.1650, -0.0325, -0.2092,\n",
              "          0.2145, -0.1883,  0.1165,  0.2196, -0.1682,  0.1324, -0.0684, -0.1890,\n",
              "          0.1287, -0.0168,  0.1094,  0.0975, -0.1920,  0.0941, -0.0527, -0.1073,\n",
              "          0.0525,  0.1983, -0.0261,  0.1334,  0.1393,  0.0834, -0.2123, -0.1517,\n",
              "          0.2005, -0.2091, -0.1164,  0.2209, -0.0537,  0.1702, -0.0563,  0.2116,\n",
              "          0.0521, -0.0705,  0.0045,  0.1145, -0.0286, -0.0165,  0.0990,  0.1318,\n",
              "         -0.1993, -0.1999,  0.1115,  0.0528, -0.0191,  0.0496, -0.2129, -0.2276,\n",
              "          0.1812,  0.1227,  0.0928, -0.0960,  0.2043,  0.0989,  0.0493, -0.0905,\n",
              "         -0.0636, -0.0302,  0.2102,  0.0634,  0.1510,  0.0470, -0.1547, -0.2177,\n",
              "         -0.1541,  0.0372,  0.0905,  0.2306, -0.0508, -0.1039, -0.1491,  0.1802,\n",
              "         -0.1314,  0.2043, -0.1359,  0.2276,  0.0177, -0.1489, -0.0337,  0.1604,\n",
              "          0.2177,  0.0170,  0.1196,  0.1210],\n",
              "        [ 0.0855,  0.0152, -0.1201, -0.0019, -0.2227,  0.1284,  0.1782,  0.1675,\n",
              "         -0.1878,  0.0966,  0.0292, -0.1182, -0.2297, -0.0400, -0.2205, -0.0731,\n",
              "         -0.0565,  0.2240, -0.2080, -0.0704,  0.0244,  0.1847, -0.0598,  0.1332,\n",
              "         -0.1927,  0.1070, -0.1103, -0.0256,  0.0671,  0.1091,  0.0606,  0.1605,\n",
              "         -0.1776, -0.2192, -0.0290, -0.1602,  0.1738,  0.0243,  0.1469,  0.0663,\n",
              "          0.2155,  0.1161, -0.1888, -0.1245,  0.1887, -0.2063, -0.1704, -0.1602,\n",
              "         -0.1777,  0.1438,  0.0859,  0.1773, -0.1334, -0.1168,  0.1574, -0.1051,\n",
              "          0.0938,  0.0470,  0.2291,  0.0633,  0.1873,  0.1689,  0.0804,  0.0366,\n",
              "          0.1596, -0.2333, -0.0691, -0.0830,  0.0728, -0.0584, -0.1584,  0.1210,\n",
              "         -0.2148,  0.1979,  0.1205, -0.2023, -0.1545, -0.0741, -0.1623,  0.1530,\n",
              "         -0.1464, -0.1475, -0.1200,  0.0070, -0.1623,  0.2192,  0.2222,  0.1517,\n",
              "         -0.1906, -0.0950, -0.1014,  0.1210, -0.2280,  0.1761, -0.1524, -0.0619,\n",
              "          0.1987, -0.1712, -0.2138, -0.1471],\n",
              "        [-0.0921,  0.0979,  0.0647,  0.2155, -0.1270,  0.1609,  0.0083, -0.1296,\n",
              "          0.1221,  0.0351, -0.1737,  0.1867, -0.1306,  0.0786,  0.0900, -0.0959,\n",
              "         -0.0796, -0.0611, -0.1102,  0.0580,  0.0873,  0.0730,  0.1238, -0.1617,\n",
              "          0.0138, -0.2015,  0.1648, -0.0079, -0.0946, -0.0582, -0.1102,  0.1064,\n",
              "          0.1762,  0.1290,  0.1750,  0.1818, -0.0010,  0.1824,  0.0261,  0.2049,\n",
              "          0.1314,  0.0238,  0.0017, -0.0321,  0.2212, -0.0775,  0.1240,  0.1459,\n",
              "          0.0394,  0.0289,  0.1197, -0.1157,  0.1372,  0.1595,  0.1112, -0.0041,\n",
              "          0.1296,  0.2064,  0.1782,  0.1967, -0.0502, -0.1165,  0.0791,  0.0742,\n",
              "         -0.1713, -0.0928,  0.2020,  0.0612, -0.1513,  0.0723,  0.1608,  0.1958,\n",
              "         -0.2149, -0.1019,  0.2334, -0.0472, -0.0652, -0.1125,  0.1072,  0.2077,\n",
              "         -0.2258, -0.0344, -0.0654, -0.2249,  0.0846,  0.0128,  0.1650, -0.0607,\n",
              "          0.0374, -0.0629,  0.0210, -0.0221,  0.2241,  0.2270,  0.0265, -0.0990,\n",
              "         -0.2218,  0.2262,  0.0820,  0.1141],\n",
              "        [ 0.1760,  0.0880, -0.0509,  0.2205,  0.2057,  0.1124, -0.1990, -0.1679,\n",
              "          0.1668, -0.0980,  0.0450, -0.0062, -0.0284,  0.0112,  0.1889,  0.1966,\n",
              "          0.0239, -0.1915, -0.0858, -0.1333, -0.0746, -0.1195, -0.0148,  0.1698,\n",
              "          0.1677, -0.0156, -0.2284,  0.1031, -0.1337, -0.1725,  0.1821,  0.0104,\n",
              "         -0.0028,  0.1013, -0.1225, -0.1475,  0.1092, -0.0563, -0.1030,  0.1504,\n",
              "          0.2104, -0.0278, -0.2300,  0.2295,  0.0531,  0.0452, -0.0562, -0.1770,\n",
              "          0.1985, -0.0791, -0.2041,  0.0471,  0.1975, -0.1062,  0.2226,  0.0080,\n",
              "         -0.0547, -0.0362,  0.0592,  0.0942, -0.1620,  0.1217, -0.1511,  0.0399,\n",
              "         -0.0426, -0.0325, -0.1673, -0.0588,  0.0298,  0.2119,  0.1561,  0.0772,\n",
              "          0.0548,  0.1868,  0.0020, -0.2206, -0.0660,  0.1302, -0.1314,  0.0872,\n",
              "          0.2188, -0.0525,  0.1476,  0.1268, -0.0174, -0.0151,  0.0509, -0.0517,\n",
              "         -0.0717,  0.0876, -0.1006,  0.1375,  0.1943,  0.0839,  0.0892,  0.2289,\n",
              "         -0.2174, -0.0820,  0.0620,  0.0049]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKQspY4Z5O54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = torch.nn.Sequential(linear1, bn1, relu, dropout, \n",
        "                            linear2, bn2, relu, dropout, \n",
        "                            linear3).to(device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9xhHV3V5bu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9HouFh15fVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-KlcKyC5q2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cost 계산을 위한 변수 설정\n",
        "\n",
        "train_total_batch = len(train_loader)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb7Ghgyj9oLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "cc0338bb-4c80-4258-d537-a61b5f2d6fa0"
      },
      "source": [
        "# Training epoch (cost 값 초기설정(0으로)과 model의 train 설정 꼭 할 것)\n",
        "model.train()\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    \n",
        "    # Train dataset을 불러오고 (X, Y 불러오기), back propagation 과 optimizer를 사용하여 loss 최적화\n",
        "    for X, Y in train_loader:\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / train_total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 0.508053780\n",
            "Epoch: 0002 cost = 0.375132650\n",
            "Epoch: 0003 cost = 0.332811415\n",
            "Epoch: 0004 cost = 0.313179821\n",
            "Epoch: 0005 cost = 0.298753560\n",
            "Epoch: 0006 cost = 0.290933579\n",
            "Epoch: 0007 cost = 0.280284733\n",
            "Epoch: 0008 cost = 0.274961054\n",
            "Epoch: 0009 cost = 0.270615608\n",
            "Epoch: 0010 cost = 0.259013474\n",
            "Epoch: 0011 cost = 0.260724068\n",
            "Epoch: 0012 cost = 0.249167472\n",
            "Epoch: 0013 cost = 0.247019738\n",
            "Epoch: 0014 cost = 0.247677296\n",
            "Epoch: 0015 cost = 0.240713581\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5W53iAs-C_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "d5dfd6e1-f616-47e9-8917-0b986a94fa0a"
      },
      "source": [
        "# Test set으로 모델의 정확도를 검증하는 코드(model의 evaluation mode 설정 꼭 할 것)\n",
        "# X_test 불러올 때 view 를 사용하여 차원 변환할 것/ Y_test를 불러올 때 labels 사용\n",
        "# Accuracy의 초기값 설정(0으로) 꼭 할 것\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    X_test = test_dataset.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = test_dataset.test_labels.to(device)\n",
        "    \n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드\n",
        "    r = random.randint(0, len(test_dataset) - 1)\n",
        "    X_single_data = test_dataset.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = test_dataset.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8834999799728394\n",
            "Label:  2\n",
            "Prediction:  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzHLk8krAn6z",
        "colab_type": "text"
      },
      "source": [
        "Q1-2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG6hPLxL_9vZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "ca1d94ce-d7b2-476c-a089-2a1f494312d5"
      },
      "source": [
        "linear1 = torch.nn.Linear(784, 200, bias=True)\n",
        "linear2 = torch.nn.Linear(200, 150, bias=True)\n",
        "linear3 = torch.nn.Linear(150, 10, bias=True)\n",
        "\n",
        "dropout = torch.nn.Dropout(p=0.3)\n",
        "relu = torch.nn.ReLU()\n",
        "\n",
        "bn1 = torch.nn.BatchNorm1d(200)\n",
        "bn2 = torch.nn.BatchNorm1d(150)\n",
        "\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)\n",
        "\n",
        "model = torch.nn.Sequential(linear1, bn1, relu, dropout, \n",
        "                            linear2, bn2, relu, dropout, \n",
        "                            linear3).to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_total_batch = len(train_loader)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    \n",
        "    for X, Y in train_loader:\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / train_total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    X_test = test_dataset.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = test_dataset.test_labels.to(device)\n",
        "    \n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    r = random.randint(0, len(test_dataset) - 1)\n",
        "    X_single_data = test_dataset.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = test_dataset.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 0.468723029\n",
            "Epoch: 0002 cost = 0.326041341\n",
            "Epoch: 0003 cost = 0.291418254\n",
            "Epoch: 0004 cost = 0.269737363\n",
            "Epoch: 0005 cost = 0.264427006\n",
            "Epoch: 0006 cost = 0.234071359\n",
            "Epoch: 0007 cost = 0.233897805\n",
            "Epoch: 0008 cost = 0.234648705\n",
            "Epoch: 0009 cost = 0.225692794\n",
            "Epoch: 0010 cost = 0.220184028\n",
            "Epoch: 0011 cost = 0.218313485\n",
            "Epoch: 0012 cost = 0.207565337\n",
            "Epoch: 0013 cost = 0.207952231\n",
            "Epoch: 0014 cost = 0.210073262\n",
            "Epoch: 0015 cost = 0.191415802\n",
            "Learning finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8849999904632568\n",
            "Label:  6\n",
            "Prediction:  6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXhgoMEnAyfg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "4dbc2612-a437-400a-b4ce-6a0781619ddc"
      },
      "source": [
        "linear1 = torch.nn.Linear(784, 75, bias=True)\n",
        "linear2 = torch.nn.Linear(75, 50, bias=True)\n",
        "linear3 = torch.nn.Linear(50, 10, bias=True)\n",
        "\n",
        "dropout = torch.nn.Dropout(p=0.3)\n",
        "relu = torch.nn.ReLU()\n",
        "\n",
        "bn1 = torch.nn.BatchNorm1d(75)\n",
        "bn2 = torch.nn.BatchNorm1d(50)\n",
        "\n",
        "torch.nn.init.xavier_uniform_(linear1.weight)\n",
        "torch.nn.init.xavier_uniform_(linear2.weight)\n",
        "torch.nn.init.xavier_uniform_(linear3.weight)\n",
        "\n",
        "model = torch.nn.Sequential(linear1, bn1, relu, dropout, \n",
        "                            linear2, bn2, relu, dropout, \n",
        "                            linear3).to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_total_batch = len(train_loader)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    \n",
        "    for X, Y in train_loader:\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / train_total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    X_test = test_dataset.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = test_dataset.test_labels.to(device)\n",
        "    \n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    r = random.randint(0, len(test_dataset) - 1)\n",
        "    X_single_data = test_dataset.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = test_dataset.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 0.540303290\n",
            "Epoch: 0002 cost = 0.404820502\n",
            "Epoch: 0003 cost = 0.365351439\n",
            "Epoch: 0004 cost = 0.338542670\n",
            "Epoch: 0005 cost = 0.327690154\n",
            "Epoch: 0006 cost = 0.325120628\n",
            "Epoch: 0007 cost = 0.309963375\n",
            "Epoch: 0008 cost = 0.302655667\n",
            "Epoch: 0009 cost = 0.290860742\n",
            "Epoch: 0010 cost = 0.298382550\n",
            "Epoch: 0011 cost = 0.282337159\n",
            "Epoch: 0012 cost = 0.290740967\n",
            "Epoch: 0013 cost = 0.282151937\n",
            "Epoch: 0014 cost = 0.274823010\n",
            "Epoch: 0015 cost = 0.271114439\n",
            "Learning finished\n",
            "Accuracy: 0.913100004196167\n",
            "Label:  9\n",
            "Prediction:  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}