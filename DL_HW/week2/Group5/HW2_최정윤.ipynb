{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"assignment1.ipynb의 사본","provenance":[{"file_id":"1CoKbmNTW6uMU9iCQgwvx0AK_Pft9-au7","timestamp":1596695010031}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Z3pLNGpdh61W","colab_type":"text"},"source":["## Q1)"]},{"cell_type":"code","metadata":{"id":"imJ0Nz_nh61c","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596700004487,"user_tz":-540,"elapsed":966,"user":{"displayName":"Jungyun Choi","photoUrl":"","userId":"11532088005133064485"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","import matplotlib.pylab as plt\n","import random"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"JplSyoJUcDtn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596700009066,"user_tz":-540,"elapsed":5532,"user":{"displayName":"Jungyun Choi","photoUrl":"","userId":"11532088005133064485"}}},"source":["# 파라미터 설정 (learning rate, training epochs, batch_size)\n","device = torch.device('cuda')\n","learning_rate = 0.1\n","training_epochs = 15\n","batch_size = 100\n","\n","\n","#train과 test set으로 나누어 MNIST data 불러오기\n","\n","train_dataset= dsets.MNIST(root='./data',\n","                           train=True,\n","                           transform=transforms.ToTensor(),\n","                           download=True)\n","test_dataset = dsets.MNIST(root ='./data',  \n","                           train = False,  \n","                           transform = transforms.ToTensor())\n","\n","\n","#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n","                                           batch_size = batch_size, \n","                                           shuffle = True,\n","                                           drop_last=True) \n","  \n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n","                                          batch_size = batch_size, \n","                                          shuffle = False,\n","                                          drop_last=True) \n","\n","\n","# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n","# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n","\n","linear1 = torch.nn.Linear(784, 100, bias=True)\n","linear2 = torch.nn.Linear(100, 100, bias=True)\n","linear3 = torch.nn.Linear(100, 10, bias=True)\n","relu = torch.nn.ReLU()\n","dropout = torch.nn.Dropout(p=0.03)\n","bn1 = torch.nn.BatchNorm1d(100)\n","bn2 = torch.nn.BatchNorm1d(100)\n","\n","#xavier initialization을 이용하여 각 layer의 weight 초기화\n","\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)\n","\n","\n","\n","# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n","\n","model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n","                            linear2, bn2,  relu, dropout,\n","                            linear3).to(device)\n","\n","\n","\n","# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","\n","\n","\n","\n","#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","\n","\n","#cost 계산을 위한 변수 설정\n","\n","train_total_batch = len(train_loader)\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtVN0egbjFAL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"status":"ok","timestamp":1596700085861,"user_tz":-540,"elapsed":82321,"user":{"displayName":"Jungyun Choi","photoUrl":"","userId":"11532088005133064485"}},"outputId":"a42cef9f-30e5-4bef-e70c-8f1fe06717ef"},"source":["#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n","model.train()\n","for epoch in range(training_epochs):\n","    \n","    avg_cost = 0\n","\n","    \n","#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n","\n","    for X, Y in train_loader:\n","        # reshape input image into [batch_size by 784]\n","        # label is not one-hot encoded\n","        X = X.view(-1, 28 * 28).to(device)\n","        Y = Y.to(device)\n","\n","        optimizer.zero_grad()\n","        hypothesis = model(X)\n","        bn_loss = criterion(hypothesis, Y)\n","        bn_loss.backward()\n","        optimizer.step()\n","\n","        avg_cost += bn_loss / train_total_batch\n","\n","    \n","      \n","        \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Epoch: 0001 cost = 0.300638944\n","Epoch: 0002 cost = 0.195512712\n","Epoch: 0003 cost = 0.160685763\n","Epoch: 0004 cost = 0.148514271\n","Epoch: 0005 cost = 0.139201418\n","Epoch: 0006 cost = 0.123943336\n","Epoch: 0007 cost = 0.133087322\n","Epoch: 0008 cost = 0.118435383\n","Epoch: 0009 cost = 0.104033753\n","Epoch: 0010 cost = 0.097112417\n","Epoch: 0011 cost = 0.104472876\n","Epoch: 0012 cost = 0.099545717\n","Epoch: 0013 cost = 0.098316140\n","Epoch: 0014 cost = 0.088534087\n","Epoch: 0015 cost = 0.087023184\n","Learning finished\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AvJTQ_wTjL1M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"status":"ok","timestamp":1596700085864,"user_tz":-540,"elapsed":82311,"user":{"displayName":"Jungyun Choi","photoUrl":"","userId":"11532088005133064485"}},"outputId":"6b8abe8e-a78b-43a8-9471-a3f1ba2ce21e"},"source":["#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n","#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n","#accuracy의 초기 값 설정(0으로) 꼭 할 것\n","\n","with torch.no_grad():\n","    model.eval()\n","    accuracy=0\n","\n","    X_test = test_dataset.test_data.view(-1, 28 * 28).float().to(device)\n","    Y_test = test_dataset.test_labels.to(device)\n","\n","    prediction = model(X_test)\n","    correct_prediction = torch.argmax(prediction, 1) == Y_test\n","    accuracy = correct_prediction.float().mean()\n","    print('Accuracy:', accuracy.item())\n","\n","    # Get one and predict\n","    r = random.randint(0, len(test_dataset) - 1)\n","    X_single_data = test_dataset.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n","    Y_single_data = test_dataset.test_labels[r:r + 1].to(device)\n","\n","    print('Label: ', Y_single_data.item())\n","    single_prediction = model(X_single_data)\n","    print('Prediction: ', torch.argmax(single_prediction, 1).item())"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Accuracy: 0.8269999623298645\n","Label:  2\n","Prediction:  2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n","  warnings.warn(\"test_data has been renamed data\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n","  warnings.warn(\"test_labels has been renamed targets\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"_AGAG5BHnu9S","colab_type":"text"},"source":["## Q1-2)"]},{"cell_type":"markdown","metadata":{"id":"qOrDxIskt2z2","colab_type":"text"},"source":["### i) Hidden layer node 수 증가"]},{"cell_type":"code","metadata":{"id":"LgDFeX8pmpy6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"status":"ok","timestamp":1596700160108,"user_tz":-540,"elapsed":156539,"user":{"displayName":"Jungyun Choi","photoUrl":"","userId":"11532088005133064485"}},"outputId":"d0b3c9da-a048-4e13-9e68-25a2fa75fbe1"},"source":["# 파라미터 설정 (learning rate, training epochs, batch_size)\n","device = torch.device('cuda:0')\n","learning_rate = 0.1\n","training_epochs = 15\n","batch_size = 100\n","\n","\n","#train과 test set으로 나누어 MNIST data 불러오기\n","\n","train_dataset= dsets.MNIST(root='./data',\n","                           train=True,\n","                           transform=transforms.ToTensor(),\n","                           download=True)\n","test_dataset = dsets.MNIST(root ='./data',  \n","                           train = False,  \n","                           transform = transforms.ToTensor())\n","\n","\n","#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n","                                           batch_size = batch_size, \n","                                           shuffle = True,\n","                                           drop_last=True) \n","  \n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n","                                          batch_size = batch_size, \n","                                          shuffle = False,\n","                                          drop_last=True) \n","\n","\n","# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n","# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n","\n","linear1 = torch.nn.Linear(784, 200, bias=True)\n","linear2 = torch.nn.Linear(200, 150, bias=True)\n","linear3 = torch.nn.Linear(150, 10, bias=True)\n","relu = torch.nn.ReLU()\n","dropout = torch.nn.Dropout(p=0.03)\n","bn1 = torch.nn.BatchNorm1d(200)\n","bn2 = torch.nn.BatchNorm1d(150)\n","\n","#xavier initialization을 이용하여 각 layer의 weight 초기화\n","\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)\n","\n","\n","\n","# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n","\n","model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n","                            linear2, bn2,  relu, dropout,\n","                            linear3).to(device)\n","\n","\n","\n","# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","\n","\n","#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","#cost 계산을 위한 변수 설정\n","\n","train_total_batch = len(train_loader)\n","\n","\n","\n","#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n","\n","for epoch in range(training_epochs):\n","    model.train()\n","    avg_cost = 0\n","\n","    \n","#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n","\n","    for X, Y in train_loader:\n","        # reshape input image into [batch_size by 784]\n","        # label is not one-hot encoded\n","        X = X.view(-1, 28 * 28).to(device)\n","        Y = Y.to(device)\n","\n","        optimizer.zero_grad()\n","        hypothesis = model(X)\n","        bn_loss = criterion(hypothesis, Y)\n","        bn_loss.backward()\n","        optimizer.step()\n","\n","        avg_cost += bn_loss / train_total_batch\n","\n","    \n","      \n","        \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')\n","\n","\n","\n","#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n","#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n","#accuracy의 초기 값 설정(0으로) 꼭 할 것\n","\n","with torch.no_grad():\n","    model.eval()\n","    accuracy=0\n","\n","    X_test = test_dataset.test_data.view(-1, 28 * 28).float().to(device)\n","    Y_test = test_dataset.test_labels.to(device)\n","\n","    prediction = model(X_test)\n","    correct_prediction = torch.argmax(prediction, 1) == Y_test\n","    accuracy = correct_prediction.float().mean()\n","    print('Accuracy:', accuracy.item())\n","\n","    # Get one and predict\n","    r = random.randint(0, len(test_dataset) - 1)\n","    X_single_data = test_dataset.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n","    Y_single_data = test_dataset.test_labels[r:r + 1].to(device)\n","\n","    print('Label: ', Y_single_data.item())\n","    single_prediction = model(X_single_data)\n","    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Epoch: 0001 cost = 0.305562794\n","Epoch: 0002 cost = 0.179520071\n","Epoch: 0003 cost = 0.155518338\n","Epoch: 0004 cost = 0.135117471\n","Epoch: 0005 cost = 0.127021611\n","Epoch: 0006 cost = 0.121332474\n","Epoch: 0007 cost = 0.106230646\n","Epoch: 0008 cost = 0.103180945\n","Epoch: 0009 cost = 0.098527424\n","Epoch: 0010 cost = 0.090761736\n","Epoch: 0011 cost = 0.088379502\n","Epoch: 0012 cost = 0.085002348\n","Epoch: 0013 cost = 0.073775344\n","Epoch: 0014 cost = 0.083117932\n","Epoch: 0015 cost = 0.078070484\n","Learning finished\n","Accuracy: 0.8282999992370605\n","Label:  8\n","Prediction:  5\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n","  warnings.warn(\"test_data has been renamed data\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n","  warnings.warn(\"test_labels has been renamed targets\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"gDbnxkP-uk9k","colab_type":"text"},"source":["### ii) Hidden layer node 수 감소"]},{"cell_type":"code","metadata":{"id":"ajHtDV-LnZbq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"status":"ok","timestamp":1596700237524,"user_tz":-540,"elapsed":233938,"user":{"displayName":"Jungyun Choi","photoUrl":"","userId":"11532088005133064485"}},"outputId":"daec75a3-767c-4957-be61-c323a836fa57"},"source":["# 파라미터 설정 (learning rate, training epochs, batch_size)\n","device = torch.device('cuda:0')\n","learning_rate = 0.1\n","training_epochs = 15\n","batch_size = 100\n","\n","\n","#train과 test set으로 나누어 MNIST data 불러오기\n","\n","train_dataset= dsets.MNIST(root='./data',\n","                           train=True,\n","                           transform=transforms.ToTensor(),\n","                           download=True)\n","test_dataset = dsets.MNIST(root ='./data',  \n","                           train = False,  \n","                           transform = transforms.ToTensor())\n","\n","\n","#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n","                                           batch_size = batch_size, \n","                                           shuffle = True,\n","                                           drop_last=True) \n","  \n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n","                                          batch_size = batch_size, \n","                                          shuffle = False,\n","                                          drop_last=True) \n","\n","\n","# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n","# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n","\n","linear1 = torch.nn.Linear(784, 75, bias=True)\n","linear2 = torch.nn.Linear(75, 50, bias=True)\n","linear3 = torch.nn.Linear(50, 10, bias=True)\n","relu = torch.nn.ReLU()\n","dropout = torch.nn.Dropout(p=0.03)\n","bn1 = torch.nn.BatchNorm1d(75)\n","bn2 = torch.nn.BatchNorm1d(50)\n","\n","#xavier initialization을 이용하여 각 layer의 weight 초기화\n","\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)\n","\n","\n","\n","# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n","\n","model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n","                            linear2, bn2,  relu, dropout,\n","                            linear3).to(device)\n","\n","\n","\n","# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","\n","\n","#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","#cost 계산을 위한 변수 설정\n","\n","train_total_batch = len(train_loader)\n","\n","\n","\n","#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n","\n","for epoch in range(training_epochs):\n","    model.train()\n","    avg_cost = 0\n","\n","    \n","#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n","\n","    for X, Y in train_loader:\n","        # reshape input image into [batch_size by 784]\n","        # label is not one-hot encoded\n","        X = X.view(-1, 28 * 28).to(device)\n","        Y = Y.to(device)\n","\n","        optimizer.zero_grad()\n","        hypothesis = model(X)\n","        bn_loss = criterion(hypothesis, Y)\n","        bn_loss.backward()\n","        optimizer.step()\n","\n","        avg_cost += bn_loss / train_total_batch\n","\n","    \n","      \n","        \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')\n","\n","\n","\n","#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n","#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n","#accuracy의 초기 값 설정(0으로) 꼭 할 것\n","\n","with torch.no_grad():\n","    model.eval()\n","    accuracy=0\n","\n","    X_test = test_dataset.test_data.view(-1, 28 * 28).float().to(device)\n","    Y_test = test_dataset.test_labels.to(device)\n","\n","    prediction = model(X_test)\n","    correct_prediction = torch.argmax(prediction, 1) == Y_test\n","    accuracy = correct_prediction.float().mean()\n","    print('Accuracy:', accuracy.item())\n","\n","    # Get one and predict\n","    r = random.randint(0, len(test_dataset) - 1)\n","    X_single_data = test_dataset.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n","    Y_single_data = test_dataset.test_labels[r:r + 1].to(device)\n","\n","    print('Label: ', Y_single_data.item())\n","    single_prediction = model(X_single_data)\n","    print('Prediction: ', torch.argmax(single_prediction, 1).item())"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Epoch: 0001 cost = 0.303691298\n","Epoch: 0002 cost = 0.202788413\n","Epoch: 0003 cost = 0.170268387\n","Epoch: 0004 cost = 0.154466957\n","Epoch: 0005 cost = 0.145586014\n","Epoch: 0006 cost = 0.140276745\n","Epoch: 0007 cost = 0.133625925\n","Epoch: 0008 cost = 0.130788818\n","Epoch: 0009 cost = 0.114367209\n","Epoch: 0010 cost = 0.119733892\n","Epoch: 0011 cost = 0.117148787\n","Epoch: 0012 cost = 0.110618398\n","Epoch: 0013 cost = 0.107757166\n","Epoch: 0014 cost = 0.114211902\n","Epoch: 0015 cost = 0.097845942\n","Learning finished\n","Accuracy: 0.8382999897003174\n","Label:  4\n","Prediction:  4\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n","  warnings.warn(\"test_data has been renamed data\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n","  warnings.warn(\"test_labels has been renamed targets\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"UgeBqxb1oF_S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"status":"ok","timestamp":1596700339792,"user_tz":-540,"elapsed":77330,"user":{"displayName":"Jungyun Choi","photoUrl":"","userId":"11532088005133064485"}},"outputId":"c2831de2-7344-45b4-84fd-0325ca4e8e09"},"source":["# 파라미터 설정 (learning rate, training epochs, batch_size)\n","device = torch.device('cuda:0')\n","learning_rate = 0.1\n","training_epochs = 15\n","batch_size = 100\n","\n","\n","#train과 test set으로 나누어 MNIST data 불러오기\n","\n","train_dataset= dsets.MNIST(root='./data',\n","                           train=True,\n","                           transform=transforms.ToTensor(),\n","                           download=True)\n","test_dataset = dsets.MNIST(root ='./data',  \n","                           train = False,  \n","                           transform = transforms.ToTensor())\n","\n","\n","#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n","\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n","                                           batch_size = batch_size, \n","                                           shuffle = True,\n","                                           drop_last=True) \n","  \n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n","                                          batch_size = batch_size, \n","                                          shuffle = False,\n","                                          drop_last=True) \n","\n","\n","# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n","# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n","\n","linear1 = torch.nn.Linear(784, 256, bias=True)\n","linear2 = torch.nn.Linear(256, 256, bias=True)\n","linear3 = torch.nn.Linear(256, 10, bias=True)\n","relu = torch.nn.ReLU()\n","dropout = torch.nn.Dropout(p=0.03)\n","bn1 = torch.nn.BatchNorm1d(256)\n","bn2 = torch.nn.BatchNorm1d(256)\n","\n","#xavier initialization을 이용하여 각 layer의 weight 초기화\n","\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)\n","\n","\n","\n","# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n","\n","model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n","                            linear2, bn2,  relu, dropout,\n","                            linear3).to(device)\n","\n","\n","\n","# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","\n","\n","\n","\n","#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","\n","\n","#cost 계산을 위한 변수 설정\n","\n","train_total_batch = len(train_loader)\n","\n","\n","\n","#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n","\n","for epoch in range(training_epochs):\n","    model.train()\n","    avg_cost = 0\n","\n","    \n","#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드\n","\n","    for X, Y in train_loader:\n","        # reshape input image into [batch_size by 784]\n","        # label is not one-hot encoded\n","        X = X.view(-1, 28 * 28).to(device)\n","        Y = Y.to(device)\n","\n","        optimizer.zero_grad()\n","        hypothesis = model(X)\n","        bn_loss = criterion(hypothesis, Y)\n","        bn_loss.backward()\n","        optimizer.step()\n","\n","        avg_cost += bn_loss / train_total_batch\n","\n","    \n","      \n","        \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')\n","\n","\n","\n","#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n","#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n","#accuracy의 초기 값 설정(0으로) 꼭 할 것\n","\n","with torch.no_grad():\n","    model.eval()\n","    accuracy=0\n","\n","    X_test = test_dataset.test_data.view(-1, 28 * 28).float().to(device)\n","    Y_test = test_dataset.test_labels.to(device)\n","\n","    prediction = model(X_test)\n","    correct_prediction = torch.argmax(prediction, 1) == Y_test\n","    accuracy = correct_prediction.float().mean()\n","    print('Accuracy:', accuracy.item())\n","\n","    # Get one and predict\n","    r = random.randint(0, len(test_dataset) - 1)\n","    X_single_data = test_dataset.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n","    Y_single_data = test_dataset.test_labels[r:r + 1].to(device)\n","\n","    print('Label: ', Y_single_data.item())\n","    single_prediction = model(X_single_data)\n","    print('Prediction: ', torch.argmax(single_prediction, 1).item())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch: 0001 cost = 0.310552359\n","Epoch: 0002 cost = 0.184533507\n","Epoch: 0003 cost = 0.150780648\n","Epoch: 0004 cost = 0.136335269\n","Epoch: 0005 cost = 0.120968997\n","Epoch: 0006 cost = 0.118954331\n","Epoch: 0007 cost = 0.108289070\n","Epoch: 0008 cost = 0.105005130\n","Epoch: 0009 cost = 0.090616472\n","Epoch: 0010 cost = 0.090751357\n","Epoch: 0011 cost = 0.093109652\n","Epoch: 0012 cost = 0.086847045\n","Epoch: 0013 cost = 0.083666965\n","Epoch: 0014 cost = 0.076678947\n","Epoch: 0015 cost = 0.077916332\n","Learning finished\n","Accuracy: 0.7560999989509583\n","Label:  0\n","Prediction:  0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n","  warnings.warn(\"test_data has been renamed data\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n","  warnings.warn(\"test_labels has been renamed targets\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"B7MQvbyQooIl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1596700727295,"user_tz":-540,"elapsed":6089,"user":{"displayName":"Jungyun Choi","photoUrl":"","userId":"11532088005133064485"}},"outputId":"8ac29e0e-f2b9-4643-a3b2-d83279eeaf8c"},"source":["!pip install tensorboardX"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Collecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\r\u001b[K     |█                               | 10kB 22.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 3.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 4.6MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.2.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4EwF3I8kxeOF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596700970525,"user_tz":-540,"elapsed":756,"user":{"displayName":"Jungyun Choi","photoUrl":"","userId":"11532088005133064485"}}},"source":["from tensorboardX import SummaryWriter\n","writer = SummaryWriter('run/mnist_graph')"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"sM4BCvGGyMrf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"status":"error","timestamp":1596700972139,"user_tz":-540,"elapsed":934,"user":{"displayName":"Jungyun Choi","photoUrl":"","userId":"11532088005133064485"}},"outputId":"c28bfade-19a7-4db2-fc8c-5ffd823aa924"},"source":["writer.add_scalars('loss/L1_loss', 0.2, 13)"],"execution_count":17,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-aa62ac659bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss/L1_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorboardX/writer.py\u001b[0m in \u001b[0;36madd_scalars\u001b[0;34m(self, main_tag, tag_scalar_dict, global_step, walltime)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mwalltime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwalltime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mwalltime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mfw_logdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_scalar_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m             \u001b[0mfw_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfw_logdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmain_tag\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfw_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_writers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'items'"]}]}]}