{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"WEEK 2 HW 한인욱.ipynb의 사본","provenance":[{"file_id":"1fhWLezZuskneOKEajExlBUyQk-UD4FYo","timestamp":1596705899783}],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"da71362f6a504662a94d32df6e5ad9ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5f3d2560643f43b28a3866f7119d5f04","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c5691b74bcda4c17b38c286e20dbe24e","IPY_MODEL_fb64ee2128ae4a2fbbdda37601a04aa7"]}},"5f3d2560643f43b28a3866f7119d5f04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c5691b74bcda4c17b38c286e20dbe24e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ddb951e060e6413a8fd7c4db123090ae","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ae052b618a314378a3e719e6b3e3babe"}},"fb64ee2128ae4a2fbbdda37601a04aa7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b28483a0cfe74ee585559749003322d5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9920512/? [00:20&lt;00:00, 11526465.06it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b8f238d996cc4978a20fd99d833ade2f"}},"ddb951e060e6413a8fd7c4db123090ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ae052b618a314378a3e719e6b3e3babe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b28483a0cfe74ee585559749003322d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b8f238d996cc4978a20fd99d833ade2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b94e96fb04848fca230e2375ff1809c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_aceb9f0df80d4340a843e0189e90712c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_69682521208d4b59b9aef19fa20b6a1b","IPY_MODEL_28ce1f982ffb4771aced837495667131"]}},"aceb9f0df80d4340a843e0189e90712c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"69682521208d4b59b9aef19fa20b6a1b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4ed1aa92c19c411881dd59a37ac1d2d8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b9ecc71d316a416bab5490117329f153"}},"28ce1f982ffb4771aced837495667131":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fcc96cc2c06a4078890a757255464d6a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 32768/? [00:00&lt;00:00, 57968.84it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2e7228d18b8b4c31bec9613f83fc8707"}},"4ed1aa92c19c411881dd59a37ac1d2d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b9ecc71d316a416bab5490117329f153":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fcc96cc2c06a4078890a757255464d6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2e7228d18b8b4c31bec9613f83fc8707":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b38be002df14680ab0af5f8040bb30a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_17d0b8be0dea498c8d71fe8dda58b5d4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_27c8a4119d6a41a6a7e60881080af4b0","IPY_MODEL_1fe152237c214bb79a42a4e3f6c7d02a"]}},"17d0b8be0dea498c8d71fe8dda58b5d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27c8a4119d6a41a6a7e60881080af4b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6d7512757854478b93afe2c99c7ff092","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9a23ca0521fc45d3b03c7fcef5f5e345"}},"1fe152237c214bb79a42a4e3f6c7d02a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bd15dfed72c34f48b0d6c8acc4668ad9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1654784/? [00:00&lt;00:00, 3542510.45it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ca24f5defa774fd9a8b42e693b0b7481"}},"6d7512757854478b93afe2c99c7ff092":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9a23ca0521fc45d3b03c7fcef5f5e345":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bd15dfed72c34f48b0d6c8acc4668ad9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ca24f5defa774fd9a8b42e693b0b7481":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1d65618fc2f47afb0117862b6025ddc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_526bf4f8b245422db3f68b74bbaa6073","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6609cc767ca34624abfcb81337e8cd95","IPY_MODEL_a4c62a4007dd456fb6013df87ada10c6"]}},"526bf4f8b245422db3f68b74bbaa6073":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6609cc767ca34624abfcb81337e8cd95":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_69016b02758947c5bf2f7ebfce9495b4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b1451e0afcde4d35800da1fc1f38e1f2"}},"a4c62a4007dd456fb6013df87ada10c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_202d1c318872466082f94135f4231fc9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8192/? [00:00&lt;00:00, 42839.04it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_55491b9c32a24e58af004e51ed03054d"}},"69016b02758947c5bf2f7ebfce9495b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b1451e0afcde4d35800da1fc1f38e1f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"202d1c318872466082f94135f4231fc9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"55491b9c32a24e58af004e51ed03054d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"tqYY3kyaFBZe","colab_type":"text"},"source":["## **1. Basic Model**"]},{"cell_type":"code","metadata":{"id":"2xCgAG0tFBZi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":834,"referenced_widgets":["da71362f6a504662a94d32df6e5ad9ce","5f3d2560643f43b28a3866f7119d5f04","c5691b74bcda4c17b38c286e20dbe24e","fb64ee2128ae4a2fbbdda37601a04aa7","ddb951e060e6413a8fd7c4db123090ae","ae052b618a314378a3e719e6b3e3babe","b28483a0cfe74ee585559749003322d5","b8f238d996cc4978a20fd99d833ade2f","2b94e96fb04848fca230e2375ff1809c","aceb9f0df80d4340a843e0189e90712c","69682521208d4b59b9aef19fa20b6a1b","28ce1f982ffb4771aced837495667131","4ed1aa92c19c411881dd59a37ac1d2d8","b9ecc71d316a416bab5490117329f153","fcc96cc2c06a4078890a757255464d6a","2e7228d18b8b4c31bec9613f83fc8707","6b38be002df14680ab0af5f8040bb30a","17d0b8be0dea498c8d71fe8dda58b5d4","27c8a4119d6a41a6a7e60881080af4b0","1fe152237c214bb79a42a4e3f6c7d02a","6d7512757854478b93afe2c99c7ff092","9a23ca0521fc45d3b03c7fcef5f5e345","bd15dfed72c34f48b0d6c8acc4668ad9","ca24f5defa774fd9a8b42e693b0b7481","a1d65618fc2f47afb0117862b6025ddc","526bf4f8b245422db3f68b74bbaa6073","6609cc767ca34624abfcb81337e8cd95","a4c62a4007dd456fb6013df87ada10c6","69016b02758947c5bf2f7ebfce9495b4","b1451e0afcde4d35800da1fc1f38e1f2","202d1c318872466082f94135f4231fc9","55491b9c32a24e58af004e51ed03054d"]},"executionInfo":{"status":"ok","timestamp":1596705867148,"user_tz":-540,"elapsed":115852,"user":{"displayName":"rany go","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRLztt21gJ3vDfYCggLsxHzcHwHcCZvYUXpjI5=s64","userId":"15846384859939162308"}},"outputId":"2b468298-3873-4c23-8626-052d7d45a7e0"},"source":["import torch\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import matplotlib.pylab as plt\n","import random\n","\n","\n","# 파라미터 설정 (learning rate, training epochs, batch_size)\n","learning_rate = 0.1\n","training_epochs = 15\n","batch_size = 100\n","\n","\n","#train과 test set으로 나누어 MNIST data 불러오기\n","\n","mnist_train = dsets.MNIST(root='MNIST_data/',\n","                          train=True,\n","                          transform=transforms.ToTensor(),\n","                          download=True)\n","\n","mnist_test = dsets.MNIST(root='MNIST_data/',\n","                         train=False,\n","                         transform=transforms.ToTensor(),\n","                         download=True)\n","\n","\n","#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n","                                          batch_size=batch_size,\n","                                          shuffle=True,\n","                                          drop_last=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,\n","                                          drop_last=True)\n","\n","\n","# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n","\n","# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n","\n","linear1 = torch.nn.Linear(784, 100, bias=True)\n","linear2 = torch.nn.Linear(100, 100, bias=True)\n","linear3 = torch.nn.Linear(100, 10, bias=True)\n","relu = torch.nn.ReLU()\n","bn1 = torch.nn.BatchNorm1d(100)\n","bn2 = torch.nn.BatchNorm1d(100)\n","dropout = torch.nn.Dropout(p=0.3)\n","\n","\n","#xavier initialization을 이용하여 각 layer의 weight 초기화\n","\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)\n","\n","\n","#model 정의하기(쌓는 순서: linear-Batch Normalization layer - ReLU- DropOut)\n","bn_model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n","                            linear2, bn2, relu,dropout,\n","                            linear3)\n","\n","# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n","bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n","\n","\n","\n","#cost 계산을 위한 변수 설정\n","train_total_batch = len(train_loader)\n","\n","\n","#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    bn_model.train()  # set the model to train mode\n","\n","    \n","    #train dataset을 불러오고, back propagation을 통해 loss를 최적화하는 과정\n","    for X, Y in train_loader:\n","        X = X.view(-1, 28 * 28)\n","        Y = Y\n","\n","        bn_optimizer.zero_grad()\n","        bn_prediction = bn_model(X)\n","        bn_loss = criterion(bn_prediction, Y)\n","        bn_loss.backward()\n","        bn_optimizer.step()\n","\n","        avg_cost += bn_loss / train_total_batch\n","\t \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')\n","\n","\n","\n","#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n","#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 .test_labels 사용\n","#accuracy의 초기 값 설정(0으로) 꼭 할 것\n","\n","with torch.no_grad():\n","    bn_model.eval()     \n","    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n","    Y_test = mnist_test.test_labels\n","    \n","    bn_acc = 0 \n","    bn_prediction = bn_model(X_test)\n","    bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y_test\n","    bn_loss += criterion(bn_prediction, Y_test)\n","    bn_acc += bn_correct_prediction.float().mean()\n","    \n","    print(\"Accuracy: \", bn_acc.item())\n","    \n","    \n","    \n","    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n","    r = random.randint(0, len(mnist_test)-1)\n","    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n","    Y_single_data = mnist_test.test_labels[r:r + 1]\n","    \n","    print('Label: ', Y_single_data.item())\n","    single_prediction = bn_model(X_single_data)\n","    print('Prediction: ', torch.argmax(single_prediction, 1).item())"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da71362f6a504662a94d32df6e5ad9ce","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b94e96fb04848fca230e2375ff1809c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b38be002df14680ab0af5f8040bb30a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1d65618fc2f47afb0117862b6025ddc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n","Processing...\n","\n","\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n","  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"],"name":"stderr"},{"output_type":"stream","text":["Done!\n","Epoch: 0001 cost = 0.501793623\n","Epoch: 0002 cost = 0.378185362\n","Epoch: 0003 cost = 0.324896991\n","Epoch: 0004 cost = 0.308057189\n","Epoch: 0005 cost = 0.295960277\n","Epoch: 0006 cost = 0.289110929\n","Epoch: 0007 cost = 0.271036565\n","Epoch: 0008 cost = 0.270288378\n","Epoch: 0009 cost = 0.257958978\n","Epoch: 0010 cost = 0.262471378\n","Epoch: 0011 cost = 0.240735635\n","Epoch: 0012 cost = 0.255622119\n","Epoch: 0013 cost = 0.247895017\n","Epoch: 0014 cost = 0.241694197\n","Epoch: 0015 cost = 0.235026151\n","Learning finished\n","Accuracy:  0.9491999745368958\n","Label:  3\n","Prediction:  3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n","  warnings.warn(\"test_data has been renamed data\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n","  warnings.warn(\"test_labels has been renamed targets\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Pzmw6WDmFBZw","colab_type":"text"},"source":["## **2. Increasing Model**"]},{"cell_type":"code","metadata":{"id":"2XJppKP8FBZz","colab_type":"code","colab":{},"outputId":"5736df3d-c8a7-488a-abac-66e60d34c8de"},"source":["import torch\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import matplotlib.pylab as plt\n","import random\n","\n","\n","# 파라미터 설정 (learning rate, training epochs, batch_size)\n","learning_rate = 0.1\n","training_epochs = 15\n","batch_size = 100\n","\n","\n","#train과 test set으로 나누어 MNIST data 불러오기\n","\n","mnist_train = dsets.MNIST(root='MNIST_data/',\n","                          train=True,\n","                          transform=transforms.ToTensor(),\n","                          download=True)\n","\n","mnist_test = dsets.MNIST(root='MNIST_data/',\n","                         train=False,\n","                         transform=transforms.ToTensor(),\n","                         download=True)\n","\n","\n","#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n","                                          batch_size=batch_size,\n","                                          shuffle=True,\n","                                          drop_last=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,\n","                                          drop_last=True)\n","\n","\n","# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n","\n","# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n","\n","linear1 = torch.nn.Linear(784, 200, bias=True)\n","linear2 = torch.nn.Linear(200, 200, bias=True)\n","linear3 = torch.nn.Linear(200, 10, bias=True)\n","relu = torch.nn.ReLU()\n","bn1 = torch.nn.BatchNorm1d(200)\n","bn2 = torch.nn.BatchNorm1d(200)\n","dropout = torch.nn.Dropout(p=0.3)\n","\n","\n","#xavier initialization을 이용하여 각 layer의 weight 초기화\n","\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)\n","\n","\n","#model 정의하기(쌓는 순서: linear-Batch Normalization layer - ReLU- DropOut)\n","bn_model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n","                            linear2, bn2, relu,dropout,\n","                            linear3)\n","\n","# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n","bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n","\n","\n","\n","#cost 계산을 위한 변수 설정\n","train_total_batch = len(train_loader)\n","\n","\n","#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    bn_model.train()  # set the model to train mode\n","\n","    \n","    #train dataset을 불러오고, back propagation을 통해 loss를 최적화하는 과정\n","    for X, Y in train_loader:\n","        X = X.view(-1, 28 * 28)\n","        Y = Y\n","\n","        bn_optimizer.zero_grad()\n","        bn_prediction = bn_model(X)\n","        bn_loss = criterion(bn_prediction, Y)\n","        bn_loss.backward()\n","        bn_optimizer.step()\n","\n","        avg_cost += bn_loss / train_total_batch\n","\t \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')\n","\n","\n","\n","#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n","#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 .test_labels 사용\n","#accuracy의 초기 값 설정(0으로) 꼭 할 것\n","\n","with torch.no_grad():\n","    bn_model.eval()     \n","    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n","    Y_test = mnist_test.test_labels\n","    \n","    bn_acc = 0 \n","    bn_prediction = bn_model(X_test)\n","    bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y_test\n","    bn_loss += criterion(bn_prediction, Y_test)\n","    bn_acc += bn_correct_prediction.float().mean()\n","    \n","    print(\"Accuracy: \", bn_acc.item())\n","    \n","    \n","    \n","    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n","    r = random.randint(0, len(mnist_test)-1)\n","    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n","    Y_single_data = mnist_test.test_labels[r:r + 1]\n","    \n","    print('Label: ', Y_single_data.item())\n","    single_prediction = bn_model(X_single_data)\n","    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n","    \n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0001 cost = 0.466080725\n","Epoch: 0002 cost = 0.327673644\n","Epoch: 0003 cost = 0.292566210\n","Epoch: 0004 cost = 0.271716744\n","Epoch: 0005 cost = 0.261581928\n","Epoch: 0006 cost = 0.241671979\n","Epoch: 0007 cost = 0.240976557\n","Epoch: 0008 cost = 0.231926531\n","Epoch: 0009 cost = 0.229610890\n","Epoch: 0010 cost = 0.217063710\n","Epoch: 0011 cost = 0.211958751\n","Epoch: 0012 cost = 0.214407966\n","Epoch: 0013 cost = 0.204376295\n","Epoch: 0014 cost = 0.201951459\n","Epoch: 0015 cost = 0.198006734\n","Learning finished\n","Accuracy:  0.8970000147819519\n","Label:  7\n","Prediction:  7\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OulGy4lkFBaB","colab_type":"text"},"source":["## **3. Decreasing Model**"]},{"cell_type":"code","metadata":{"id":"x5VPFLKXFBaE","colab_type":"code","colab":{},"outputId":"8cf015fe-f2ec-4e4a-ac3b-8890654c6f6c"},"source":["import torch\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import matplotlib.pylab as plt\n","import random\n","\n","\n","# 파라미터 설정 (learning rate, training epochs, batch_size)\n","learning_rate = 0.1\n","training_epochs = 15\n","batch_size = 100\n","\n","\n","#train과 test set으로 나누어 MNIST data 불러오기\n","\n","mnist_train = dsets.MNIST(root='MNIST_data/',\n","                          train=True,\n","                          transform=transforms.ToTensor(),\n","                          download=True)\n","\n","mnist_test = dsets.MNIST(root='MNIST_data/',\n","                         train=False,\n","                         transform=transforms.ToTensor(),\n","                         download=True)\n","\n","\n","#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n","                                          batch_size=batch_size,\n","                                          shuffle=True,\n","                                          drop_last=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,\n","                                          drop_last=True)\n","\n","\n","# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n","\n","# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n","\n","linear1 = torch.nn.Linear(784, 50, bias=True)\n","linear2 = torch.nn.Linear(50, 50, bias=True)\n","linear3 = torch.nn.Linear(50, 10, bias=True)\n","relu = torch.nn.ReLU()\n","bn1 = torch.nn.BatchNorm1d(50)\n","bn2 = torch.nn.BatchNorm1d(50)\n","dropout = torch.nn.Dropout(p=0.3)\n","\n","\n","#xavier initialization을 이용하여 각 layer의 weight 초기화\n","\n","torch.nn.init.xavier_uniform_(linear1.weight)\n","torch.nn.init.xavier_uniform_(linear2.weight)\n","torch.nn.init.xavier_uniform_(linear3.weight)\n","\n","\n","#model 정의하기(쌓는 순서: linear-Batch Normalization layer - ReLU- DropOut)\n","bn_model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n","                            linear2, bn2, relu,dropout,\n","                            linear3)\n","\n","# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","#optimizer 정의하기 (Adam optimizer를 사용할 것!)\n","bn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\n","\n","\n","\n","#cost 계산을 위한 변수 설정\n","train_total_batch = len(train_loader)\n","\n","\n","#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n","for epoch in range(training_epochs):\n","    avg_cost = 0\n","    bn_model.train()  # set the model to train mode\n","\n","    \n","    #train dataset을 불러오고, back propagation을 통해 loss를 최적화하는 과정\n","    for X, Y in train_loader:\n","        X = X.view(-1, 28 * 28)\n","        Y = Y\n","\n","        bn_optimizer.zero_grad()\n","        bn_prediction = bn_model(X)\n","        bn_loss = criterion(bn_prediction, Y)\n","        bn_loss.backward()\n","        bn_optimizer.step()\n","\n","        avg_cost += bn_loss / train_total_batch\n","\t \n","    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n","\n","print('Learning finished')\n","\n","\n","\n","#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n","#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 .test_labels 사용\n","#accuracy의 초기 값 설정(0으로) 꼭 할 것\n","\n","with torch.no_grad():\n","    bn_model.eval()     \n","    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n","    Y_test = mnist_test.test_labels\n","    \n","    bn_acc = 0 \n","    bn_prediction = bn_model(X_test)\n","    bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y_test\n","    bn_loss += criterion(bn_prediction, Y_test)\n","    bn_acc += bn_correct_prediction.float().mean()\n","    \n","    print(\"Accuracy: \", bn_acc.item())\n","    \n","    \n","    \n","    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n","    r = random.randint(0, len(mnist_test)-1)\n","    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n","    Y_single_data = mnist_test.test_labels[r:r + 1]\n","    \n","    print('Label: ', Y_single_data.item())\n","    single_prediction = bn_model(X_single_data)\n","    print('Prediction: ', torch.argmax(single_prediction, 1).item())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 0001 cost = 0.593761981\n","Epoch: 0002 cost = 0.447104007\n","Epoch: 0003 cost = 0.401066989\n","Epoch: 0004 cost = 0.390282810\n","Epoch: 0005 cost = 0.376429737\n","Epoch: 0006 cost = 0.386880755\n","Epoch: 0007 cost = 0.370123804\n","Epoch: 0008 cost = 0.359081507\n","Epoch: 0009 cost = 0.353553027\n","Epoch: 0010 cost = 0.351909220\n","Epoch: 0011 cost = 0.334892690\n","Epoch: 0012 cost = 0.343113780\n","Epoch: 0013 cost = 0.337895274\n","Epoch: 0014 cost = 0.334494233\n","Epoch: 0015 cost = 0.335095257\n","Learning finished\n","Accuracy:  0.9415000081062317\n","Label:  7\n","Prediction:  0\n"],"name":"stdout"}]}]}