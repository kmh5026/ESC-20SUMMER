{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week2: HW \n",
    "### 손지우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1. 주석 기반 코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ReLU + BatchNorm](https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-09_6_mnist_batchnorm.ipynb)  \n",
    "[ReLU + Dropout](https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-09_5_mnist_nn_dropout.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정 (learning rate, training epochs, batch_size)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test set으로 나누어 MNIST data 불러오기\n",
    "\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할 것!)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "\n",
    "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0629e-02,  8.4158e-02, -1.3903e-01,  3.5614e-02, -1.3131e-01,\n",
       "         -2.0189e-01,  1.4222e-01,  2.3109e-01, -1.1594e-02, -9.8138e-02,\n",
       "          1.4809e-01, -7.9048e-03,  1.1158e-02,  1.6674e-01, -1.9916e-01,\n",
       "         -1.9218e-01,  8.4182e-02,  8.2943e-02,  1.5958e-02, -3.1829e-02,\n",
       "          1.4884e-01, -4.6380e-03,  2.0886e-01, -1.8597e-01, -2.6672e-02,\n",
       "          5.8527e-02,  1.4898e-01,  1.0450e-01,  1.8793e-01,  6.6393e-03,\n",
       "          2.1926e-02,  2.1737e-01,  9.1966e-02,  1.1258e-01, -2.1619e-01,\n",
       "          2.5430e-02,  1.8140e-01, -2.0899e-01, -1.0319e-01,  3.9974e-02,\n",
       "          2.3275e-01,  3.7698e-02, -1.3693e-01, -4.9869e-02,  2.3313e-01,\n",
       "         -1.0087e-01, -2.2375e-01,  3.7332e-02,  2.2167e-01, -2.0185e-01,\n",
       "         -7.2648e-02, -1.8003e-01,  2.3124e-01, -7.1334e-02,  1.6326e-01,\n",
       "          6.9623e-02,  1.9456e-01,  1.3823e-01, -1.6573e-01,  1.0222e-02,\n",
       "          2.2103e-01, -6.4158e-02, -7.9413e-02,  2.1403e-01, -1.3068e-01,\n",
       "          9.1833e-02,  2.2274e-01, -1.9403e-01, -1.2171e-01, -2.1504e-01,\n",
       "         -1.5813e-01,  2.0407e-01,  6.0705e-02,  2.0414e-01, -7.2801e-02,\n",
       "          5.6013e-02, -1.2935e-01, -3.3411e-02, -9.0940e-02,  1.5500e-01,\n",
       "         -7.3261e-02, -7.0562e-02,  2.7222e-02, -1.5687e-01,  1.9697e-01,\n",
       "          2.9683e-02,  1.9993e-02,  3.6079e-03,  2.4326e-02, -1.4522e-01,\n",
       "         -1.9711e-01,  1.0009e-01,  1.3585e-01, -1.7184e-01,  3.7026e-02,\n",
       "          9.5112e-02, -2.1464e-01, -1.4822e-01,  1.0054e-01, -9.3506e-03],\n",
       "        [ 1.4689e-01, -1.1983e-01, -1.3503e-02, -2.0550e-01,  2.7662e-02,\n",
       "          1.8435e-01, -2.2119e-01,  1.6242e-01, -6.2722e-02, -1.0764e-01,\n",
       "         -1.1131e-01,  1.7435e-01,  4.3074e-02, -1.0524e-01, -2.2071e-02,\n",
       "         -2.0681e-01, -1.8722e-01, -1.1362e-01, -2.1501e-01, -3.7385e-02,\n",
       "         -6.8938e-02, -1.0922e-01,  4.8126e-02, -1.9369e-01,  1.0167e-01,\n",
       "          8.7725e-03,  7.8495e-02, -1.0344e-01, -1.2648e-01,  1.5394e-01,\n",
       "          2.2221e-01, -9.9597e-02, -3.2177e-02,  1.9830e-01, -1.3196e-02,\n",
       "          1.2183e-01,  2.5627e-02,  1.7099e-01, -6.7939e-02,  1.8347e-01,\n",
       "         -1.4662e-01,  1.1689e-01, -1.9668e-01, -2.8762e-02, -2.1232e-02,\n",
       "         -1.3444e-01, -1.2131e-01, -1.8175e-01, -1.4309e-01, -1.7763e-01,\n",
       "          8.8822e-02, -2.0047e-01, -1.1563e-01,  2.0657e-01,  1.9424e-01,\n",
       "          8.7437e-02, -1.4150e-01,  1.3476e-01,  1.1360e-01, -1.9472e-01,\n",
       "          1.4418e-02, -1.6231e-01,  1.4749e-01,  7.8501e-02, -1.1981e-01,\n",
       "         -1.9863e-01, -3.8404e-02,  5.0669e-02, -4.2821e-02,  1.0512e-01,\n",
       "          1.8598e-01,  1.0540e-01, -1.3574e-02,  4.5411e-02,  2.2257e-01,\n",
       "         -1.1891e-02, -1.1251e-01,  1.5790e-01, -1.9629e-01, -1.0534e-01,\n",
       "         -1.7532e-01, -6.6995e-02,  4.5706e-02,  1.9385e-01,  1.3894e-01,\n",
       "          7.6774e-02,  1.6517e-01,  2.0993e-01, -1.2536e-02,  1.6057e-01,\n",
       "          1.3849e-01, -1.9771e-01, -2.0445e-01, -3.5105e-02, -8.4770e-02,\n",
       "         -1.4268e-01,  2.1657e-01,  1.6079e-02, -7.2812e-02,  5.0286e-02],\n",
       "        [-1.7218e-01, -1.3720e-01,  1.2707e-01,  2.7161e-02,  1.7097e-01,\n",
       "          2.1591e-01, -1.9399e-01, -2.8466e-02, -3.2802e-02, -8.0304e-03,\n",
       "         -2.1838e-01, -1.5901e-02,  1.8195e-01, -1.1478e-01,  1.4069e-02,\n",
       "          1.0259e-01, -1.5566e-01, -2.2785e-01, -1.3593e-01,  1.3868e-02,\n",
       "         -1.0073e-01,  2.2031e-01, -1.1375e-01,  1.0100e-02, -2.3194e-01,\n",
       "          8.8435e-02,  2.0277e-01, -1.5662e-01,  4.8966e-03, -1.8101e-02,\n",
       "          7.9601e-02,  1.4013e-01,  2.1626e-01, -2.2215e-01, -2.2486e-01,\n",
       "          2.9542e-02,  1.2904e-01, -1.2440e-01,  1.2919e-05,  1.7266e-01,\n",
       "         -2.2372e-01,  6.6826e-02,  7.0880e-02,  1.6662e-01, -7.1975e-02,\n",
       "          2.1432e-01, -1.2434e-01, -9.1517e-02,  1.4123e-01,  1.1164e-01,\n",
       "         -1.5659e-01,  8.7773e-02,  1.3294e-01,  2.0084e-01, -1.0632e-01,\n",
       "          1.4247e-01, -7.2172e-02,  1.4069e-01, -1.5240e-01, -6.5192e-02,\n",
       "         -2.4549e-02,  2.0727e-02,  1.2525e-01, -1.9497e-01, -1.9708e-01,\n",
       "          1.5639e-01, -2.0392e-01,  7.2194e-02,  1.9737e-01,  7.5752e-02,\n",
       "          1.4027e-01,  2.1542e-01,  2.1386e-01, -1.2196e-01, -8.5120e-02,\n",
       "         -2.2456e-01,  1.0544e-01,  1.2912e-02,  1.4558e-01,  7.3699e-02,\n",
       "          2.8953e-03, -1.8666e-01, -3.9395e-02,  1.2240e-01,  1.8979e-01,\n",
       "         -1.1979e-01, -1.8365e-01,  1.8938e-02, -1.2276e-01, -9.8455e-02,\n",
       "         -6.7742e-02, -1.4193e-01, -3.6543e-02, -1.8293e-01,  1.1486e-01,\n",
       "          4.5124e-02, -5.5152e-02,  2.7319e-02,  3.6633e-02,  1.1275e-01],\n",
       "        [-8.0010e-02,  6.9066e-03,  2.1252e-01,  1.0821e-01,  1.8767e-01,\n",
       "         -2.2199e-01,  5.3116e-02, -1.4257e-01, -2.1158e-01, -1.4701e-01,\n",
       "         -1.7834e-01,  1.1778e-01,  2.1258e-01, -4.2422e-04, -1.0541e-01,\n",
       "          1.7681e-01,  2.3157e-01,  2.1564e-01,  2.2486e-01,  1.5427e-01,\n",
       "          1.8100e-01, -8.3964e-02,  8.0406e-02,  1.3956e-01, -3.2296e-02,\n",
       "          1.7521e-01, -6.8805e-02,  3.4537e-02, -1.4329e-01,  2.1950e-01,\n",
       "         -1.4540e-01,  5.8616e-02, -6.5331e-02,  2.0219e-02, -1.8773e-01,\n",
       "         -1.6692e-01,  1.2577e-01, -1.3938e-02, -1.2953e-01, -7.9364e-02,\n",
       "          3.3717e-03, -9.1108e-02, -1.3456e-01,  4.8819e-02, -1.3755e-01,\n",
       "          2.5856e-02,  9.6650e-02,  8.2156e-02,  6.7854e-02, -6.0623e-02,\n",
       "          3.8132e-02, -1.8767e-01,  5.7717e-02, -5.3082e-03,  9.8963e-02,\n",
       "         -7.9118e-02,  1.9286e-01, -1.7589e-01,  3.1280e-02,  1.6086e-01,\n",
       "          1.0437e-01,  1.5499e-01,  2.1432e-01,  5.8080e-02, -8.3503e-02,\n",
       "          1.5104e-01, -2.2297e-01, -3.2302e-02,  1.5242e-01,  1.9528e-01,\n",
       "         -6.6289e-02,  7.9102e-02,  1.7065e-02,  5.3818e-02,  7.4877e-02,\n",
       "          4.9402e-03, -2.0671e-01, -9.1580e-02, -5.4901e-02,  1.8985e-01,\n",
       "         -1.9487e-01, -6.2386e-02,  1.7930e-01,  4.8319e-02,  5.7782e-02,\n",
       "          2.7951e-02, -2.1810e-02, -1.2392e-03, -1.0058e-01, -2.0083e-01,\n",
       "          7.2662e-03, -2.0870e-01,  1.0220e-01,  9.7512e-03,  9.1475e-02,\n",
       "          5.1476e-02,  6.7957e-02, -9.1544e-02, -1.0610e-01, -1.6424e-01],\n",
       "        [ 1.9099e-01,  2.6697e-02,  2.2725e-01, -6.1813e-02,  1.7537e-01,\n",
       "         -1.9211e-01,  1.2067e-01,  1.4811e-01,  1.8093e-01, -1.3127e-01,\n",
       "         -1.8424e-01,  1.3390e-01,  3.5187e-02, -1.9368e-01,  1.3467e-01,\n",
       "         -4.9153e-02,  1.9594e-02, -1.3322e-01, -7.9679e-02, -1.6075e-01,\n",
       "          1.0105e-01, -1.8318e-01, -2.0479e-01,  7.9491e-02,  1.4551e-01,\n",
       "          4.9770e-03,  6.3438e-02,  1.2039e-01,  1.6457e-01,  2.2124e-01,\n",
       "          2.1411e-01,  1.4853e-01,  6.5635e-02,  9.2814e-02,  1.6803e-01,\n",
       "         -4.1271e-02,  1.6233e-01,  1.2442e-01, -2.2374e-01, -6.1373e-02,\n",
       "          1.7776e-01,  2.6982e-02,  3.0232e-02,  1.7135e-01, -1.8790e-01,\n",
       "         -5.0299e-02, -1.3882e-01,  1.9823e-01, -1.8827e-01,  1.0934e-01,\n",
       "          1.8577e-01,  1.1058e-01, -5.5237e-02,  1.9032e-01, -9.6072e-02,\n",
       "         -1.1016e-01,  7.8237e-03, -1.3567e-01, -1.0196e-01,  2.0485e-02,\n",
       "          1.0745e-01,  1.0247e-01, -4.5427e-02, -2.0221e-01, -9.9845e-02,\n",
       "          1.0558e-01, -1.2127e-01,  1.3528e-02,  1.3346e-01, -2.3861e-02,\n",
       "          1.2097e-01,  8.8084e-02, -9.4938e-02, -9.3577e-03, -2.2545e-01,\n",
       "          1.6041e-01,  1.7000e-01, -2.0841e-02, -1.1751e-01,  2.2280e-01,\n",
       "         -4.9673e-02,  1.2891e-01,  8.4365e-02, -1.5640e-01,  2.0924e-03,\n",
       "         -2.0566e-01,  2.1811e-01, -8.5892e-02, -1.3222e-01,  2.0179e-01,\n",
       "          1.0544e-01,  9.2537e-02,  2.2029e-01,  1.0855e-01,  1.0151e-01,\n",
       "          9.6548e-02, -1.6717e-01,  7.0564e-02,  1.9787e-01,  7.2209e-03],\n",
       "        [ 8.7263e-02, -3.0407e-02,  8.6344e-02,  1.6109e-01,  1.9990e-01,\n",
       "          1.6912e-01,  1.8709e-01, -9.4041e-02,  2.3274e-01,  6.9473e-02,\n",
       "          2.1213e-01, -1.9603e-01, -1.1533e-02, -2.1085e-01,  1.6528e-01,\n",
       "          1.6892e-01,  1.4287e-01,  7.5278e-02,  1.2033e-01,  2.1871e-01,\n",
       "         -1.8940e-01,  7.5539e-02, -1.7915e-01,  2.2839e-01, -1.6276e-01,\n",
       "         -1.8408e-01,  8.2048e-02,  1.3541e-01, -2.1408e-01,  4.1102e-02,\n",
       "          2.0894e-01, -4.2053e-02, -1.9302e-01,  1.5579e-01,  1.4381e-01,\n",
       "          7.1462e-02,  6.3249e-02,  1.7763e-01,  7.0968e-02,  2.0200e-01,\n",
       "          1.7472e-01,  1.1464e-01, -3.3135e-02, -1.9600e-01,  2.8911e-02,\n",
       "         -3.3728e-02,  1.6750e-01, -1.6457e-01,  8.8102e-02, -1.0337e-01,\n",
       "         -2.0544e-01,  6.8217e-02,  1.1651e-01, -1.0556e-01,  2.1558e-01,\n",
       "         -9.7338e-03, -7.8767e-02, -1.0679e-01,  5.2947e-03, -3.7304e-02,\n",
       "         -1.8002e-01,  1.8849e-01, -2.0330e-01, -2.3230e-01,  1.5510e-01,\n",
       "         -1.4786e-01, -1.0301e-01,  1.0941e-01,  1.5396e-01,  1.1671e-01,\n",
       "         -8.2132e-02, -2.2942e-01, -1.9637e-01, -2.0424e-01,  1.7878e-01,\n",
       "          1.8332e-01, -1.4989e-01, -1.4008e-01, -2.2293e-01, -2.2148e-01,\n",
       "          2.2706e-01,  1.4741e-01, -5.5899e-02, -2.2076e-01, -7.7846e-02,\n",
       "          6.8902e-02,  2.1812e-01,  1.4594e-01,  1.1380e-01, -8.2967e-02,\n",
       "          1.9289e-01,  1.1935e-01, -1.0242e-01,  1.0149e-01, -5.3499e-02,\n",
       "         -3.2862e-02, -1.6848e-01,  8.3771e-02,  4.4783e-02,  2.2046e-01],\n",
       "        [-4.7839e-03,  9.6326e-02,  1.9991e-01, -8.9409e-02, -1.9658e-01,\n",
       "         -1.5952e-01,  1.4663e-01,  8.0397e-02,  1.4074e-01, -3.0543e-02,\n",
       "          1.8651e-01,  1.3992e-01,  1.5467e-01,  6.3703e-02, -2.3044e-01,\n",
       "         -6.4079e-02,  3.0482e-02, -2.4411e-02, -1.1735e-02, -9.1368e-02,\n",
       "         -2.0955e-01,  1.4504e-01, -2.0569e-01, -1.1105e-03,  1.3741e-01,\n",
       "          3.3362e-02, -7.8515e-02, -5.9753e-02, -4.3942e-02, -1.4264e-01,\n",
       "          1.5410e-01,  1.3722e-01, -1.5967e-01,  2.3325e-02,  1.9937e-01,\n",
       "         -1.2827e-01, -2.2561e-01, -4.7075e-02,  3.3863e-02,  4.2492e-02,\n",
       "          1.5423e-01,  2.0505e-01, -7.7679e-02,  8.0706e-02, -1.5344e-01,\n",
       "         -1.5264e-01,  1.2672e-01,  2.1066e-01,  2.2786e-01,  4.7017e-02,\n",
       "         -2.0712e-01,  2.7131e-02,  4.2980e-02,  2.2626e-01, -1.2641e-01,\n",
       "         -1.3980e-01,  1.8236e-01,  1.8219e-01, -1.8141e-01, -1.9354e-01,\n",
       "          1.5630e-01, -1.5636e-01,  1.7270e-01,  2.1514e-01, -1.8256e-01,\n",
       "         -1.8251e-01,  8.6448e-02,  9.0063e-02, -6.5978e-02, -2.1489e-01,\n",
       "          1.8885e-01, -1.3851e-01, -1.2228e-01, -6.9801e-02, -9.0091e-02,\n",
       "          2.0720e-02,  1.1802e-01, -1.3412e-01, -6.1608e-02, -2.1993e-01,\n",
       "          2.0650e-01, -7.4755e-02,  1.6967e-01,  2.1975e-01, -2.0431e-01,\n",
       "         -1.1316e-01, -1.0958e-01,  2.5027e-02,  1.4294e-01,  7.4574e-02,\n",
       "         -4.6614e-02,  1.2197e-01, -1.6533e-01, -2.2265e-01, -8.4785e-02,\n",
       "          1.5651e-02,  4.6890e-02,  2.0852e-01, -1.4177e-01,  1.8988e-01],\n",
       "        [ 3.1900e-02,  1.4756e-01,  1.4329e-01,  2.0309e-01,  1.5575e-01,\n",
       "          2.3315e-02, -3.2308e-02, -3.2961e-02, -1.8298e-01,  2.2737e-01,\n",
       "         -1.6904e-01, -1.8951e-01, -2.1996e-01,  3.6003e-02, -1.4521e-01,\n",
       "         -7.2823e-02,  1.0226e-01,  1.5481e-01, -2.6798e-03, -1.5488e-01,\n",
       "          3.6168e-02,  1.3636e-01, -4.2075e-02, -9.1753e-02, -1.5443e-01,\n",
       "         -1.3541e-01, -1.6435e-01,  2.2942e-01, -2.1509e-01, -1.3349e-01,\n",
       "          4.3055e-02, -7.5731e-02, -7.5392e-02, -5.9660e-02, -1.6650e-02,\n",
       "         -6.6975e-03,  1.6512e-01, -1.8460e-01,  1.3697e-01, -1.7665e-01,\n",
       "          1.0339e-01,  7.9006e-02,  6.9464e-02, -3.3766e-02,  2.2523e-02,\n",
       "         -1.5334e-01, -3.1776e-02, -1.7793e-01, -7.4105e-02, -1.3052e-02,\n",
       "          7.9010e-02,  8.9601e-02,  1.1981e-01,  6.1522e-02, -3.1066e-02,\n",
       "          9.5648e-03,  2.2228e-02,  1.5198e-02, -1.9049e-01,  5.7781e-03,\n",
       "          3.2097e-02,  2.1510e-01,  8.5528e-02,  1.0888e-01,  2.2600e-01,\n",
       "         -1.7230e-01,  7.9957e-03,  4.4321e-02,  1.6416e-01, -7.7497e-02,\n",
       "          1.6480e-01,  5.4240e-02,  3.2224e-02,  1.2612e-01, -1.3677e-01,\n",
       "         -1.4436e-01,  1.3650e-01,  1.8975e-01, -1.0281e-01, -1.1866e-02,\n",
       "          5.7434e-02,  1.2753e-01, -1.1441e-01,  1.0852e-02,  6.7405e-02,\n",
       "          9.3430e-02, -1.9043e-02, -1.6419e-02,  1.4974e-01,  1.3030e-01,\n",
       "         -1.6491e-01,  1.8604e-01,  1.7350e-01,  3.8979e-02,  1.4700e-01,\n",
       "         -5.8885e-02,  5.4761e-02,  7.4164e-02,  1.2233e-01,  1.2728e-01],\n",
       "        [-1.4586e-01,  2.2148e-01, -2.2935e-01, -2.1912e-01, -1.1909e-01,\n",
       "         -1.5804e-02,  1.2368e-01, -4.1968e-02, -2.2464e-01,  4.1112e-03,\n",
       "          1.9760e-01, -4.4665e-02,  1.2101e-02, -2.0032e-01,  4.1459e-02,\n",
       "          2.2147e-01, -1.9498e-01, -9.0679e-02,  6.0111e-02, -2.3192e-01,\n",
       "          3.5411e-02,  1.0900e-01, -1.7306e-01,  1.5752e-01, -2.5701e-02,\n",
       "         -1.1375e-01,  1.8330e-01,  3.8190e-02,  1.6553e-02,  4.1169e-02,\n",
       "         -5.3842e-02, -2.3109e-01,  1.6616e-01, -8.6130e-02, -1.6438e-01,\n",
       "         -8.0415e-02, -2.1264e-02, -1.8469e-01, -8.8217e-02, -3.2509e-02,\n",
       "         -2.8462e-02,  2.0576e-01, -2.0443e-01,  1.3180e-01,  2.0651e-01,\n",
       "          6.8121e-03,  3.9050e-03, -2.1016e-01, -2.2753e-01, -2.2772e-01,\n",
       "         -7.4823e-02,  1.5969e-01,  5.6127e-02,  1.8721e-01,  2.3152e-01,\n",
       "         -8.2473e-02, -8.6482e-02,  1.2556e-01,  1.1198e-01,  3.3545e-02,\n",
       "          2.1550e-01,  9.6547e-02, -2.1075e-01, -2.1064e-01,  1.6576e-01,\n",
       "          1.8485e-01, -1.6650e-01,  2.0900e-01,  4.0146e-02,  3.3458e-02,\n",
       "          7.6816e-03,  1.5233e-01, -2.2909e-01, -3.3111e-02, -1.9224e-02,\n",
       "         -1.5915e-02,  1.6265e-01,  2.2510e-01, -5.6053e-02, -2.2314e-02,\n",
       "         -1.2097e-01,  1.7750e-01, -2.2449e-01, -2.5447e-02,  1.7076e-02,\n",
       "         -9.2250e-02,  7.3472e-02,  1.7742e-01, -1.7650e-01, -6.5576e-02,\n",
       "          2.1953e-01, -1.7910e-01,  1.6545e-01, -1.5098e-01, -9.2640e-02,\n",
       "         -2.3034e-01,  1.0570e-01, -1.8162e-01, -1.6473e-01, -1.8715e-01],\n",
       "        [-2.0460e-02, -2.0960e-01, -2.3231e-01, -1.6798e-01,  1.6402e-03,\n",
       "         -1.0606e-01, -7.0415e-03,  2.0331e-02,  3.4625e-02,  1.3822e-01,\n",
       "         -1.0403e-01,  9.5590e-02, -1.1220e-01, -1.8602e-01,  1.4373e-01,\n",
       "          1.4002e-01, -1.5289e-01, -8.3983e-02, -1.1280e-02, -1.0993e-01,\n",
       "          1.5358e-01,  1.8877e-01, -1.6617e-01, -3.1061e-02,  6.6061e-02,\n",
       "         -1.7813e-01,  9.9685e-02, -9.0518e-02,  1.7998e-01, -5.6021e-02,\n",
       "          1.4584e-01, -1.5808e-01,  1.9427e-01,  1.3550e-01,  9.3781e-02,\n",
       "          1.7151e-01,  1.6035e-01,  6.6821e-02,  1.4106e-01, -3.0678e-02,\n",
       "         -2.0451e-01, -9.2661e-02, -8.0333e-02,  2.1987e-01,  2.4937e-02,\n",
       "          2.3313e-01,  9.3207e-03,  6.2930e-02,  2.7065e-02,  4.6218e-02,\n",
       "          4.7244e-02,  1.0781e-01,  1.4598e-01,  2.2886e-01, -2.2282e-01,\n",
       "          1.8078e-01,  1.7771e-01, -2.0941e-01,  7.3214e-02,  1.9707e-01,\n",
       "          2.7371e-03,  6.8625e-02, -2.3072e-01,  2.0181e-01, -1.6219e-01,\n",
       "          1.6454e-01, -4.0790e-02,  1.2650e-01, -1.4498e-01, -1.7622e-01,\n",
       "         -1.6336e-01, -4.7030e-02,  9.4428e-02,  2.5360e-03, -1.0349e-01,\n",
       "          1.5930e-01, -1.0972e-01, -1.0502e-01,  1.9841e-01, -2.1632e-01,\n",
       "         -2.1276e-01, -1.1582e-01,  9.0023e-02,  2.8575e-02,  1.3975e-01,\n",
       "          6.8383e-02,  1.8282e-01,  1.4155e-01,  1.4121e-02,  1.3413e-03,\n",
       "          3.2841e-03,  5.1657e-02, -3.9996e-03, -3.3756e-02, -1.0971e-01,\n",
       "          1.1275e-01, -2.2947e-01,  1.8010e-01,  2.1065e-01, -1.0596e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xavier initialization을 이용하여 각 layer의 weight 초기화 \n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-Batch Normalization Layer - ReLU- DropOut)\n",
    "\n",
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout, \n",
    "                            linear2, bn2, relu, dropout, \n",
    "                            linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function 정의하기 (CrossEntropy를 사용할 것!)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 정의하기 (Adam optimizer를 사용할 것!)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost 계산을 위한 변수 설정 \n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.492597938\n",
      "Epoch: 0002 cost = 0.369442731\n",
      "Epoch: 0003 cost = 0.327968687\n",
      "Epoch: 0004 cost = 0.309517503\n",
      "Epoch: 0005 cost = 0.295981616\n",
      "Epoch: 0006 cost = 0.286550671\n",
      "Epoch: 0007 cost = 0.277565539\n",
      "Epoch: 0008 cost = 0.265243977\n",
      "Epoch: 0009 cost = 0.273790479\n",
      "Epoch: 0010 cost = 0.253159583\n",
      "Epoch: 0011 cost = 0.261577159\n",
      "Epoch: 0012 cost = 0.248263180\n",
      "Epoch: 0013 cost = 0.250646114\n",
      "Epoch: 0014 cost = 0.243508860\n",
      "Epoch: 0015 cost = 0.242987826\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "# Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것) \n",
    "for epoch in range(training_epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0  # cost 초기값 설정\n",
    "    \n",
    "    #train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드 \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28)\n",
    "        Y = Y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X) \n",
    "        cost = criterion(hypothesis, Y) \n",
    "        cost.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        avg_cost += cost / train_total_batch\n",
    "        \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bunga\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:60: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\bunga\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:50: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9381999969482422\n",
      "Label:  0\n",
      "Prediction:  0\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것 \n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval() #evaluation mode (검증하는 단계이므로 dropout 제외)\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test # prediction 값과 실제 test data 값이 같은가 (correct = 1)\n",
    "    accuracy = correct_prediction.float().mean() # 0 or 1 값들의 평균 >>> 정확도\n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    ##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드 \n",
    "    r = random.randint(0, len(mnist_test) - 1) # randint: 범위 내 임의의 정수(난수) 추출 \n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hidden Node 조작 시 차이점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 전체적으로 node 수를 늘렸을 경우\n",
    "#### 784 > 1000 > 1000 > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node 수 변경\n",
    "\n",
    "linear4 = torch.nn.Linear(784, 1000, bias=True)\n",
    "linear5 = torch.nn.Linear(1000, 1000, bias=True)\n",
    "linear6 = torch.nn.Linear(1000, 10, bias=True)\n",
    "\n",
    "bn3 = torch.nn.BatchNorm1d(1000)\n",
    "bn4 = torch.nn.BatchNorm1d(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0173, -0.0509,  0.0105,  ...,  0.0746, -0.0470,  0.0411],\n",
       "        [ 0.0601, -0.0477,  0.0480,  ...,  0.0228, -0.0266,  0.0731],\n",
       "        [ 0.0743,  0.0598,  0.0692,  ...,  0.0770,  0.0665, -0.0248],\n",
       "        ...,\n",
       "        [-0.0480, -0.0630, -0.0059,  ..., -0.0096,  0.0758, -0.0673],\n",
       "        [-0.0201, -0.0186, -0.0167,  ...,  0.0221, -0.0032,  0.0057],\n",
       "        [ 0.0279,  0.0049, -0.0100,  ...,  0.0506,  0.0142,  0.0616]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear4.weight)\n",
    "torch.nn.init.xavier_uniform_(linear5.weight)\n",
    "torch.nn.init.xavier_uniform_(linear6.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 모델 설정\n",
    "model2 = torch.nn.Sequential(linear4, bn3, relu, dropout, \n",
    "                            linear5, bn4, relu, dropout, \n",
    "                            linear6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.518427730\n",
      "Epoch: 0002 cost = 0.305316567\n",
      "Epoch: 0003 cost = 0.253975689\n",
      "Epoch: 0004 cost = 0.236607134\n",
      "Epoch: 0005 cost = 0.217563853\n",
      "Epoch: 0006 cost = 0.214124203\n",
      "Epoch: 0007 cost = 0.200371519\n",
      "Epoch: 0008 cost = 0.191649124\n",
      "Epoch: 0009 cost = 0.189661220\n",
      "Epoch: 0010 cost = 0.177701846\n",
      "Epoch: 0011 cost = 0.178726092\n",
      "Epoch: 0012 cost = 0.168122888\n",
      "Epoch: 0013 cost = 0.166439384\n",
      "Epoch: 0014 cost = 0.159359455\n",
      "Epoch: 0015 cost = 0.161055058\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "# Training epoch \n",
    "for epoch in range(training_epochs):\n",
    "    model2.train()\n",
    "    avg_cost = 0  # cost 초기값 설정\n",
    "    \n",
    "    #train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여 loss를 최적화하는 코드 \n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28)\n",
    "        Y = Y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model2(X) \n",
    "        cost = criterion(hypothesis, Y) \n",
    "        cost.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        avg_cost += cost / train_total_batch\n",
    "        \n",
    "        \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    \n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7526999711990356\n",
      "Label:  6\n",
      "Prediction:  6\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model2.eval() #evaluation mode (검증하는 단계이므로 dropout 제외)\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model2(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test \n",
    "    accuracy = correct_prediction.float().mean() \n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test) - 1) \n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model2(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 전체적으로 node 수를 줄였을 경우\n",
    "#### 784 > 50 > 50 > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node 수 변경\n",
    "\n",
    "linear7 = torch.nn.Linear(784, 50, bias=True)\n",
    "linear8 = torch.nn.Linear(50, 50, bias=True)\n",
    "linear9 = torch.nn.Linear(50, 10, bias=True)\n",
    "\n",
    "bn5 = torch.nn.BatchNorm1d(50)\n",
    "bn6 = torch.nn.BatchNorm1d(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2768, -0.0066,  0.2594,  0.0037,  0.0748, -0.0950,  0.2350, -0.0216,\n",
       "         -0.1150,  0.1747,  0.2733,  0.2373, -0.2357,  0.0113,  0.0312, -0.0127,\n",
       "          0.1781, -0.0041,  0.1235, -0.1992, -0.1354,  0.2730, -0.1826, -0.1496,\n",
       "          0.1826, -0.1633, -0.1312, -0.1416,  0.2052,  0.1681, -0.0636, -0.0413,\n",
       "         -0.1979,  0.1553, -0.1949, -0.0291,  0.2636,  0.2663,  0.1208,  0.1718,\n",
       "         -0.2483, -0.2745, -0.1082, -0.0256,  0.1525, -0.0417, -0.1570,  0.2243,\n",
       "          0.0455,  0.0081],\n",
       "        [ 0.0974, -0.2759,  0.3013,  0.2777,  0.2216, -0.2469,  0.2586,  0.1442,\n",
       "          0.0942,  0.2417,  0.2862, -0.0251, -0.1470,  0.0844, -0.2684, -0.2361,\n",
       "          0.1356, -0.0309,  0.2563, -0.1077, -0.1500,  0.1550,  0.0836, -0.1327,\n",
       "          0.1939, -0.2548,  0.0937,  0.0287,  0.1231, -0.0540,  0.1403,  0.0395,\n",
       "          0.3039, -0.1964,  0.2183,  0.0231, -0.1095, -0.0296, -0.0055, -0.1248,\n",
       "         -0.1033, -0.1588, -0.2953,  0.0908,  0.1779, -0.3046,  0.1231,  0.0594,\n",
       "          0.1646,  0.2493],\n",
       "        [ 0.1324, -0.0727, -0.1850, -0.2217, -0.0212,  0.0986,  0.1260,  0.1225,\n",
       "         -0.2117,  0.1470, -0.2484, -0.3008, -0.0207,  0.2779, -0.2677,  0.0472,\n",
       "          0.1263, -0.1560,  0.1445,  0.2366, -0.0426,  0.2102,  0.0777,  0.1499,\n",
       "         -0.2337,  0.3144, -0.0733,  0.2378,  0.3042, -0.2009,  0.1324, -0.0019,\n",
       "         -0.2593, -0.2698, -0.1053, -0.3095, -0.2838, -0.1011,  0.0346, -0.0775,\n",
       "         -0.1589,  0.2846, -0.1303, -0.1015,  0.1358, -0.1865, -0.2275, -0.1829,\n",
       "          0.3156, -0.2876],\n",
       "        [-0.1568, -0.0026, -0.0715,  0.2931, -0.0293,  0.2700, -0.2739,  0.2940,\n",
       "          0.3141,  0.1965, -0.0397, -0.3077,  0.0739,  0.2962,  0.2887,  0.0516,\n",
       "         -0.1067, -0.0617, -0.1082, -0.1645,  0.3114,  0.2986, -0.1697,  0.1205,\n",
       "         -0.2952,  0.3122, -0.2860, -0.1117, -0.1002, -0.2573, -0.0921, -0.2274,\n",
       "         -0.0989, -0.1674, -0.1055, -0.3007,  0.0251, -0.3066, -0.3124,  0.0102,\n",
       "          0.0763, -0.1297, -0.0896, -0.0222,  0.2486, -0.3104, -0.0377, -0.1296,\n",
       "         -0.2725, -0.2301],\n",
       "        [ 0.0278,  0.0350, -0.1696,  0.1950, -0.1097,  0.1780,  0.1657,  0.1578,\n",
       "         -0.0819, -0.0208,  0.1299,  0.3012,  0.1025,  0.0913,  0.2174, -0.2396,\n",
       "          0.2502, -0.0090, -0.2917,  0.2458,  0.2623,  0.0682, -0.1608, -0.1604,\n",
       "         -0.2054,  0.0954, -0.2154,  0.1149, -0.2160, -0.2592, -0.1325,  0.1683,\n",
       "         -0.2704,  0.0518,  0.0618, -0.2375,  0.0229,  0.2754,  0.1586, -0.2428,\n",
       "         -0.2576,  0.1085, -0.2194, -0.1742,  0.0206, -0.1338, -0.1950,  0.3155,\n",
       "          0.2079, -0.1361],\n",
       "        [ 0.2627, -0.1662,  0.0666,  0.2847, -0.1734,  0.2830, -0.2426,  0.0181,\n",
       "          0.1663, -0.1508, -0.0980, -0.2214,  0.0517,  0.2768,  0.2890, -0.2585,\n",
       "         -0.0075,  0.0852,  0.2611, -0.1634, -0.2277,  0.1057,  0.2629,  0.0201,\n",
       "          0.1979,  0.2901, -0.2647, -0.0393, -0.3002, -0.0936,  0.2698,  0.2299,\n",
       "         -0.1555,  0.2881,  0.1117,  0.2849, -0.0879,  0.2126, -0.1012,  0.2772,\n",
       "         -0.0744, -0.2769, -0.0781,  0.0659, -0.0390, -0.2311,  0.0393,  0.1904,\n",
       "         -0.2947,  0.1957],\n",
       "        [-0.0204, -0.1153, -0.0074,  0.0373,  0.2214,  0.2004, -0.2069, -0.2266,\n",
       "          0.3150, -0.1830,  0.1414,  0.1735, -0.2359,  0.2366, -0.1671, -0.3097,\n",
       "         -0.2766,  0.2310, -0.0733,  0.3025,  0.2649,  0.0783,  0.1035, -0.2297,\n",
       "         -0.0835,  0.0568, -0.2701, -0.1510, -0.1241, -0.0436,  0.2835,  0.0650,\n",
       "         -0.2709, -0.1250,  0.0414,  0.2421,  0.2959, -0.1210,  0.2577,  0.3063,\n",
       "          0.2238,  0.0551, -0.0968, -0.1978,  0.3065, -0.1617, -0.2938,  0.2310,\n",
       "         -0.2000, -0.2142],\n",
       "        [-0.1965, -0.2255, -0.0327, -0.1653,  0.0973,  0.0324,  0.2198,  0.2995,\n",
       "          0.0701, -0.0664, -0.2741,  0.0540,  0.0042,  0.1059,  0.0333, -0.0934,\n",
       "          0.0365,  0.0638,  0.2229,  0.2994, -0.1535, -0.2478, -0.1780, -0.1914,\n",
       "         -0.2287, -0.0069, -0.1837, -0.2268, -0.1524, -0.1617,  0.2517, -0.3150,\n",
       "         -0.1219, -0.0926,  0.1011,  0.0526,  0.2166,  0.1176,  0.2445,  0.2467,\n",
       "          0.2125, -0.2747,  0.0611, -0.1914,  0.2634, -0.3140,  0.1310, -0.2914,\n",
       "          0.0689, -0.1587],\n",
       "        [ 0.1173,  0.1008,  0.2868,  0.1432, -0.2740, -0.1473, -0.2803,  0.2997,\n",
       "          0.0492, -0.1165,  0.1029,  0.2802, -0.1242, -0.1937, -0.0014, -0.1104,\n",
       "         -0.1950, -0.2072,  0.1250,  0.2793, -0.0218,  0.3057,  0.1034, -0.1393,\n",
       "         -0.1690,  0.1932,  0.2158, -0.2171, -0.2958,  0.2531, -0.2522, -0.2300,\n",
       "          0.2222,  0.1772,  0.2207, -0.1686,  0.0568,  0.0248,  0.1281, -0.2300,\n",
       "          0.0613,  0.2454,  0.0782,  0.0256,  0.1391,  0.0157, -0.1931, -0.0986,\n",
       "         -0.1337, -0.0450],\n",
       "        [ 0.1268, -0.1586, -0.2068, -0.1321,  0.3103, -0.1527, -0.3149, -0.0577,\n",
       "         -0.0519,  0.2436,  0.2649,  0.2907, -0.2259, -0.1145, -0.0034, -0.2738,\n",
       "          0.1785, -0.1826, -0.1848, -0.0689, -0.1823,  0.2913, -0.1533, -0.1184,\n",
       "          0.2524,  0.2035, -0.2795, -0.1657, -0.1869, -0.0143, -0.2044, -0.1603,\n",
       "         -0.2448, -0.0526,  0.1056,  0.2650, -0.1932,  0.2037, -0.2375, -0.3046,\n",
       "          0.2018, -0.1430,  0.2484, -0.2166, -0.1179, -0.0850, -0.2799,  0.1396,\n",
       "         -0.1201,  0.2156]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.xavier_uniform_(linear7.weight)\n",
    "torch.nn.init.xavier_uniform_(linear8.weight)\n",
    "torch.nn.init.xavier_uniform_(linear9.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 모델 설정\n",
    "model3 = torch.nn.Sequential(linear7, bn5, relu, dropout, \n",
    "                            linear8, bn6, relu, dropout, \n",
    "                            linear9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.580256581\n",
      "Epoch: 0002 cost= 0.435751230\n",
      "Epoch: 0003 cost= 0.401764810\n",
      "Epoch: 0004 cost= 0.386084884\n",
      "Epoch: 0005 cost= 0.374100596\n",
      "Epoch: 0006 cost= 0.363532782\n",
      "Epoch: 0007 cost= 0.342887193\n",
      "Epoch: 0008 cost= 0.346728772\n",
      "Epoch: 0009 cost= 0.331339180\n",
      "Epoch: 0010 cost= 0.355457664\n",
      "Epoch: 0011 cost= 0.348862529\n",
      "Epoch: 0012 cost= 0.331119746\n",
      "Epoch: 0013 cost= 0.329321146\n",
      "Epoch: 0014 cost= 0.324384987\n",
      "Epoch: 0015 cost= 0.321096927\n",
      "Learning finished.\n"
     ]
    }
   ],
   "source": [
    "model3.train()\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X,Y in train_loader:\n",
    "        X = X.view(-1, 28*28)\n",
    "        Y = Y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model3(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        avg_cost += cost/ train_total_batch\n",
    "    \n",
    "    print('Epoch:','%04d' % (epoch+1),'cost=', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9430000185966492\n",
      "Label:  2\n",
      "Prediction:  2\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model3.eval() #evaluation mode (검증하는 단계이므로 dropout 제외)\n",
    "    \n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model3(X_test)\n",
    "    \n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test \n",
    "    accuracy = correct_prediction.float().mean() \n",
    "    print('Accuracy:', accuracy.item())\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test) - 1) \n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float()\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "    \n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = model3(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
