{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1-1)\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train과 test set으로 나누어 MNIST data 불러오기\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset loader에 train과 test할당하기(batch size, shuffle, drop_last 잘 설정할것!)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Layer 쌓기 (조건: 3개의 Layer 사용, DropOut 사용 (p=0.3), ReLU 함수 사용, Batch normalization하기)\n",
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,100), 2nd Layer(100,100),3rd Layer(100,10)\n",
    "linear1 = torch.nn.Linear(784, 100, bias=True)\n",
    "linear2 = torch.nn.Linear(100, 100, bias=True)\n",
    "linear3 = torch.nn.Linear(100, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = torch.nn.BatchNorm1d(100)\n",
    "bn2 = torch.nn.BatchNorm1d(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1705, -0.0645,  0.1800,  0.1802,  0.1703,  0.1303, -0.1934,  0.1475,\n",
       "          0.2066,  0.2283, -0.0985, -0.0142, -0.1564,  0.1342, -0.1575, -0.1771,\n",
       "         -0.2079,  0.0791,  0.0404,  0.1153, -0.1480,  0.1202,  0.2274,  0.2032,\n",
       "         -0.0060, -0.0896, -0.1560, -0.1778,  0.1009,  0.1711, -0.0967,  0.2232,\n",
       "         -0.0501,  0.2000,  0.0812,  0.1326, -0.2145, -0.0174,  0.1273,  0.1139,\n",
       "          0.1220,  0.2320, -0.0646, -0.2181,  0.0025,  0.0364,  0.1440,  0.2151,\n",
       "         -0.1932, -0.0106,  0.1983,  0.1032,  0.0936,  0.0068,  0.1349,  0.2009,\n",
       "         -0.0169, -0.1541, -0.0118,  0.0229, -0.2052, -0.1700,  0.0066,  0.0144,\n",
       "          0.1112,  0.1943, -0.0945, -0.0184, -0.1992, -0.1985,  0.0043,  0.2204,\n",
       "         -0.1416,  0.1688,  0.1050,  0.2257,  0.0783,  0.2126, -0.1226, -0.1962,\n",
       "          0.1485,  0.2098, -0.1196,  0.0221, -0.2192,  0.1278,  0.1387, -0.0675,\n",
       "         -0.0740, -0.0945,  0.1729,  0.0038, -0.1371,  0.0677,  0.2317, -0.2116,\n",
       "          0.0614, -0.0742,  0.0244,  0.2156],\n",
       "        [ 0.0994, -0.0819, -0.2097, -0.1648,  0.1632, -0.1068,  0.0345,  0.0605,\n",
       "         -0.1000, -0.0450, -0.1162, -0.1604,  0.1862,  0.1765,  0.1941,  0.1301,\n",
       "         -0.1359, -0.0549,  0.0408,  0.0898, -0.2291,  0.1287, -0.1408,  0.1849,\n",
       "         -0.0290, -0.1005, -0.1009, -0.1532, -0.2257, -0.2104, -0.2106, -0.0467,\n",
       "         -0.1929, -0.0836,  0.2130,  0.0110, -0.2170,  0.2131,  0.1057,  0.0844,\n",
       "         -0.1553,  0.2317, -0.0051, -0.0663,  0.2192,  0.1230, -0.1511, -0.2251,\n",
       "          0.1138, -0.2150, -0.0587, -0.2311,  0.2040, -0.0113, -0.0570,  0.0478,\n",
       "         -0.1577, -0.1241, -0.0479, -0.1712,  0.0388,  0.1624, -0.2165, -0.1199,\n",
       "          0.0711,  0.0315,  0.0317,  0.1172, -0.2220, -0.2330,  0.0720,  0.2321,\n",
       "         -0.0055, -0.0731, -0.0288,  0.1536,  0.2176,  0.2260,  0.1301,  0.1341,\n",
       "         -0.2269, -0.0625,  0.2150, -0.1692,  0.0853,  0.0321,  0.0526, -0.1160,\n",
       "         -0.0687,  0.1129,  0.1213,  0.1797,  0.0510, -0.0062, -0.1125,  0.1656,\n",
       "          0.0004,  0.0383, -0.0114, -0.0430],\n",
       "        [-0.0425,  0.1530,  0.0791,  0.1875, -0.0505, -0.1555,  0.1422,  0.0186,\n",
       "         -0.1776,  0.0327,  0.0251,  0.1714,  0.0979,  0.0966,  0.1254,  0.0903,\n",
       "         -0.0988,  0.0369,  0.1082, -0.0567, -0.1454,  0.0659,  0.0566, -0.0594,\n",
       "          0.1086,  0.2134, -0.0713, -0.1367, -0.0332,  0.0667,  0.2200,  0.0931,\n",
       "         -0.1593, -0.0711,  0.0523, -0.2130, -0.1369,  0.0205, -0.1363,  0.2178,\n",
       "          0.1183, -0.0470,  0.0546, -0.1178,  0.0211,  0.1264,  0.0484,  0.2115,\n",
       "          0.1978,  0.0050, -0.2133, -0.2023, -0.2076,  0.0704,  0.0540, -0.0332,\n",
       "          0.1464,  0.0213,  0.0134, -0.0615,  0.2233, -0.1195,  0.0129,  0.1308,\n",
       "          0.2292,  0.0913, -0.0871,  0.0706, -0.2249, -0.1761,  0.0531,  0.2240,\n",
       "          0.2234, -0.0118,  0.2230, -0.1430, -0.0165, -0.2059, -0.1336,  0.0439,\n",
       "          0.1121,  0.1093, -0.0895,  0.0801, -0.1612, -0.1925, -0.2330,  0.1253,\n",
       "         -0.0194,  0.2193,  0.1354,  0.0036,  0.2115, -0.1358, -0.2132, -0.0757,\n",
       "         -0.0788, -0.0693,  0.0813, -0.0069],\n",
       "        [-0.0176, -0.1651, -0.1145,  0.1735, -0.1506, -0.1810, -0.1263,  0.2179,\n",
       "         -0.2000,  0.2246,  0.1188, -0.2093,  0.1209, -0.0034,  0.2131,  0.0709,\n",
       "          0.1468,  0.0291, -0.1791,  0.2310, -0.0068,  0.1391, -0.1619,  0.2203,\n",
       "          0.0618,  0.1431,  0.1807, -0.0101,  0.0639, -0.1882, -0.0805, -0.1840,\n",
       "         -0.2011,  0.2157,  0.1635, -0.1744, -0.0722,  0.0448,  0.1242, -0.0361,\n",
       "          0.1887,  0.2173, -0.2163, -0.2309, -0.1291, -0.1835,  0.1710,  0.0779,\n",
       "          0.2048, -0.0102, -0.0057, -0.0613,  0.0650,  0.2037,  0.0077, -0.0305,\n",
       "          0.1613, -0.1469, -0.1313, -0.2242, -0.2040,  0.1313,  0.0283, -0.1208,\n",
       "         -0.0354, -0.0159,  0.2136,  0.0670, -0.1949, -0.0094, -0.1403, -0.1290,\n",
       "         -0.1379, -0.1455,  0.0772,  0.0831, -0.1016, -0.0138,  0.1573,  0.0765,\n",
       "         -0.1046,  0.1966, -0.0282, -0.0869,  0.1177, -0.2174,  0.0512,  0.0063,\n",
       "         -0.1949,  0.1527, -0.2333, -0.1487,  0.1457, -0.1554, -0.0994, -0.1983,\n",
       "         -0.0823,  0.0765, -0.0275, -0.1955],\n",
       "        [ 0.0480, -0.0640,  0.1161,  0.2292, -0.0044,  0.1664, -0.0416,  0.0756,\n",
       "         -0.1631,  0.1289,  0.1487,  0.0893, -0.2044,  0.0579,  0.0838,  0.0058,\n",
       "          0.2158, -0.1125, -0.1331, -0.1285,  0.0401,  0.1654, -0.0429,  0.0131,\n",
       "         -0.0936,  0.1360,  0.1946,  0.0055, -0.0957,  0.0439, -0.0576,  0.1118,\n",
       "         -0.0230,  0.0124, -0.0793,  0.0139, -0.2215, -0.0028,  0.1065, -0.0152,\n",
       "          0.1933, -0.1126,  0.0078, -0.0250, -0.0323,  0.1119,  0.0995,  0.2029,\n",
       "         -0.2033,  0.1552, -0.2310,  0.0394,  0.1048,  0.2090, -0.0448,  0.1157,\n",
       "         -0.1097, -0.0074,  0.0964, -0.1796, -0.2064,  0.0666,  0.1733, -0.2135,\n",
       "         -0.1333,  0.0524,  0.0122,  0.0435, -0.0836,  0.1966, -0.1974,  0.0894,\n",
       "         -0.1253,  0.0700, -0.1967, -0.0418, -0.1588,  0.0142,  0.2249, -0.0011,\n",
       "         -0.0058, -0.0127, -0.1314,  0.0418, -0.1207, -0.1275,  0.1902, -0.0342,\n",
       "          0.2014,  0.2305, -0.1946, -0.0810, -0.1721, -0.0573,  0.0935,  0.0627,\n",
       "          0.0334, -0.2288,  0.0329, -0.0971],\n",
       "        [ 0.0202,  0.0164, -0.1973, -0.1315,  0.0824, -0.2091, -0.1331, -0.2226,\n",
       "          0.0650, -0.1606, -0.0643, -0.1384,  0.0906, -0.0494,  0.0674, -0.0133,\n",
       "          0.1202, -0.1020,  0.1130, -0.0491, -0.0089,  0.1335, -0.1704,  0.0319,\n",
       "         -0.1107, -0.1331,  0.1266, -0.0688,  0.0072, -0.0497,  0.0187, -0.0556,\n",
       "         -0.2268, -0.0408,  0.0086, -0.1924, -0.1156, -0.0864,  0.1294,  0.0905,\n",
       "          0.1160, -0.0524,  0.1078,  0.1704, -0.1001,  0.1538, -0.0213, -0.2043,\n",
       "         -0.1146, -0.0997, -0.0651,  0.1838,  0.2011,  0.2045, -0.1843, -0.0083,\n",
       "          0.2243, -0.1097,  0.1040, -0.2305,  0.0846, -0.1287, -0.0132,  0.2312,\n",
       "          0.2172, -0.0655, -0.1621, -0.0093, -0.0078,  0.1450,  0.2127,  0.2139,\n",
       "         -0.0873, -0.1689, -0.1469, -0.0950,  0.0806,  0.2325,  0.1459,  0.0577,\n",
       "         -0.1923,  0.1455, -0.0020, -0.0577, -0.0423, -0.0175,  0.1290, -0.0289,\n",
       "         -0.1786,  0.1916,  0.2021,  0.0386, -0.0834, -0.1872, -0.0100,  0.2257,\n",
       "         -0.1299, -0.0091,  0.0693, -0.1692],\n",
       "        [ 0.0548, -0.1336, -0.1146,  0.1051,  0.0972,  0.1521, -0.0160, -0.0279,\n",
       "          0.1058,  0.2232, -0.1649, -0.2241, -0.0391, -0.0097, -0.0168,  0.0176,\n",
       "         -0.0766, -0.0455,  0.1729, -0.1161,  0.2285,  0.1638, -0.0338,  0.2214,\n",
       "         -0.0093,  0.0021, -0.0807, -0.0251,  0.1523,  0.0978, -0.0518,  0.2220,\n",
       "          0.2278, -0.0273, -0.0230, -0.0928, -0.0131,  0.1779,  0.1878,  0.2212,\n",
       "          0.0355,  0.0266, -0.1612,  0.0252, -0.2329, -0.1578,  0.0969,  0.1835,\n",
       "         -0.0252,  0.0118, -0.1324, -0.1819, -0.1443, -0.0970, -0.1204, -0.1060,\n",
       "          0.1926,  0.1065, -0.1401,  0.0626,  0.0322,  0.1012,  0.0616,  0.0705,\n",
       "         -0.1885,  0.1053,  0.0606, -0.2221, -0.0869, -0.0750, -0.0710, -0.1626,\n",
       "          0.0311,  0.2276, -0.1857,  0.0177,  0.0476, -0.2058, -0.0636,  0.1799,\n",
       "         -0.0907, -0.0412,  0.0208,  0.0097,  0.1629,  0.0415,  0.0997,  0.1087,\n",
       "          0.0851, -0.1384,  0.0831, -0.0252,  0.1926, -0.1495,  0.1639,  0.1152,\n",
       "         -0.1704,  0.0492, -0.0183,  0.0983],\n",
       "        [ 0.1389, -0.2215, -0.0367,  0.1588,  0.1343, -0.0022, -0.1924,  0.2215,\n",
       "          0.1296, -0.1694,  0.1600,  0.0984,  0.0172, -0.0427,  0.1871,  0.1962,\n",
       "         -0.2160, -0.0403,  0.0051,  0.0258,  0.2187,  0.1352,  0.1127, -0.1904,\n",
       "          0.0219, -0.1372, -0.2039,  0.1452,  0.1886, -0.0641, -0.0958, -0.0976,\n",
       "         -0.0319,  0.1647, -0.1869, -0.2061, -0.1840,  0.1258, -0.1646, -0.0427,\n",
       "          0.1018, -0.1778,  0.1109,  0.0755, -0.1681, -0.2012,  0.0281, -0.2289,\n",
       "         -0.1022, -0.1773, -0.1472, -0.1384, -0.0892, -0.0125,  0.0963,  0.0126,\n",
       "         -0.0944, -0.0280,  0.0654, -0.1340,  0.0972, -0.0489,  0.1944, -0.1721,\n",
       "         -0.0974, -0.1797, -0.0316,  0.2166,  0.0471, -0.1542, -0.0859, -0.0245,\n",
       "          0.1370, -0.1033,  0.0359, -0.1010,  0.0291, -0.0778,  0.2257,  0.1883,\n",
       "          0.1521,  0.1944, -0.0542,  0.1515,  0.0122, -0.0388,  0.1058,  0.0303,\n",
       "         -0.0989,  0.1568, -0.0036, -0.1600, -0.0128,  0.1272,  0.0974,  0.0083,\n",
       "          0.2196,  0.1739, -0.0648, -0.0234],\n",
       "        [-0.0355, -0.0912, -0.0555,  0.0301,  0.0443,  0.0203, -0.0675, -0.0123,\n",
       "          0.2116, -0.2179, -0.1163,  0.1983, -0.0931,  0.0071,  0.1215,  0.1900,\n",
       "          0.1528, -0.1676, -0.1165,  0.1363, -0.1238, -0.1586,  0.1824,  0.0966,\n",
       "         -0.0500, -0.0668, -0.0075, -0.0419, -0.0142,  0.0984, -0.0445, -0.2066,\n",
       "          0.0121, -0.0984,  0.0852, -0.1494, -0.1215, -0.0152,  0.0973, -0.1640,\n",
       "         -0.1764,  0.0081, -0.0031, -0.0112,  0.1672,  0.1297, -0.0895, -0.1262,\n",
       "         -0.1928,  0.2294, -0.0929,  0.0964,  0.0075,  0.0688,  0.0470, -0.0911,\n",
       "          0.1548,  0.0236, -0.1268, -0.0238,  0.2083,  0.2018, -0.1217, -0.0372,\n",
       "         -0.1928,  0.2208, -0.1504,  0.2267, -0.0160, -0.0361,  0.0035,  0.0331,\n",
       "          0.2308, -0.2113, -0.0513,  0.1071, -0.0265,  0.2107, -0.1461,  0.1835,\n",
       "         -0.1171, -0.0227,  0.0334, -0.2115, -0.1336,  0.1967, -0.0689,  0.1262,\n",
       "         -0.0874, -0.0106, -0.0884,  0.1879, -0.0380,  0.0336, -0.1008, -0.1579,\n",
       "          0.0811, -0.1757,  0.1238, -0.1716],\n",
       "        [ 0.0143, -0.0920,  0.0384, -0.0672,  0.1711,  0.1691,  0.0741, -0.1327,\n",
       "         -0.0401,  0.0474,  0.0656,  0.0079,  0.1299,  0.0684, -0.0919, -0.1274,\n",
       "          0.0777, -0.0013, -0.1304, -0.0944, -0.2188, -0.1449, -0.1977,  0.1668,\n",
       "         -0.0717, -0.0845, -0.0609, -0.0201, -0.0624, -0.0837,  0.0792, -0.2143,\n",
       "         -0.0667,  0.0713, -0.0042,  0.1700, -0.1885, -0.2101, -0.0149, -0.1600,\n",
       "         -0.1653,  0.1346, -0.0528,  0.1208, -0.1288, -0.2097, -0.1133, -0.1887,\n",
       "          0.1168, -0.2278, -0.0945,  0.1051, -0.0139, -0.2164,  0.1104,  0.0118,\n",
       "          0.0945, -0.2297,  0.1587,  0.2034, -0.0674,  0.2247, -0.1326, -0.2179,\n",
       "          0.0692,  0.0109,  0.2046, -0.0428,  0.0537,  0.0812,  0.1525,  0.2157,\n",
       "         -0.1410, -0.1249,  0.0359,  0.0400, -0.1689, -0.0923, -0.2198, -0.1365,\n",
       "         -0.0824, -0.0706, -0.0343,  0.0079,  0.0226, -0.2081, -0.0145,  0.1519,\n",
       "          0.1652, -0.1613,  0.2215,  0.2228, -0.1520,  0.1563, -0.0761, -0.0470,\n",
       "          0.0614, -0.1251, -0.1072, -0.0121]], requires_grad=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-BatchNormalization Layer - ReLU- DropOut)\n",
    "model = torch.nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                            linear2, bn2, relu, dropout,\n",
    "                            linear3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.335887700\n",
      "Epoch: 0002 cost = 0.313184172\n",
      "Epoch: 0003 cost = 0.295466810\n",
      "Epoch: 0004 cost = 0.282583892\n",
      "Epoch: 0005 cost = 0.281316608\n",
      "Epoch: 0006 cost = 0.280670136\n",
      "Epoch: 0007 cost = 0.269507557\n",
      "Epoch: 0008 cost = 0.259180784\n",
      "Epoch: 0009 cost = 0.263238132\n",
      "Epoch: 0010 cost = 0.249530151\n",
      "Epoch: 0011 cost = 0.247522503\n",
      "Epoch: 0012 cost = 0.239278078\n",
      "Epoch: 0013 cost = 0.234324142\n",
      "Epoch: 0014 cost = 0.229313299\n",
      "Epoch: 0015 cost = 0.230627790\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model.train() #모델의 train 설정\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여loss를 최적화하는 코드\n",
    "\n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28)\n",
    "        Y = Y\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9416999816894531\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  2\n",
      "Prediction:  2\n"
     ]
    }
   ],
   "source": [
    "##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드\n",
    "r = random.randint(0, len(mnist_test)-1)\n",
    "X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "print('Label: ', Y_single_data.item())\n",
    "single_prediction = model(X_single_data)\n",
    "print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 Layer의 Hidden node 수 : 1st Layer (784,200), 2nd Layer(200,150),3rd Layer(150,10)\n",
    "linear4 = torch.nn.Linear(784, 200, bias=True)\n",
    "linear5 = torch.nn.Linear(200, 150, bias=True)\n",
    "linear6 = torch.nn.Linear(150, 10, bias=True)\n",
    "relu = torch.nn.ReLU()\n",
    "dropout = torch.nn.Dropout(p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn3 = torch.nn.BatchNorm1d(200)\n",
    "bn4 = torch.nn.BatchNorm1d(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.4672e-01,  1.5654e-01, -1.6575e-01,  1.4880e-01,  1.8373e-02,\n",
       "          1.3713e-01, -8.7996e-02, -1.3954e-01, -6.6759e-02,  1.2603e-01,\n",
       "          8.9636e-03, -7.9346e-03,  4.9936e-02, -2.0084e-01, -4.1163e-03,\n",
       "         -4.0050e-02, -2.2770e-01,  1.1679e-01,  1.4914e-02,  1.0584e-01,\n",
       "          2.2078e-01,  1.5680e-01, -4.1690e-03,  1.2568e-01,  1.2713e-01,\n",
       "         -1.1062e-01,  6.4293e-02, -1.3294e-01,  1.3039e-01,  1.5983e-01,\n",
       "          1.4998e-01, -1.7174e-01, -1.0874e-01, -5.2801e-02, -1.9846e-01,\n",
       "          7.0162e-02,  6.8782e-02, -2.2479e-01, -3.0747e-02, -9.4651e-02,\n",
       "          1.0536e-01, -7.3873e-02, -1.2459e-01, -1.7649e-02, -1.2496e-01,\n",
       "          2.1045e-01, -5.7543e-02,  1.2448e-01,  1.9021e-01,  2.1779e-01,\n",
       "         -9.9571e-02, -1.0864e-01,  1.6517e-01, -1.3926e-01,  1.3346e-01,\n",
       "          1.0061e-01, -2.6361e-02, -2.0429e-01, -2.2332e-02, -7.4129e-03,\n",
       "         -9.1403e-02, -2.2731e-01, -7.7730e-02, -9.2895e-02,  1.0127e-01,\n",
       "         -6.0260e-02, -6.8571e-02, -2.2844e-01, -2.1241e-01, -2.2865e-01,\n",
       "          1.8761e-01,  2.2140e-01, -1.3762e-01, -3.2759e-03, -2.5023e-03,\n",
       "          9.4734e-02, -1.0958e-01, -1.6497e-01, -2.1663e-01,  1.0762e-01,\n",
       "          7.6862e-02,  2.5026e-02,  4.9933e-02, -9.8135e-02, -1.2724e-01,\n",
       "          3.2955e-02, -1.5800e-01,  1.0967e-01, -4.9246e-02,  5.2033e-02,\n",
       "          4.0292e-02, -9.4844e-03, -4.3909e-03, -1.2612e-01,  1.9974e-01,\n",
       "         -1.1745e-01, -2.2539e-01,  2.3086e-01, -6.5499e-02, -1.4407e-02],\n",
       "        [ 4.4758e-02,  1.7641e-01,  2.0038e-02,  2.0557e-01,  3.6721e-02,\n",
       "          8.9119e-02, -5.8379e-02,  1.0138e-01, -2.1297e-01,  1.6229e-01,\n",
       "         -6.1363e-02, -8.5978e-02,  2.1191e-01, -2.2986e-01,  6.8372e-02,\n",
       "         -1.7284e-01,  1.5606e-01, -1.3854e-01,  1.2286e-01,  2.1202e-02,\n",
       "          1.7036e-01,  1.0433e-02,  3.3855e-02,  1.3466e-01, -6.7802e-04,\n",
       "         -5.0039e-02, -1.3197e-01,  1.3571e-01,  1.6964e-01, -1.6017e-01,\n",
       "         -2.0344e-01, -1.9497e-01,  8.8137e-02,  2.0839e-01, -1.9157e-01,\n",
       "         -2.0829e-01, -1.5021e-01, -2.0632e-01,  1.1313e-01, -2.1057e-01,\n",
       "         -1.1902e-01, -1.7651e-01, -4.4490e-02, -1.7526e-01,  8.4527e-02,\n",
       "          1.4970e-01, -1.2422e-01, -1.0305e-01, -1.9440e-01, -1.7125e-01,\n",
       "         -8.7792e-02,  2.4927e-02, -2.3201e-01,  1.0626e-01,  2.3016e-01,\n",
       "         -9.7882e-02,  1.3904e-01, -1.7591e-01, -1.2517e-01, -9.3334e-02,\n",
       "          2.2710e-01, -3.1856e-03,  1.3387e-01,  6.1987e-02, -8.0879e-03,\n",
       "         -1.4540e-01, -3.9624e-03, -2.3291e-01,  8.0099e-02,  2.3252e-02,\n",
       "         -5.7779e-02, -1.4663e-01,  1.9731e-01, -2.2417e-01, -1.2397e-01,\n",
       "          9.4282e-02, -5.3243e-02, -3.2085e-02,  1.3609e-01, -6.9557e-02,\n",
       "         -5.8211e-02,  1.8080e-01,  2.3192e-01,  1.4717e-01, -2.2531e-01,\n",
       "          1.7068e-02,  3.7956e-02, -1.2040e-01, -9.3607e-02, -1.6184e-01,\n",
       "          6.6257e-02, -5.8466e-02, -9.8838e-02,  1.7733e-01, -8.9192e-02,\n",
       "         -2.9688e-02,  2.1493e-01, -1.7936e-01,  9.0391e-03, -3.6771e-02],\n",
       "        [ 1.0476e-01,  1.8727e-01, -2.2355e-01,  2.1266e-01, -2.0318e-01,\n",
       "         -5.9286e-02, -2.1644e-01, -2.6703e-02,  3.8096e-02, -1.5480e-02,\n",
       "          1.3368e-01,  1.5669e-01,  8.7092e-02,  1.3585e-01, -1.5690e-01,\n",
       "         -1.1208e-01,  2.0366e-01,  1.1706e-01, -8.6343e-02,  2.5268e-03,\n",
       "         -1.8169e-01, -1.8699e-01,  2.0272e-01,  5.5658e-02, -1.2396e-01,\n",
       "         -1.1626e-01,  4.8840e-02, -4.7182e-02,  1.9124e-01,  8.8370e-03,\n",
       "         -1.0838e-01, -4.0843e-02, -2.2192e-01,  2.1793e-01,  1.2740e-01,\n",
       "         -2.6505e-02,  2.2179e-01,  1.1306e-01,  1.8511e-01,  1.4498e-01,\n",
       "          7.3986e-02,  1.1064e-01, -7.8152e-02, -2.1775e-01, -2.1969e-01,\n",
       "         -3.4687e-02, -1.8090e-01,  2.2951e-01,  2.1404e-01, -1.8621e-01,\n",
       "          1.9000e-01, -1.2930e-01, -6.9127e-02,  2.1639e-01, -2.1389e-01,\n",
       "         -1.9213e-01,  1.5144e-01,  1.5526e-01, -2.1311e-02,  8.3465e-02,\n",
       "          8.0028e-02,  8.2307e-02,  6.4605e-02, -2.1282e-01, -1.5298e-01,\n",
       "         -1.0356e-01,  1.2625e-01,  2.1191e-01,  2.0523e-01, -2.2480e-01,\n",
       "         -9.0677e-02, -1.4672e-01, -9.6502e-02,  1.1720e-01,  5.2870e-02,\n",
       "          9.9353e-03,  9.5205e-02,  1.7494e-01, -1.6210e-01,  1.3968e-01,\n",
       "          3.3386e-02, -1.5120e-01, -9.3644e-02, -9.0388e-02, -6.5774e-02,\n",
       "         -2.0952e-01,  4.8600e-02,  2.0149e-02,  1.2746e-01, -2.4996e-02,\n",
       "          6.5452e-02, -5.4280e-02, -2.3221e-02,  2.0148e-01, -2.1460e-01,\n",
       "         -1.1647e-01,  1.4144e-01,  1.7576e-01, -2.1320e-01, -3.2269e-02],\n",
       "        [ 2.0334e-01, -5.4224e-03, -2.1168e-01, -2.2088e-01, -2.0221e-01,\n",
       "          1.6071e-01, -1.7164e-01, -5.5653e-02, -9.1812e-02,  2.0880e-01,\n",
       "         -1.6004e-01,  4.1732e-02, -1.3053e-01, -1.3990e-01,  1.0761e-01,\n",
       "         -1.6280e-01, -1.8716e-01, -1.9784e-01, -1.6051e-01,  3.6710e-02,\n",
       "          9.4446e-02,  7.6322e-02,  1.4910e-01, -1.9007e-01,  5.6179e-02,\n",
       "         -1.8178e-01, -1.2459e-01, -1.3105e-01, -2.2786e-01,  9.8942e-02,\n",
       "         -1.9899e-01,  2.0566e-01, -1.1530e-01, -8.2983e-02,  1.9286e-01,\n",
       "          1.7511e-02, -1.6197e-01, -1.1276e-01, -5.8519e-02, -1.6533e-01,\n",
       "          1.8434e-01,  2.7803e-04,  2.5281e-02, -1.4628e-01,  9.7092e-02,\n",
       "          5.7363e-02,  7.4672e-02,  1.1328e-01, -1.1028e-01,  2.1068e-01,\n",
       "         -2.2739e-01, -2.2281e-01,  7.9202e-02, -5.6243e-02,  1.3209e-01,\n",
       "          1.5426e-01,  1.2051e-01, -1.4524e-01, -2.7978e-02,  8.5750e-02,\n",
       "         -1.3470e-01, -2.1306e-01,  8.9624e-02, -4.7093e-03,  9.7321e-02,\n",
       "         -7.6034e-02,  1.6076e-01,  1.5979e-01,  7.7953e-02, -1.3698e-01,\n",
       "          9.9603e-02, -2.2327e-01,  1.6955e-01,  2.0862e-01, -8.0613e-02,\n",
       "          8.1359e-02,  3.0094e-03,  1.7865e-01, -9.4061e-03,  1.2329e-01,\n",
       "          7.9986e-02,  6.2767e-02,  7.3806e-02, -2.2466e-01,  2.3203e-01,\n",
       "          8.8068e-02,  2.2574e-01,  1.9941e-01, -1.1984e-01, -1.0877e-01,\n",
       "          1.0162e-01, -1.8934e-01,  8.0330e-02,  2.0380e-02,  7.9732e-02,\n",
       "         -8.8386e-02, -1.2595e-01, -3.3305e-03,  1.3210e-01, -1.6476e-01],\n",
       "        [-1.8795e-01,  5.1699e-02, -1.6812e-01, -8.1552e-03,  1.3855e-01,\n",
       "          1.9451e-01, -3.0324e-02,  1.9777e-01, -1.1131e-01, -1.0482e-01,\n",
       "          2.2661e-01,  2.1441e-01,  1.5275e-01, -4.7533e-02,  2.1691e-01,\n",
       "          1.5226e-01,  8.7491e-02, -1.9024e-01, -5.1760e-02, -6.5398e-02,\n",
       "         -1.8272e-01, -4.2268e-02,  7.9481e-02, -1.8806e-01, -4.8930e-02,\n",
       "          7.2814e-02, -2.0930e-01, -1.0229e-01,  2.0557e-01,  1.4573e-01,\n",
       "          2.0456e-01,  2.1219e-01, -1.3944e-01,  5.5530e-02, -1.4097e-01,\n",
       "          5.3965e-02,  1.1064e-01,  2.0484e-01, -7.9832e-02, -1.5263e-02,\n",
       "         -4.2996e-02, -6.8289e-02,  1.8331e-01, -1.5256e-01,  1.5709e-01,\n",
       "         -1.8134e-01,  7.9908e-02,  2.0189e-01, -1.8706e-01, -1.7417e-01,\n",
       "         -1.9624e-01, -2.2420e-01,  1.1681e-01, -1.6043e-01, -2.1791e-01,\n",
       "          3.4526e-02, -5.1213e-02, -2.1308e-01, -1.9719e-01, -1.0538e-01,\n",
       "         -1.1898e-01,  6.4259e-03, -6.3055e-02, -5.3371e-02,  9.5469e-02,\n",
       "         -1.1818e-01,  1.9153e-01, -1.1997e-01,  9.3416e-02, -2.0801e-01,\n",
       "         -8.5154e-02,  8.0115e-02,  3.3282e-02, -5.7728e-02,  2.1435e-01,\n",
       "          9.2974e-02,  1.9482e-01,  2.2664e-01, -1.6877e-01, -6.8644e-02,\n",
       "          1.5019e-01, -7.6890e-02, -3.2830e-02,  2.2694e-01, -2.1151e-02,\n",
       "          1.5325e-01, -1.4158e-01, -1.6155e-01, -6.2556e-02,  2.0730e-01,\n",
       "          6.1933e-02,  2.1056e-01,  6.6489e-02, -8.0920e-02, -2.1250e-01,\n",
       "          1.6718e-01, -2.0388e-01, -1.0469e-01,  1.8203e-01, -4.2535e-02],\n",
       "        [ 1.8525e-01, -1.3496e-01,  1.1786e-01, -3.4900e-03,  1.0915e-01,\n",
       "          3.6814e-02, -9.8127e-02,  2.1382e-01, -2.0265e-02,  8.3069e-02,\n",
       "          1.4304e-01, -1.4682e-01,  2.3147e-01, -2.9936e-02, -1.2121e-01,\n",
       "          7.6292e-02, -2.1921e-01, -1.5723e-01,  4.5498e-02,  1.2251e-01,\n",
       "         -2.0813e-01,  2.1084e-01, -2.7615e-03, -1.4643e-01,  1.3521e-01,\n",
       "         -2.2432e-01,  1.2278e-01,  8.7809e-02,  1.3883e-01,  5.4925e-02,\n",
       "          1.5500e-01,  1.5162e-01, -1.0667e-01, -1.4586e-01, -1.9130e-01,\n",
       "         -1.6544e-01, -1.9209e-01, -1.2793e-01,  1.4394e-01, -2.6606e-02,\n",
       "          1.5938e-01, -8.2031e-02,  2.3203e-02, -1.9339e-02, -1.2301e-01,\n",
       "         -6.8367e-02,  3.7430e-02,  1.2700e-01,  9.6141e-02, -1.2857e-01,\n",
       "         -3.1835e-02, -4.7791e-02,  1.7674e-01, -1.3996e-01,  4.9032e-02,\n",
       "          1.6323e-01, -1.3982e-01, -1.5058e-01, -2.0468e-01,  1.2747e-02,\n",
       "          6.2057e-02,  1.0392e-01,  1.2230e-01,  1.2670e-01, -1.9685e-01,\n",
       "         -1.1079e-01, -9.4238e-02,  2.1102e-01,  2.9112e-02, -1.9144e-01,\n",
       "         -2.1196e-01, -1.5642e-01, -1.6533e-01, -6.3406e-02,  3.5979e-02,\n",
       "         -2.2754e-01,  9.9114e-02, -4.8258e-02, -2.1946e-01,  1.7541e-01,\n",
       "         -2.8867e-02, -3.6487e-02,  9.7329e-02,  1.3059e-01, -4.9674e-03,\n",
       "         -7.2676e-02, -7.0165e-02, -8.4195e-02,  2.8335e-02,  2.0021e-03,\n",
       "         -1.2416e-02, -6.2698e-02, -1.9208e-01,  1.5365e-02, -1.8889e-01,\n",
       "         -2.7161e-02, -1.0210e-01,  1.0769e-01,  2.1006e-01, -7.6918e-02],\n",
       "        [ 2.0188e-01,  1.3336e-01, -2.2796e-01,  7.3961e-02, -1.0304e-01,\n",
       "         -1.9428e-01, -1.1191e-01, -2.0107e-01, -2.0270e-01,  8.4727e-02,\n",
       "         -1.3716e-01, -1.7897e-01, -1.7187e-01, -1.4672e-01, -1.7796e-02,\n",
       "          4.2023e-02,  9.2526e-02, -8.8543e-02,  1.3628e-01,  5.5127e-02,\n",
       "         -4.9049e-04, -1.4972e-01, -1.8664e-01, -1.6029e-02, -1.3417e-02,\n",
       "          9.0158e-02,  1.4269e-01,  1.5691e-01,  1.2728e-01,  3.9454e-02,\n",
       "          1.4220e-01,  8.1518e-02,  3.5595e-02,  1.5533e-01, -6.5575e-02,\n",
       "         -1.8570e-01, -3.8394e-02, -4.8947e-03, -1.9070e-01,  1.3085e-01,\n",
       "          7.2817e-02, -1.0590e-01,  2.2396e-01,  2.1375e-01,  1.4028e-01,\n",
       "          1.6343e-01, -1.3430e-01, -1.5844e-01,  1.5613e-01,  1.4334e-01,\n",
       "          1.4049e-01,  1.7984e-01,  2.0686e-01,  1.7009e-01,  2.3598e-02,\n",
       "          7.8531e-02, -2.2207e-01, -1.0692e-01,  3.3468e-02, -7.8975e-02,\n",
       "          2.9008e-02,  1.5372e-01, -7.8587e-02,  1.1937e-01, -6.3902e-02,\n",
       "          1.9283e-01, -1.8030e-01, -7.6877e-02, -1.6952e-01,  6.4143e-02,\n",
       "          1.3329e-01, -2.1826e-01, -5.5282e-02, -2.1561e-01,  1.6588e-01,\n",
       "         -4.5978e-02, -1.5223e-01, -7.7914e-04, -1.7071e-01,  1.3749e-01,\n",
       "          1.5062e-02,  1.7970e-01,  3.6508e-02,  2.0732e-01,  1.7904e-01,\n",
       "         -1.1862e-01, -6.3906e-02,  4.5468e-02, -1.1866e-01, -2.2925e-01,\n",
       "          8.1975e-02,  8.8751e-02,  2.0544e-01,  2.1194e-01,  6.6355e-02,\n",
       "          1.9914e-01, -1.4459e-01,  2.2524e-01, -7.5150e-02, -2.0556e-01],\n",
       "        [ 3.0199e-03,  1.2281e-01, -1.1051e-01, -1.5677e-01, -1.7223e-01,\n",
       "          6.6056e-02,  1.0745e-01, -1.1588e-01, -1.0741e-02, -2.1123e-01,\n",
       "          4.8465e-02, -1.8779e-01,  2.6765e-02, -4.7788e-02,  1.3841e-01,\n",
       "         -1.2091e-01,  2.1489e-01, -1.6087e-01,  1.6840e-01, -1.8129e-01,\n",
       "          8.6943e-02,  2.2009e-01,  2.0455e-01,  1.4194e-01, -1.4318e-01,\n",
       "          1.2976e-01, -9.8650e-02, -7.8544e-02, -2.9601e-02, -8.6885e-02,\n",
       "          1.1783e-01,  7.2003e-02, -1.5626e-01, -6.1226e-02,  1.5325e-01,\n",
       "         -1.8717e-01,  1.8439e-02,  3.3208e-03, -1.0348e-01,  1.6346e-01,\n",
       "          1.8711e-01,  1.5526e-02,  6.7529e-02,  8.1953e-03,  3.9084e-02,\n",
       "          1.6896e-01,  5.4419e-02, -1.7068e-01,  4.2303e-02, -7.9963e-04,\n",
       "         -5.3123e-02, -1.5102e-01,  6.1128e-02, -1.3325e-01, -1.6641e-01,\n",
       "          1.9013e-01,  2.3074e-01,  7.0731e-02, -9.3776e-02,  1.3826e-01,\n",
       "          5.1297e-02,  1.9018e-01, -1.8725e-01, -1.5236e-01,  2.1409e-01,\n",
       "          1.8767e-01,  1.4084e-01, -1.3926e-01,  7.1327e-02,  1.9645e-01,\n",
       "          2.0589e-01, -1.3958e-01,  1.2371e-01, -6.1018e-03, -7.8139e-02,\n",
       "          1.1483e-01, -1.3785e-01, -1.3652e-01,  1.9254e-01,  8.2932e-03,\n",
       "          2.0546e-01, -2.5315e-02, -7.8859e-02,  1.5214e-01,  1.8180e-01,\n",
       "         -1.6828e-02, -1.0081e-01,  2.2941e-03,  8.2849e-02, -7.0130e-02,\n",
       "          1.0734e-01, -1.2983e-02, -2.9529e-02,  5.4832e-03,  1.4931e-01,\n",
       "         -2.8481e-02,  1.9764e-01,  1.2162e-01,  5.5866e-02, -3.3024e-02],\n",
       "        [ 1.7827e-01, -1.6086e-01,  3.7990e-02, -1.4460e-01,  1.0120e-01,\n",
       "         -6.1911e-02,  9.5425e-02, -1.1799e-01, -7.6362e-02, -1.8157e-01,\n",
       "          1.2341e-01,  2.2584e-01,  2.2538e-01, -7.2983e-02, -1.3688e-01,\n",
       "          1.4446e-01, -1.7223e-01,  2.2457e-01, -1.4263e-01,  7.2293e-02,\n",
       "         -3.7317e-02, -4.0195e-02, -1.7345e-01, -1.5753e-01, -4.2036e-05,\n",
       "         -2.1775e-01,  6.8566e-02,  2.1042e-01, -1.3048e-01, -2.1555e-01,\n",
       "          4.4403e-02,  9.1831e-03, -8.6484e-02,  2.2086e-01,  1.3086e-01,\n",
       "          1.5367e-01, -1.6627e-01,  1.4434e-01, -1.9899e-01,  1.2315e-01,\n",
       "          4.6375e-02,  1.9127e-01,  1.1909e-02,  2.0159e-01,  1.8471e-01,\n",
       "         -1.7837e-01, -1.9990e-02,  6.9600e-02, -1.0840e-01, -1.8744e-01,\n",
       "          2.1237e-01, -1.7175e-01, -1.2385e-01, -2.1765e-01, -8.6917e-02,\n",
       "         -5.2186e-02, -1.0664e-01, -1.0094e-01, -5.2576e-03,  4.2377e-02,\n",
       "         -1.4884e-01, -6.1999e-02, -9.0060e-02,  2.5760e-02, -9.2085e-02,\n",
       "         -2.2178e-01, -1.1131e-01, -2.0469e-01,  4.6106e-02,  1.6786e-01,\n",
       "         -6.2366e-02, -4.4571e-02, -1.6329e-01, -1.3549e-03, -5.9686e-02,\n",
       "         -2.1800e-01, -1.4139e-01, -1.3279e-01, -1.4379e-02,  1.2279e-01,\n",
       "          1.0779e-01, -1.3608e-01,  7.6027e-03,  2.0891e-01, -4.2261e-02,\n",
       "          1.3421e-01,  2.2598e-02, -2.3315e-01,  1.2347e-02,  8.8129e-02,\n",
       "          4.5762e-02, -2.1712e-02,  4.3529e-02, -1.2786e-01, -1.0344e-01,\n",
       "          1.3994e-01,  1.8324e-01,  1.6684e-01,  4.2964e-03,  1.2270e-01],\n",
       "        [-4.9278e-02,  2.0566e-01,  1.6608e-01,  1.2352e-01,  2.7307e-02,\n",
       "         -1.8218e-01,  1.8047e-01,  7.4527e-02, -1.4888e-01,  1.3253e-01,\n",
       "          8.3222e-02, -9.6596e-02,  1.8986e-01, -3.4671e-02,  1.4673e-01,\n",
       "          1.6245e-01, -4.0215e-02,  4.7670e-02, -4.7300e-02, -6.5318e-02,\n",
       "         -1.5112e-01, -9.3357e-02,  1.6889e-01,  9.9581e-02,  2.2986e-01,\n",
       "         -2.3033e-01, -1.3348e-01,  5.5246e-02, -2.3314e-01,  2.0432e-01,\n",
       "          1.3541e-01,  7.0462e-02, -7.9555e-02,  2.1849e-01, -1.9103e-01,\n",
       "          4.2473e-02,  9.7368e-02,  9.9481e-02,  9.6434e-02, -4.8733e-02,\n",
       "         -9.4677e-02,  2.6037e-02, -1.2193e-01,  8.3705e-02,  6.2580e-02,\n",
       "         -3.3672e-02, -8.0549e-02, -1.8363e-02,  7.6251e-03, -2.0821e-01,\n",
       "         -1.7273e-01,  2.7384e-03, -5.3875e-02, -8.6351e-03, -7.4216e-02,\n",
       "         -6.6884e-02, -8.7654e-02,  8.2009e-03,  1.9441e-01, -9.0113e-02,\n",
       "          2.1158e-01, -1.8166e-01,  1.5836e-01, -4.6792e-02,  5.1327e-02,\n",
       "          1.4101e-02, -3.4692e-03, -8.8878e-02,  1.1872e-01,  1.8260e-01,\n",
       "          1.5921e-01, -1.7167e-01, -1.3506e-03, -8.1895e-02,  2.2041e-01,\n",
       "          2.1224e-01, -1.8428e-01, -5.7950e-02,  2.1308e-01, -4.3194e-02,\n",
       "          4.3508e-02, -1.1068e-01, -1.3167e-01,  4.4748e-02, -1.1941e-01,\n",
       "          6.4289e-02, -1.4900e-02, -2.6759e-02, -1.7289e-02, -1.9399e-01,\n",
       "         -1.8176e-01, -3.6481e-02, -1.8168e-01, -4.3244e-02, -4.2653e-02,\n",
       "          1.9977e-01,  1.1604e-01,  1.0972e-01,  9.2790e-02, -1.1524e-01]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #xavier initialization을 이용하여 각 layer의 weight 초기화\n",
    "torch.nn.init.xavier_uniform_(linear1.weight)\n",
    "torch.nn.init.xavier_uniform_(linear2.weight)\n",
    "torch.nn.init.xavier_uniform_(linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Sequential을 이용하여 model 정의하기(쌓는 순서: linear-BatchNormalization Layer - ReLU- DropOut)\n",
    "model_2 = torch.nn.Sequential(linear4, bn1, relu, dropout,\n",
    "                            linear5, bn2, relu, dropout,\n",
    "                            linear6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost/loss & optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=learning_rate)\n",
    "\n",
    "#cost 계산을 위한 변수 설정\n",
    "train_total_batch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.187100127\n",
      "Epoch: 0002 cost = 0.188789889\n",
      "Epoch: 0003 cost = 0.185400680\n",
      "Epoch: 0004 cost = 0.177967817\n",
      "Epoch: 0005 cost = 0.183117822\n",
      "Epoch: 0006 cost = 0.185920298\n",
      "Epoch: 0007 cost = 0.175034031\n",
      "Epoch: 0008 cost = 0.176049247\n",
      "Epoch: 0009 cost = 0.164263591\n",
      "Epoch: 0010 cost = 0.168677479\n",
      "Epoch: 0011 cost = 0.165325537\n",
      "Epoch: 0012 cost = 0.170857623\n",
      "Epoch: 0013 cost = 0.163866669\n",
      "Epoch: 0014 cost = 0.155931011\n",
      "Epoch: 0015 cost = 0.148930907\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "#Training epoch (cost 값 초기 설정(0으로)과 model의 train 설정 꼭 할 것)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    model_2.train() #모델의 train 설정\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "#train dataset을 불러오고(X,Y 불러오기), back propagation과 optimizer를 사용하여loss를 최적화하는 코드\n",
    "\n",
    "    for X, Y in train_loader:\n",
    "        X = X.view(-1, 28 * 28)\n",
    "        Y = Y\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model_2(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9043999910354614\n"
     ]
    }
   ],
   "source": [
    "#test data로 모델의 정확도를 검증하는 코드 (model의 evaluation mode 설정 꼭 할 것)\n",
    "#X_test 불러올 때 view를 사용하여 차원 변환할 것/ Y_test를 불러올때 labels사용\n",
    "#accuracy의 초기 값 설정(0으로) 꼭 할 것\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_2.eval()\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float()\n",
    "    Y_test = mnist_test.test_labels\n",
    "\n",
    "    prediction = model_2(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5\n",
      "Prediction:  5\n"
     ]
    }
   ],
   "source": [
    "##Test set에서 random으로 data를 뽑아 Label과 Prediction을 비교하는 코드\n",
    "r = random.randint(0, len(mnist_test)-1)\n",
    "X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 *28).float()\n",
    "Y_single_data = mnist_test.test_labels[r:r + 1]\n",
    "print('Label: ', Y_single_data.item())\n",
    "single_prediction = model_2(X_single_data)\n",
    "print('Prediction: ', torch.argmax(single_prediction, 1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
